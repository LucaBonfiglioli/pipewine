{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipewine","text":"<p>Pipewine is a complete rewrite of the Pipelime library and intends to serve the same purpose: provide a set of tools to manipulate multi-modal small/medium-sized datasets, mainly for research purposes in the CV/ML domain.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Unified access pattern to datasets of various formats, origin and content.</li> <li>Underfolder, a quick and easy filesystem-based dataset format good for small/medium datasets.</li> <li>Common data encoding formats for images, text and metadata.</li> <li>Common operators to manipulate existing datasets.</li> <li>Workflows that transform data in complex DAGs (Directed Acyclic Graph) pipelines.</li> <li>CLI (Command Line Interface) to quickly run simple workflows without writing a full python script.</li> <li>Extendibility, allowing the user to easily extend the library in many aspects, adding components that seamlessly integrate with the built-in ones:<ul> <li>Add custom dataset formats</li> <li>Add custom data encoding formats</li> <li>Add custom operators on datasets</li> <li>Register components to the CLI</li> </ul> </li> </ul>"},{"location":"#rationale","title":"\u2b50\ufe0f Rationale","text":"<p>Pipewine started from a refactoring of some core components of the Pipelime library, but it soon turned into a complete rewrite that aims at solving some architectural issues of Pipelime, namely:</p> <ul> <li>Dependency Hell: Pipelime had so many of dependencies and contstraints that made it very hard to install in many environments due to conflicts and incompatibilities. Outdated dependencies at the core of the library (e.g. Pydantic 1) make it incompatible with new python packages that use Pydantic 2+. Upgrading these dependencies is currently impossible without breaking everything.</li> <li>Over-reliance on Pydantic: Making most library components inherit from Pydantic <code>BaseModel</code>'s (expecially with the 1.0 major) completely violate type-safety and result in type-checking errors if used with any modern python type-checker like MyPy or PyLance. As a result, IntelliSense and autocompletion do not work properly, requiring the user to fill their code with <code>type: ignore</code> directives. Furthermore, Pydantic runtime validation has some serious performance issues, slowing down the computation significantly (especially before rewriting the whole library in Rust).</li> <li>Lack of typed abstractions: Many components of Pipelime (e.g. <code>SamplesSequence</code>, <code>Sample</code>, <code>SampleStage</code>, <code>PipedSequence</code> and <code>PipelimeCommand</code>) do not carry any information on the type of data they operate on, thus making the usage of Pipelime error-prone: the user cannot rely on the type checker to know what type of data is currently processing. Some later components like <code>Entity</code> and <code>Action</code> aim at mitigating this problem by encapsulating samples inside a Pydantic model, but they ended up being almost unused because they integrate poorly with the rest of the library.</li> <li>Confusing CLI: Pipelime CLI was built with the purpose of being able to directly instantiate any (possibly nested) Pydantic model. This resulted in a very powerful CLI that could do pretty much anything but was also very confusing to use (and even worse to maintain). As a contributor of the pipelime library, I never met anybody able to read and understand the CLI validation errors.</li> <li>Everything is CLI: All Pipelime components are also CLI components. This has the positive effect of eliminating the need to manually write CLI hooks for data operators, but also couples the CLI with the API, requiring everything to be serializable as a list of key-value pairs of (possibly nested) builtin values. We solved this using Pydantic, but was it really worth the cost?</li> <li>Feature Creep: Lots of features like streams, remotes, choixe add a lot of complexity but are mostly unused. </li> </ul> <p>Key development decisions behind Pipewine:</p> <ul> <li>Minimal Dependencies: Rely on the standard library as much as possible, only depend on widely-used and maintained 3rd-party libraries. </li> <li>Type-Safety: The library heavily relies on Python type annotations to achieve a desirable level of type safety at development-time. Runtime type checking is limited to external data validation to not hinder the performance too much. The user should be able to rely on any modern static type checker to notice and correct bugs. </li> <li>Pydantic: Limit the use of Pydantic for stuff that is not strictly external data validation. When serialization and runtime validation are not needed, plain dataclasses are a perfect alternative.</li> <li>CLI Segregation: The CLI is merely a tool to quickly access some of the core library functionalities, no core component should ever depend on it. </li> <li>Limited Compatibility Pipewine should be able to read data written by Pipelime and potentially be used alongside it, but it is not intended to be a backward-compatible update, it is in fact a separate project with a separate development cycle.</li> <li>Feature Pruning Avoid including complex features that no one is going to use, instead, focus on keeping the library easy to extend. </li> </ul>"},{"location":"autoapi/summary/","title":"Summary","text":"<ul> <li>pipewine<ul> <li>bundle</li> <li>cli<ul> <li>extension</li> <li>main</li> <li>mappers</li> <li>ops</li> <li>sinks</li> <li>sources</li> <li>utils</li> <li>workflows</li> </ul> </li> <li>dataset</li> <li>grabber</li> <li>item</li> <li>mappers<ul> <li>base</li> <li>cache</li> <li>compose</li> <li>crypto</li> <li>item_transform</li> <li>key_transform</li> </ul> </li> <li>operators<ul> <li>base</li> <li>cache</li> <li>functional</li> <li>iter</li> <li>merge</li> <li>rand</li> <li>split</li> </ul> </li> <li>parsers<ul> <li>base</li> <li>image_parser</li> <li>metadata_parser</li> <li>numpy_parser</li> <li>pickle_parser</li> </ul> </li> <li>reader</li> <li>sample</li> <li>sinks<ul> <li>base</li> <li>fs_utils</li> <li>underfolder</li> </ul> </li> <li>sources<ul> <li>base</li> <li>images_folder</li> <li>underfolder</li> </ul> </li> <li>workflows<ul> <li>drawing</li> <li>events</li> <li>execution</li> <li>model</li> <li>tracking</li> </ul> </li> </ul> </li> </ul>"},{"location":"autoapi/pipewine/","title":"pipewine","text":"<p>Pipewine root package, containing all the core classes and functions of the library.</p> <p>Everything except the <code>pipewine.workflows</code> and <code>pipewine.cli</code> modules is imported here, so that the user can conveniently access the most important classes and functions directly from the <code>pipewine</code> package.</p> <p>The Pipewine API reference documentation is available as docstrings in every public module, class, function and attribute. This form of documentation assumes is intended to be used with an interactive Python environment, such as IPython or Jupyter, or through the static documentation website.</p> <p>The API Reference assumes that the developer is familiar with the basic concepts of the Pipewine library, available in the \"Usage\" section of the documentation.</p>"},{"location":"autoapi/pipewine/#pipewine.__version__","title":"<code>__version__ = '0.2.0'</code>  <code>module-attribute</code>","text":"<p>Pipewine package version.</p>"},{"location":"autoapi/pipewine/bundle/","title":"bundle","text":"<p><code>Bundle</code> class and metaclass for defining bounded dataclasses.</p>"},{"location":"autoapi/pipewine/bundle/#pipewine.bundle.Bundle","title":"<code>Bundle</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Base class for all <code>Bundle</code> types, which are dataclasses whose fields are bounded to a specific type <code>T</code>.</p> <p>Besides the <code>as_dict</code> and <code>from_dict</code> methods, <code>Bundle</code> classes should not have define any other methods, as they are meant to be simple anemic data containers.</p> Source code in <code>pipewine/bundle.py</code> <pre><code>class Bundle(Generic[T], metaclass=BundleMeta, _is_root=True):\n    \"\"\"Base class for all `Bundle` types, which are dataclasses whose fields are\n    bounded to a specific type `T`.\n\n    Besides the `as_dict` and `from_dict` methods, `Bundle` classes should not have\n    define any other methods, as they are meant to be simple anemic data containers.\n    \"\"\"\n\n    def __init__(self, /, **kwargs: T) -&gt; None:\n        \"\"\"Constructor stub that does nothing, as it will be replaced by the one\n        generated by the `dataclass` decorator on sub-class creation.\n        \"\"\"\n        pass\n\n    def as_dict(self) -&gt; dict[str, T]:\n        \"\"\"Convert the `Bundle` instance to a plain python dictionary.\n\n        Returns:\n            dict[str, T]: A dictionary mapping the field names to their values.\n        \"\"\"\n        return self.__dict__\n\n    @classmethod\n    def from_dict(cls, **data) -&gt; Self:\n        \"\"\"Create a new `Bundle` instance from a dictionary. This method is supposed to\n        be called on the concrete sub-classes, not on the `Bundle` base class for it has\n        no fields.\n        \"\"\"\n        return cls(**data)\n\n    def __getstate__(self) -&gt; dict[str, T]:\n        return self.as_dict()\n\n    def __setstate__(self, data: dict[str, T]) -&gt; None:\n        for k, v in data.items():\n            setattr(self, k, v)\n</code></pre>"},{"location":"autoapi/pipewine/bundle/#pipewine.bundle.Bundle.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Constructor stub that does nothing, as it will be replaced by the one generated by the <code>dataclass</code> decorator on sub-class creation.</p> Source code in <code>pipewine/bundle.py</code> <pre><code>def __init__(self, /, **kwargs: T) -&gt; None:\n    \"\"\"Constructor stub that does nothing, as it will be replaced by the one\n    generated by the `dataclass` decorator on sub-class creation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/bundle/#pipewine.bundle.Bundle.as_dict","title":"<code>as_dict()</code>","text":"<p>Convert the <code>Bundle</code> instance to a plain python dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, T]</code> <p>dict[str, T]: A dictionary mapping the field names to their values.</p> Source code in <code>pipewine/bundle.py</code> <pre><code>def as_dict(self) -&gt; dict[str, T]:\n    \"\"\"Convert the `Bundle` instance to a plain python dictionary.\n\n    Returns:\n        dict[str, T]: A dictionary mapping the field names to their values.\n    \"\"\"\n    return self.__dict__\n</code></pre>"},{"location":"autoapi/pipewine/bundle/#pipewine.bundle.Bundle.from_dict","title":"<code>from_dict(**data)</code>  <code>classmethod</code>","text":"<p>Create a new <code>Bundle</code> instance from a dictionary. This method is supposed to be called on the concrete sub-classes, not on the <code>Bundle</code> base class for it has no fields.</p> Source code in <code>pipewine/bundle.py</code> <pre><code>@classmethod\ndef from_dict(cls, **data) -&gt; Self:\n    \"\"\"Create a new `Bundle` instance from a dictionary. This method is supposed to\n    be called on the concrete sub-classes, not on the `Bundle` base class for it has\n    no fields.\n    \"\"\"\n    return cls(**data)\n</code></pre>"},{"location":"autoapi/pipewine/bundle/#pipewine.bundle.BundleMeta","title":"<code>BundleMeta</code>","text":"<p>               Bases: <code>ABCMeta</code></p> <p>Metaclass for <code>Bundle</code> classes, which automatically applies the <code>dataclass</code> decorator on sub-class creation, and explicitly tells the type checker that the <code>Bundle</code> class behaves like a dataclass, without having to explicitly use the <code>@dataclass</code> decorator.</p> <p>This metaclass is not meant to be used directly, just inheriting from <code>Bundle</code> is enough to get the desired behavior.</p> Source code in <code>pipewine/bundle.py</code> <pre><code>@dataclass_transform(kw_only_default=True)\nclass BundleMeta(ABCMeta):\n    \"\"\"Metaclass for `Bundle` classes, which automatically applies the `dataclass`\n    decorator on sub-class creation, and explicitly tells the type checker that the\n    `Bundle` class behaves like a dataclass, without having to explicitly use the\n    `@dataclass` decorator.\n\n    This metaclass is not meant to be used directly, just inheriting from `Bundle` is\n    enough to get the desired behavior.\n    \"\"\"\n\n    def __new__(\n        cls,\n        name: str,\n        bases: tuple[type, ...],\n        namespace: dict[str, Any],\n        /,\n        **kwds: Any,\n    ):\n        the_cls = super().__new__(cls, name, bases, namespace)\n        if not kwds.get(\"_is_root\"):\n            the_cls = dataclass(the_cls)  # type: ignore\n        return the_cls\n</code></pre>"},{"location":"autoapi/pipewine/dataset/","title":"dataset","text":"<p>Base classes for Pipewine datasets.</p>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>ABC</code>, <code>Sequence[T]</code></p> <p>Base class for all Pipewine datasets. A dataset is a collection of samples that can be iterated over, sliced, and indexed, implementing the <code>Sequence</code> protocol.</p> <p>Subclasses must implement the <code>size</code>, <code>get_sample</code>, and <code>get_slice</code> abstract methods to define the dataset behavior.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>class Dataset[T: Sample](ABC, Sequence[T]):\n    \"\"\"Base class for all Pipewine datasets. A dataset is a collection of samples\n    that can be iterated over, sliced, and indexed, implementing the `Sequence`\n    protocol.\n\n    Subclasses must implement the `size`, `get_sample`, and `get_slice` abstract\n    methods to define the dataset behavior.\n    \"\"\"\n\n    @abstractmethod\n    def size(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset, same as `len()`.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_sample(self, idx: int) -&gt; T:\n        \"\"\"Return the sample at the given index.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_slice(self, idx: slice) -&gt; \"Dataset[T]\":\n        \"\"\"Return a new dataset containing the samples in the given slice.\"\"\"\n        pass\n\n    def __len__(self) -&gt; int:\n        return self.size()\n\n    @overload\n    def __getitem__(self, idx: int) -&gt; T: ...\n    @overload\n    def __getitem__(self, idx: slice) -&gt; \"Dataset[T]\": ...\n    def __getitem__(self, idx: int | slice) -&gt; T | \"Dataset[T]\":\n        if isinstance(idx, int):\n            if idx &gt;= self.size():\n                raise IndexError(idx)\n            return self.get_sample(idx)\n        else:\n            return self.get_slice(idx)\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.Dataset.get_sample","title":"<code>get_sample(idx)</code>  <code>abstractmethod</code>","text":"<p>Return the sample at the given index.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>@abstractmethod\ndef get_sample(self, idx: int) -&gt; T:\n    \"\"\"Return the sample at the given index.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.Dataset.get_slice","title":"<code>get_slice(idx)</code>  <code>abstractmethod</code>","text":"<p>Return a new dataset containing the samples in the given slice.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>@abstractmethod\ndef get_slice(self, idx: slice) -&gt; \"Dataset[T]\":\n    \"\"\"Return a new dataset containing the samples in the given slice.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.Dataset.size","title":"<code>size()</code>  <code>abstractmethod</code>","text":"<p>Return the number of samples in the dataset, same as <code>len()</code>.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>@abstractmethod\ndef size(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset, same as `len()`.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.LazyDataset","title":"<code>LazyDataset</code>","text":"<p>               Bases: <code>Dataset[T]</code></p> <p>Dataset implementation that lazily generates samples on demand, calling a user-provided function to get the requested samples.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>class LazyDataset[T: Sample](Dataset[T]):\n    \"\"\"Dataset implementation that lazily generates samples on demand, calling a\n    user-provided function to get the requested samples.\n    \"\"\"\n\n    def __init__(\n        self,\n        size: int,\n        get_sample_fn: Callable[[int], T],\n        index_fn: Callable[[int], int] | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            size (int): Number of samples in the dataset.\n            get_sample_fn (Callable[[int], T]): Function that returns the sample at\n                the given index.\n            index_fn (Callable[[int], int] | None, optional): Additional function that\n                can be used to change the index before calling `get_sample_fn` with it.\n                Defaults to None, in which case the index is passed as-is to\n                `get_sample_fn`.\n        \"\"\"\n        self._size = size\n        self._get_sample_fn = get_sample_fn\n        self._index_fn = index_fn\n\n    def size(self) -&gt; int:\n        return self._size\n\n    def get_sample(self, idx: int) -&gt; T:\n        return self._get_sample_fn(self._index_fn(idx) if self._index_fn else idx)\n\n    def _slice_fn(self, step: int, start: int, x: int) -&gt; int:\n        return x * step + start\n\n    def get_slice(self, idx: slice) -&gt; Dataset[T]:\n        start, stop, step = idx.indices(self.size())\n        return LazyDataset(\n            max(0, math.ceil((stop - start) / step)),\n            self.get_sample,\n            partial(self._slice_fn, step, start),\n        )\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.LazyDataset.__init__","title":"<code>__init__(size, get_sample_fn, index_fn=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Number of samples in the dataset.</p> required <code>get_sample_fn</code> <code>Callable[[int], T]</code> <p>Function that returns the sample at the given index.</p> required <code>index_fn</code> <code>Callable[[int], int] | None</code> <p>Additional function that can be used to change the index before calling <code>get_sample_fn</code> with it. Defaults to None, in which case the index is passed as-is to <code>get_sample_fn</code>.</p> <code>None</code> Source code in <code>pipewine/dataset.py</code> <pre><code>def __init__(\n    self,\n    size: int,\n    get_sample_fn: Callable[[int], T],\n    index_fn: Callable[[int], int] | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        size (int): Number of samples in the dataset.\n        get_sample_fn (Callable[[int], T]): Function that returns the sample at\n            the given index.\n        index_fn (Callable[[int], int] | None, optional): Additional function that\n            can be used to change the index before calling `get_sample_fn` with it.\n            Defaults to None, in which case the index is passed as-is to\n            `get_sample_fn`.\n    \"\"\"\n    self._size = size\n    self._get_sample_fn = get_sample_fn\n    self._index_fn = index_fn\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.ListDataset","title":"<code>ListDataset</code>","text":"<p>               Bases: <code>Dataset[T]</code></p> <p>Simple dataset implementation that wraps a list of samples.</p> Source code in <code>pipewine/dataset.py</code> <pre><code>class ListDataset[T: Sample](Dataset[T]):\n    \"\"\"Simple dataset implementation that wraps a list of samples.\"\"\"\n\n    def __init__(self, samples: Sequence[T]) -&gt; None:\n        \"\"\"\n        Args:\n            samples (Sequence[T]): List of samples to wrap in the dataset.\n        \"\"\"\n        super().__init__()\n        self._samples = samples\n\n    def get_sample(self, idx: int) -&gt; T:\n        return self._samples[idx]\n\n    def get_slice(self, idx: slice) -&gt; \"Dataset[T]\":\n        return self.__class__(self._samples[idx])\n\n    def size(self) -&gt; int:\n        return len(self._samples)\n</code></pre>"},{"location":"autoapi/pipewine/dataset/#pipewine.dataset.ListDataset.__init__","title":"<code>__init__(samples)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>samples</code> <code>Sequence[T]</code> <p>List of samples to wrap in the dataset.</p> required Source code in <code>pipewine/dataset.py</code> <pre><code>def __init__(self, samples: Sequence[T]) -&gt; None:\n    \"\"\"\n    Args:\n        samples (Sequence[T]): List of samples to wrap in the dataset.\n    \"\"\"\n    super().__init__()\n    self._samples = samples\n</code></pre>"},{"location":"autoapi/pipewine/grabber/","title":"grabber","text":"<p>Multiprocessing utilities for iterating over a sequence with parallelism.</p>"},{"location":"autoapi/pipewine/grabber/#pipewine.grabber.Grabber","title":"<code>Grabber</code>","text":"<p>Grabber utility for iterating over a sequence using parallelism.</p> Source code in <code>pipewine/grabber.py</code> <pre><code>class Grabber:\n    \"\"\"Grabber utility for iterating over a sequence using parallelism.\"\"\"\n\n    def __init__(\n        self, num_workers: int = 0, prefetch: int = 2, keep_order: bool = True\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            num_workers (int, optional): Number of worker processes to use for\n                parallelism. If 0, no parallelism is used. Defaults to 0.\n            prefetch (int, optional): Number of elements to prefetch in each worker\n                process. Defaults to 2.\n            keep_order (bool, optional): Whether to keep the order of the elements in\n                the sequence. If `True`, the elements are yielded in the same order as they\n                appear in the sequence. If `False`, the elements are yielded in the\n                order they are processed. Defaults to `True`.\n        \"\"\"\n        super().__init__()\n        self.num_workers = num_workers\n        self.prefetch = prefetch\n        self.keep_order = keep_order\n\n    def __call__[\n        T\n    ](\n        self,\n        seq: Sequence[T],\n        *,\n        callback: Callable[[int], None] | None = None,\n        worker_init_fn: tuple[Callable, Sequence] | None = None,\n    ) -&gt; _GrabContext[T]:\n        \"\"\"Create a context manager for iterating over a sequence with parallelism.\n\n        Note: only the call to the `__getitem__` method is actually parallelized, the\n        rest of the iteration is done in the main process, making this useful when\n        iterating over `LazyDataset` instances, that may perform expensive operations\n        when fetching the samples.\n\n        Args:\n            seq (Sequence[T]): Sequence of elements to iterate over.\n            callback (Callable[[int], None] | None, optional): Optional callback\n                function to be called on each iteration. The callback function should\n                accept the index of the current element as its only argument. Defaults\n                to `None`.\n            worker_init_fn (tuple[Callable, Sequence] | None, optional): Optional tuple\n                containing a function and a sequence of arguments to be called in each\n                worker process before starting the iteration. Defaults to `None`.\n\n        Returns:\n            _GrabContext[T]: Context manager for iterating over the sequence with\n                parallelism.\n\n        Examples:\n            ```python\n            seq = list(range(10))\n            grabber = Grabber(num_workers=4, prefetch=4)\n            with grabber(seq) as it:\n                for idx, elem in it:\n                    print(idx, elem)\n            ```\n        \"\"\"\n        return _GrabContext(\n            self.num_workers,\n            self.prefetch,\n            self.keep_order,\n            seq,\n            callback=callback,\n            worker_init_fn=worker_init_fn,\n        )\n</code></pre>"},{"location":"autoapi/pipewine/grabber/#pipewine.grabber.Grabber.__call__","title":"<code>__call__(seq, *, callback=None, worker_init_fn=None)</code>","text":"<p>Create a context manager for iterating over a sequence with parallelism.</p> <p>Note: only the call to the <code>__getitem__</code> method is actually parallelized, the rest of the iteration is done in the main process, making this useful when iterating over <code>LazyDataset</code> instances, that may perform expensive operations when fetching the samples.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Sequence[T]</code> <p>Sequence of elements to iterate over.</p> required <code>callback</code> <code>Callable[[int], None] | None</code> <p>Optional callback function to be called on each iteration. The callback function should accept the index of the current element as its only argument. Defaults to <code>None</code>.</p> <code>None</code> <code>worker_init_fn</code> <code>tuple[Callable, Sequence] | None</code> <p>Optional tuple containing a function and a sequence of arguments to be called in each worker process before starting the iteration. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>_GrabContext[T]</code> <p>_GrabContext[T]: Context manager for iterating over the sequence with parallelism.</p> <p>Examples:</p> <pre><code>seq = list(range(10))\ngrabber = Grabber(num_workers=4, prefetch=4)\nwith grabber(seq) as it:\n    for idx, elem in it:\n        print(idx, elem)\n</code></pre> Source code in <code>pipewine/grabber.py</code> <pre><code>def __call__[\n    T\n](\n    self,\n    seq: Sequence[T],\n    *,\n    callback: Callable[[int], None] | None = None,\n    worker_init_fn: tuple[Callable, Sequence] | None = None,\n) -&gt; _GrabContext[T]:\n    \"\"\"Create a context manager for iterating over a sequence with parallelism.\n\n    Note: only the call to the `__getitem__` method is actually parallelized, the\n    rest of the iteration is done in the main process, making this useful when\n    iterating over `LazyDataset` instances, that may perform expensive operations\n    when fetching the samples.\n\n    Args:\n        seq (Sequence[T]): Sequence of elements to iterate over.\n        callback (Callable[[int], None] | None, optional): Optional callback\n            function to be called on each iteration. The callback function should\n            accept the index of the current element as its only argument. Defaults\n            to `None`.\n        worker_init_fn (tuple[Callable, Sequence] | None, optional): Optional tuple\n            containing a function and a sequence of arguments to be called in each\n            worker process before starting the iteration. Defaults to `None`.\n\n    Returns:\n        _GrabContext[T]: Context manager for iterating over the sequence with\n            parallelism.\n\n    Examples:\n        ```python\n        seq = list(range(10))\n        grabber = Grabber(num_workers=4, prefetch=4)\n        with grabber(seq) as it:\n            for idx, elem in it:\n                print(idx, elem)\n        ```\n    \"\"\"\n    return _GrabContext(\n        self.num_workers,\n        self.prefetch,\n        self.keep_order,\n        seq,\n        callback=callback,\n        worker_init_fn=worker_init_fn,\n    )\n</code></pre>"},{"location":"autoapi/pipewine/grabber/#pipewine.grabber.Grabber.__init__","title":"<code>__init__(num_workers=0, prefetch=2, keep_order=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>Number of worker processes to use for parallelism. If 0, no parallelism is used. Defaults to 0.</p> <code>0</code> <code>prefetch</code> <code>int</code> <p>Number of elements to prefetch in each worker process. Defaults to 2.</p> <code>2</code> <code>keep_order</code> <code>bool</code> <p>Whether to keep the order of the elements in the sequence. If <code>True</code>, the elements are yielded in the same order as they appear in the sequence. If <code>False</code>, the elements are yielded in the order they are processed. Defaults to <code>True</code>.</p> <code>True</code> Source code in <code>pipewine/grabber.py</code> <pre><code>def __init__(\n    self, num_workers: int = 0, prefetch: int = 2, keep_order: bool = True\n) -&gt; None:\n    \"\"\"\n    Args:\n        num_workers (int, optional): Number of worker processes to use for\n            parallelism. If 0, no parallelism is used. Defaults to 0.\n        prefetch (int, optional): Number of elements to prefetch in each worker\n            process. Defaults to 2.\n        keep_order (bool, optional): Whether to keep the order of the elements in\n            the sequence. If `True`, the elements are yielded in the same order as they\n            appear in the sequence. If `False`, the elements are yielded in the\n            order they are processed. Defaults to `True`.\n    \"\"\"\n    super().__init__()\n    self.num_workers = num_workers\n    self.prefetch = prefetch\n    self.keep_order = keep_order\n</code></pre>"},{"location":"autoapi/pipewine/grabber/#pipewine.grabber.InheritedData","title":"<code>InheritedData</code>","text":"<p>Data that is inherited by all subprocesses at creation time. This is a workaround to allow arbitrary data to be shared between the main and child process using inheritance.</p> <p>This is crucial to pass things like sockets, file descriptors, and other resources that can only be shared between processes through inheritance.</p> <p>Note: The data stored in this class is only shared at the time of the subprocess creation, and it is not updated if the data changes in the main or child process, as it is not shared memory nor managed by any synchronization mechanism.</p> Source code in <code>pipewine/grabber.py</code> <pre><code>class InheritedData:\n    \"\"\"Data that is inherited by all subprocesses at creation time. This is a\n    workaround to allow arbitrary data to be shared between the main and child process\n    using inheritance.\n\n    This is crucial to pass things like sockets, file descriptors, and other resources\n    that can only be shared between processes through inheritance.\n\n    Note: The data stored in this class is only shared at the time of the subprocess\n    creation, and it is not updated if the data changes in the main or child process, as\n    it is not shared memory nor managed by any synchronization mechanism.\n    \"\"\"\n\n    data: dict[str, Any] = {}\n    \"\"\"Dict-like container for all the data that is inherited by the subprocesses.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/grabber/#pipewine.grabber.InheritedData.data","title":"<code>data = {}</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Dict-like container for all the data that is inherited by the subprocesses.</p>"},{"location":"autoapi/pipewine/item/","title":"item","text":"<p><code>Item</code> base class and implementations to represent data items in Pipewine.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.CachedItem","title":"<code>CachedItem</code>","text":"<p>               Bases: <code>Item[T]</code></p> <p>A <code>CachedItem</code> is an <code>Item</code> that wraps another item and caches the value it returns when it is requested for the first time. Subsequent requests will return the cached value without calling the wrapped item again.</p> Source code in <code>pipewine/item.py</code> <pre><code>class CachedItem[T: Any](Item[T]):\n    \"\"\"A `CachedItem` is an `Item` that wraps another item and caches the value it\n    returns when it is requested for the first time. Subsequent requests will return\n    the cached value without calling the wrapped item again.\n    \"\"\"\n\n    def __init__(self, source: Item[T], shared: bool | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            source (Item[T]): The item to wrap and cache.\n            shared (bool | None, optional): The sharedness of the item. If `None`, the\n                sharedness of the item is the same as the sharedness of the wrapped item.\n                Defaults to None.\n        \"\"\"\n        self._source = source\n        self._cache = None\n        self._shared = shared\n\n    def _get(self) -&gt; T:\n        if self._cache is None:\n            self._cache = self._source()\n        return self._cache\n\n    def _get_parser(self) -&gt; Parser[T]:\n        return self._source._get_parser()\n\n    def _is_shared(self) -&gt; bool:\n        if self._shared is None:\n            return self._source.is_shared\n        return self._shared\n\n    def with_sharedness(self, shared: bool) -&gt; Self:\n        return type(self)(self._source, shared=shared)\n\n    @property\n    def source(self) -&gt; Item[T]:\n        \"\"\"Return the wrapped item.\"\"\"\n        return self._source\n\n    @property\n    def source_recursive(self) -&gt; Item[T]:\n        \"\"\"Return the original source item, unwrapping any cached items in between.\"\"\"\n        source: Item[T] = self\n        while isinstance(source, CachedItem):\n            source = source.source\n        return source\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.CachedItem.source","title":"<code>source</code>  <code>property</code>","text":"<p>Return the wrapped item.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.CachedItem.source_recursive","title":"<code>source_recursive</code>  <code>property</code>","text":"<p>Return the original source item, unwrapping any cached items in between.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.CachedItem.__init__","title":"<code>__init__(source, shared=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source</code> <code>Item[T]</code> <p>The item to wrap and cache.</p> required <code>shared</code> <code>bool | None</code> <p>The sharedness of the item. If <code>None</code>, the sharedness of the item is the same as the sharedness of the wrapped item. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/item.py</code> <pre><code>def __init__(self, source: Item[T], shared: bool | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        source (Item[T]): The item to wrap and cache.\n        shared (bool | None, optional): The sharedness of the item. If `None`, the\n            sharedness of the item is the same as the sharedness of the wrapped item.\n            Defaults to None.\n    \"\"\"\n    self._source = source\n    self._cache = None\n    self._shared = shared\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item","title":"<code>Item</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Pipewine items. An item holds a reference to a single, serializable unit of data of parameterized type <code>T</code>.</p> <p>It provides methods to access the data, the way it is parsed, and whether it is considered shared or not.</p> <p>Item instances are immutable, all methods that modify the item return instead a new object with the desired changes, applying no in-place modifications to the original object.</p> <p>Every subclass must implement the <code>_get</code>, <code>_get_parser</code>, <code>_is_shared</code> and <code>with_sharedness</code> methods to provide the item's functionality.</p> Source code in <code>pipewine/item.py</code> <pre><code>class Item[T: Any](ABC):\n    \"\"\"Base class for all Pipewine items. An item holds a reference to a single,\n    serializable unit of data of parameterized type `T`.\n\n    It provides methods to access the data, the way it is parsed, and whether it is\n    considered shared or not.\n\n    Item instances are immutable, all methods that modify the item return instead a new\n    object with the desired changes, applying no in-place modifications to the\n    original object.\n\n    Every subclass must implement the `_get`, `_get_parser`, `_is_shared` and\n    `with_sharedness` methods to provide the item's functionality.\n    \"\"\"\n\n    @abstractmethod\n    def _get(self) -&gt; T:\n        \"\"\"Return the data held by the item.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_parser(self) -&gt; Parser[T]:\n        \"\"\"Return the parser used to parse the data.\"\"\"\n        pass\n\n    @abstractmethod\n    def _is_shared(self) -&gt; bool:\n        \"\"\"Return whether the item is shared or not.\"\"\"\n        pass\n\n    @property\n    def parser(self) -&gt; Parser[T]:\n        \"\"\"Return the parser used to parse the data.\"\"\"\n        return self._get_parser()\n\n    @property\n    def is_shared(self) -&gt; bool:\n        \"\"\"Return whether the item is shared or not.\"\"\"\n        return self._is_shared()\n\n    def with_value(self, value: T) -&gt; \"MemoryItem[T]\":\n        \"\"\"Change the value referenced by the item, returning a new item with the new\n        value. The returned item will always be an instance of `MemoryItem`, as it is\n        the type of item that holds references to values stored directly in memory.\n\n        Args:\n            value (T): New value to reference.\n\n        Returns:\n            MemoryItem[T]: New item with the new value.\n        \"\"\"\n        return MemoryItem(value, self._get_parser(), shared=self.is_shared)\n\n    def with_parser(self, parser: Parser[T]) -&gt; \"MemoryItem[T]\":\n        \"\"\"Change the parser used to parse the data, returning a new item with the new\n        parser. The returned item will always be an instance of `MemoryItem`, because\n        changing the parser requires the data to be loaded first and stored in memory.\n\n        Args:\n            parser (Parser[T]): New parser to use.\n\n        Returns:\n            MemoryItem[T]: New item with the new parser.\n        \"\"\"\n        return MemoryItem(self(), parser, shared=self.is_shared)\n\n    @abstractmethod\n    def with_sharedness(self, shared: bool) -&gt; Self:\n        \"\"\"Change the sharedness of the item, returning a new item with the new\n        sharedness. Changing the sharedness of an item does not require the data to be\n        loaded, so the returned item will always be of the same type as the original.\n\n        Args:\n            shared (bool): New sharedness to set.\n\n        Returns:\n            Self: New item with the new sharedness.\n        \"\"\"\n        pass\n\n    def __call__(self) -&gt; T:\n        \"\"\"Return the data held by the item.\"\"\"\n        return self._get()\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.is_shared","title":"<code>is_shared</code>  <code>property</code>","text":"<p>Return whether the item is shared or not.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.parser","title":"<code>parser</code>  <code>property</code>","text":"<p>Return the parser used to parse the data.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.__call__","title":"<code>__call__()</code>","text":"<p>Return the data held by the item.</p> Source code in <code>pipewine/item.py</code> <pre><code>def __call__(self) -&gt; T:\n    \"\"\"Return the data held by the item.\"\"\"\n    return self._get()\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item._get","title":"<code>_get()</code>  <code>abstractmethod</code>","text":"<p>Return the data held by the item.</p> Source code in <code>pipewine/item.py</code> <pre><code>@abstractmethod\ndef _get(self) -&gt; T:\n    \"\"\"Return the data held by the item.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item._get_parser","title":"<code>_get_parser()</code>  <code>abstractmethod</code>","text":"<p>Return the parser used to parse the data.</p> Source code in <code>pipewine/item.py</code> <pre><code>@abstractmethod\ndef _get_parser(self) -&gt; Parser[T]:\n    \"\"\"Return the parser used to parse the data.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item._is_shared","title":"<code>_is_shared()</code>  <code>abstractmethod</code>","text":"<p>Return whether the item is shared or not.</p> Source code in <code>pipewine/item.py</code> <pre><code>@abstractmethod\ndef _is_shared(self) -&gt; bool:\n    \"\"\"Return whether the item is shared or not.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.with_parser","title":"<code>with_parser(parser)</code>","text":"<p>Change the parser used to parse the data, returning a new item with the new parser. The returned item will always be an instance of <code>MemoryItem</code>, because changing the parser requires the data to be loaded first and stored in memory.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>Parser[T]</code> <p>New parser to use.</p> required <p>Returns:</p> Type Description <code>MemoryItem[T]</code> <p>MemoryItem[T]: New item with the new parser.</p> Source code in <code>pipewine/item.py</code> <pre><code>def with_parser(self, parser: Parser[T]) -&gt; \"MemoryItem[T]\":\n    \"\"\"Change the parser used to parse the data, returning a new item with the new\n    parser. The returned item will always be an instance of `MemoryItem`, because\n    changing the parser requires the data to be loaded first and stored in memory.\n\n    Args:\n        parser (Parser[T]): New parser to use.\n\n    Returns:\n        MemoryItem[T]: New item with the new parser.\n    \"\"\"\n    return MemoryItem(self(), parser, shared=self.is_shared)\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.with_sharedness","title":"<code>with_sharedness(shared)</code>  <code>abstractmethod</code>","text":"<p>Change the sharedness of the item, returning a new item with the new sharedness. Changing the sharedness of an item does not require the data to be loaded, so the returned item will always be of the same type as the original.</p> <p>Parameters:</p> Name Type Description Default <code>shared</code> <code>bool</code> <p>New sharedness to set.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>New item with the new sharedness.</p> Source code in <code>pipewine/item.py</code> <pre><code>@abstractmethod\ndef with_sharedness(self, shared: bool) -&gt; Self:\n    \"\"\"Change the sharedness of the item, returning a new item with the new\n    sharedness. Changing the sharedness of an item does not require the data to be\n    loaded, so the returned item will always be of the same type as the original.\n\n    Args:\n        shared (bool): New sharedness to set.\n\n    Returns:\n        Self: New item with the new sharedness.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.Item.with_value","title":"<code>with_value(value)</code>","text":"<p>Change the value referenced by the item, returning a new item with the new value. The returned item will always be an instance of <code>MemoryItem</code>, as it is the type of item that holds references to values stored directly in memory.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>T</code> <p>New value to reference.</p> required <p>Returns:</p> Type Description <code>MemoryItem[T]</code> <p>MemoryItem[T]: New item with the new value.</p> Source code in <code>pipewine/item.py</code> <pre><code>def with_value(self, value: T) -&gt; \"MemoryItem[T]\":\n    \"\"\"Change the value referenced by the item, returning a new item with the new\n    value. The returned item will always be an instance of `MemoryItem`, as it is\n    the type of item that holds references to values stored directly in memory.\n\n    Args:\n        value (T): New value to reference.\n\n    Returns:\n        MemoryItem[T]: New item with the new value.\n    \"\"\"\n    return MemoryItem(value, self._get_parser(), shared=self.is_shared)\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.MemoryItem","title":"<code>MemoryItem</code>","text":"<p>               Bases: <code>Item[T]</code></p> <p>A <code>MemoryItem</code> is an <code>Item</code> that holds a reference to a value stored directly in memory. It is the most common type of item, as it is used to represent data that is already loaded and parsed, typically used to store results of computations or intermediate data.</p> Source code in <code>pipewine/item.py</code> <pre><code>class MemoryItem[T: Any](Item[T]):\n    \"\"\"A `MemoryItem` is an `Item` that holds a reference to a value stored directly in\n    memory. It is the most common type of item, as it is used to represent data that is\n    already loaded and parsed, typically used to store results of computations or\n    intermediate data.\n    \"\"\"\n\n    def __init__(self, value: T, parser: Parser[T], shared: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            value (T): The value associated with the item.\n            parser (Parser[T]): The parser used to eventually serialize the value in\n                case the item is later stored in a file.\n            shared (bool, optional): The sharedness of the item. Defaults to False.\n        \"\"\"\n        self._value = value\n        self._parser = parser\n        self._shared = shared\n\n    def _get(self) -&gt; T:\n        return self._value\n\n    def _get_parser(self) -&gt; Parser[T]:\n        return self._parser\n\n    def _is_shared(self) -&gt; bool:\n        return self._shared\n\n    def with_sharedness(self, shared: bool) -&gt; Self:\n        return type(self)(self._value, self._parser, shared=shared)\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.MemoryItem.__init__","title":"<code>__init__(value, parser, shared=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>value</code> <code>T</code> <p>The value associated with the item.</p> required <code>parser</code> <code>Parser[T]</code> <p>The parser used to eventually serialize the value in case the item is later stored in a file.</p> required <code>shared</code> <code>bool</code> <p>The sharedness of the item. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/item.py</code> <pre><code>def __init__(self, value: T, parser: Parser[T], shared: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        value (T): The value associated with the item.\n        parser (Parser[T]): The parser used to eventually serialize the value in\n            case the item is later stored in a file.\n        shared (bool, optional): The sharedness of the item. Defaults to False.\n    \"\"\"\n    self._value = value\n    self._parser = parser\n    self._shared = shared\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.StoredItem","title":"<code>StoredItem</code>","text":"<p>               Bases: <code>Item[T]</code></p> <p>A <code>StoredItem</code> is an <code>Item</code> that reads data from an external source, such as a file or a database. It holds a reference to a <code>Reader</code> and a <code>Parser</code> that are used to read and parse the data when it is requested.</p> Source code in <code>pipewine/item.py</code> <pre><code>class StoredItem[T: Any](Item[T]):\n    \"\"\"A `StoredItem` is an `Item` that reads data from an external source, such as a\n    file or a database. It holds a reference to a `Reader` and a `Parser` that are used\n    to read and parse the data when it is requested.\n    \"\"\"\n\n    def __init__(self, reader: Reader, parser: Parser[T], shared: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            reader (Reader): The reader used to read the data from the external source.\n            parser (Parser[T]): The parser used to parse the data read by the reader and\n                to eventually serialize it in case the item is later stored somewhere\n                else.\n            shared (bool, optional): The sharedness of the item. Defaults to False.\n        \"\"\"\n        self._reader = reader\n        self._parser = parser\n        self._shared = shared\n\n    def _get(self) -&gt; T:\n        return self._parser.parse(self._reader.read())\n\n    def _get_parser(self) -&gt; Parser[T]:\n        return self._parser\n\n    def _is_shared(self) -&gt; bool:\n        return self._shared\n\n    def with_sharedness(self, shared: bool) -&gt; Self:\n        return type(self)(self._reader, self._parser, shared=shared)\n\n    @property\n    def reader(self) -&gt; Reader:\n        \"\"\"Return the reader used to read the data from the external source.\"\"\"\n        return self._reader\n</code></pre>"},{"location":"autoapi/pipewine/item/#pipewine.item.StoredItem.reader","title":"<code>reader</code>  <code>property</code>","text":"<p>Return the reader used to read the data from the external source.</p>"},{"location":"autoapi/pipewine/item/#pipewine.item.StoredItem.__init__","title":"<code>__init__(reader, parser, shared=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>reader</code> <code>Reader</code> <p>The reader used to read the data from the external source.</p> required <code>parser</code> <code>Parser[T]</code> <p>The parser used to parse the data read by the reader and to eventually serialize it in case the item is later stored somewhere else.</p> required <code>shared</code> <code>bool</code> <p>The sharedness of the item. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/item.py</code> <pre><code>def __init__(self, reader: Reader, parser: Parser[T], shared: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        reader (Reader): The reader used to read the data from the external source.\n        parser (Parser[T]): The parser used to parse the data read by the reader and\n            to eventually serialize it in case the item is later stored somewhere\n            else.\n        shared (bool, optional): The sharedness of the item. Defaults to False.\n    \"\"\"\n    self._reader = reader\n    self._parser = parser\n    self._shared = shared\n</code></pre>"},{"location":"autoapi/pipewine/reader/","title":"reader","text":"<p><code>Reader</code> base class and implementations used by Pipewine <code>Item</code> instances to read  data.</p>"},{"location":"autoapi/pipewine/reader/#pipewine.reader.LocalFileReader","title":"<code>LocalFileReader</code>","text":"<p>               Bases: <code>Reader</code></p> <p>Reader implementation that reads data from a local file.</p> Source code in <code>pipewine/reader.py</code> <pre><code>class LocalFileReader(Reader):\n    \"\"\"Reader implementation that reads data from a local file.\"\"\"\n\n    def __init__(self, path: Path):\n        \"\"\"\n        Args:\n            path (Path): Path to the file to read.\n        \"\"\"\n        self._path = path\n\n    def read(self) -&gt; bytes:\n        with open(self._path, \"rb\") as fp:\n            result = fp.read()\n        return result\n\n    @property\n    def path(self) -&gt; Path:\n        \"\"\"Return the path to the file being read.\"\"\"\n        return self._path\n</code></pre>"},{"location":"autoapi/pipewine/reader/#pipewine.reader.LocalFileReader.path","title":"<code>path</code>  <code>property</code>","text":"<p>Return the path to the file being read.</p>"},{"location":"autoapi/pipewine/reader/#pipewine.reader.LocalFileReader.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file to read.</p> required Source code in <code>pipewine/reader.py</code> <pre><code>def __init__(self, path: Path):\n    \"\"\"\n    Args:\n        path (Path): Path to the file to read.\n    \"\"\"\n    self._path = path\n</code></pre>"},{"location":"autoapi/pipewine/reader/#pipewine.reader.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Pipewine readers. A reader is an object that reads data from a source and returns it as a byte string.</p> <p>All subclasses must implement the <code>read</code> method.</p> Source code in <code>pipewine/reader.py</code> <pre><code>class Reader(ABC):\n    \"\"\"Base class for all Pipewine readers. A reader is an object that reads data from\n    a source and returns it as a byte string.\n\n    All subclasses must implement the `read` method.\n    \"\"\"\n\n    @abstractmethod\n    def read(self) -&gt; bytes:\n        \"\"\"Read data from the source and return it as a byte string.\"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/reader/#pipewine.reader.Reader.read","title":"<code>read()</code>  <code>abstractmethod</code>","text":"<p>Read data from the source and return it as a byte string.</p> Source code in <code>pipewine/reader.py</code> <pre><code>@abstractmethod\ndef read(self) -&gt; bytes:\n    \"\"\"Read data from the source and return it as a byte string.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sample/","title":"sample","text":"<p><code>Sample</code> base class and its implementations to represent data samples in Pipewine.</p>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample","title":"<code>Sample</code>","text":"<p>               Bases: <code>ABC</code>, <code>Mapping[str, Item]</code></p> <p>Base class for all Pipewine samples. A sample is a collection of items, each of which references a single, serializable unit of data.</p> <p>Samples are immutable mappings from item keys (strings) to item instances. They provide methods to access the items, keys, and values, as well as to create new samples with different keys and values.</p> <p>Subclasses must implement the <code>_get_item</code>, <code>_size</code>, <code>keys</code>, and <code>with_items</code> methods.</p> Source code in <code>pipewine/sample.py</code> <pre><code>class Sample(ABC, Mapping[str, Item]):\n    \"\"\"Base class for all Pipewine samples. A sample is a collection of items, each of\n    which references a single, serializable unit of data.\n\n    Samples are immutable mappings from item keys (strings) to item instances. They\n    provide methods to access the items, keys, and values, as well as to create new\n    samples with different keys and values.\n\n    Subclasses must implement the `_get_item`, `_size`, `keys`, and `with_items`\n    methods.\n    \"\"\"\n\n    @abstractmethod\n    def _get_item(self, key: str) -&gt; Item:\n        \"\"\"Return the item corresponding to the given key.\n\n        Args:\n            key (str): Key of the item to return.\n\n        Returns:\n            Item: The item object.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _size(self) -&gt; int:\n        \"\"\"Return the number of items in the sample.\"\"\"\n        pass\n\n    @abstractmethod\n    def keys(self) -&gt; KeysView[str]:\n        \"\"\"Return a keys view of the items in the sample.\"\"\"\n        pass\n\n    @abstractmethod\n    def with_items(self, **items: Item) -&gt; Self:\n        \"\"\"Return a new sample with the given items added or replaced. The new sample\n        is guaranteed to be of the same type as the original sample.\n\n        Args:\n            **items (Item): Items to add or replace in the sample.\n\n        Returns:\n            Self: The new sample with the given items added or replaced.\n        \"\"\"\n        pass\n\n    def with_item(self, key: str, item: Item) -&gt; Self:\n        \"\"\"Return a new sample with the given item added or replaced. The new sample is\n        guaranteed to be of the same type as the original sample.\n\n        Args:\n            key (str): Key of the item to add or replace.\n            item (Item): Item to add or replace.\n\n        Returns:\n            Self: The new sample with the given item added or replaced.\n        \"\"\"\n        return self.with_items(**{key: item})\n\n    def with_values(self, **values: Any) -&gt; Self:\n        \"\"\"Return a new sample with the values of the items with the given keys changed\n        to the given values. The new sample is guaranteed to be of the same type as the\n        original sample.\n\n        This method differs from `with_items` in that it only changes the values of the\n        items, preserving their type and parser.\n\n        Returns:\n            Self: The new sample with the modified values.\n        \"\"\"\n        dict_values = {k: self._get_item(k).with_value(v) for k, v in values.items()}\n        return self.with_items(**dict_values)\n\n    def with_value(self, key: str, value: Any) -&gt; Self:\n        \"\"\"Return a new sample with the value of the item with the given key changed to\n        the given value. The new sample is guaranteed to be of the same type as the\n        original sample.\n\n        This method differs from `with_item` in that it only changes the value of the\n        item, preserving its type and parser.\n\n        Args:\n            key (str): Key of the item to change.\n            value (Any): New value to set.\n\n        Returns:\n            Self: The new sample with the modified value.\n        \"\"\"\n        return self.with_values(**{key: value})\n\n    def without(self, *keys: str) -&gt; \"TypelessSample\":\n        \"\"\"Return a new sample with the items with the given keys removed. The new\n        sample will always be an instance of `TypelessSample`, because removing an item\n        always implies that the resulting sample cannot be of the same type as the\n        original sample.\n\n        Args:\n            *keys (str): Keys of the items to remove.\n\n        Returns:\n            TypelessSample: The new sample with the items removed.\n        \"\"\"\n        items = {k: self._get_item(k) for k in self.keys() if k not in keys}\n        return TypelessSample(**items)\n\n    def with_only(self, *keys: str) -&gt; \"TypelessSample\":\n        \"\"\"Return a new sample with only the items with the given keys. The new sample\n        will always be an instance of `TypelessSample`, because keeping only a subset of\n        the items always implies that the resulting sample cannot be of the same type as\n        the original sample.\n\n        Args:\n            *keys (str): Keys of the items to keep.\n\n        Returns:\n            TypelessSample: The new sample with only the items with the given keys.\n        \"\"\"\n        items = {k: self._get_item(k) for k in self.keys() if k in keys}\n        return TypelessSample(**items)\n\n    def remap(\n        self, fromto: Mapping[str, str], exclude: bool = False\n    ) -&gt; \"TypelessSample\":\n        \"\"\"Return a new sample with the items remapped according to the given mapping.\n        The new sample will always be an instance of `TypelessSample`, because remapping\n        the items always implies that the resulting sample cannot be of the same type as\n        the original sample.\n\n        Args:\n            fromto (Mapping[str, str]): Mapping from old keys to new keys.\n            exclude (bool): Whether to exclude items not in the mapping.\n\n        Returns:\n            TypelessSample: The new sample with the items remapped.\n        \"\"\"\n        if exclude:\n            items = {k: self._get_item(k) for k in self.keys() if k in fromto}\n        else:\n            items = {k: self._get_item(k) for k in self.keys()}\n        for k_from, k_to in fromto.items():\n            if k_from in items:\n                items[k_to] = items.pop(k_from)\n        return TypelessSample(**items)\n\n    def typeless(self) -&gt; \"TypelessSample\":\n        \"\"\"Return a new sample with the same items as the original sample, but as an\n        instance of `TypelessSample`. This method is useful when needing to explicitly\n        convert a sample to a typeless sample.\n\n        E.g. when needing to add a new item to a typed sample, it is necessary to\n        drop the type information and convert it to a typeless sample first, otherwise\n        the `with_items` method (that returns a sample with the same type as the\n        original) would not allow adding items with keys not present in the typed\n        sample definition.\n        \"\"\"\n        return TypelessSample(**self)\n\n    def __getitem__(self, key: str) -&gt; Item:\n        return self._get_item(key)\n\n    def __iter__(self) -&gt; Iterator[str]:\n        return iter(self.keys())\n\n    def __len__(self) -&gt; int:\n        return self._size()\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample._get_item","title":"<code>_get_item(key)</code>  <code>abstractmethod</code>","text":"<p>Return the item corresponding to the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of the item to return.</p> required <p>Returns:</p> Name Type Description <code>Item</code> <code>Item</code> <p>The item object.</p> Source code in <code>pipewine/sample.py</code> <pre><code>@abstractmethod\ndef _get_item(self, key: str) -&gt; Item:\n    \"\"\"Return the item corresponding to the given key.\n\n    Args:\n        key (str): Key of the item to return.\n\n    Returns:\n        Item: The item object.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample._size","title":"<code>_size()</code>  <code>abstractmethod</code>","text":"<p>Return the number of items in the sample.</p> Source code in <code>pipewine/sample.py</code> <pre><code>@abstractmethod\ndef _size(self) -&gt; int:\n    \"\"\"Return the number of items in the sample.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.keys","title":"<code>keys()</code>  <code>abstractmethod</code>","text":"<p>Return a keys view of the items in the sample.</p> Source code in <code>pipewine/sample.py</code> <pre><code>@abstractmethod\ndef keys(self) -&gt; KeysView[str]:\n    \"\"\"Return a keys view of the items in the sample.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.remap","title":"<code>remap(fromto, exclude=False)</code>","text":"<p>Return a new sample with the items remapped according to the given mapping. The new sample will always be an instance of <code>TypelessSample</code>, because remapping the items always implies that the resulting sample cannot be of the same type as the original sample.</p> <p>Parameters:</p> Name Type Description Default <code>fromto</code> <code>Mapping[str, str]</code> <p>Mapping from old keys to new keys.</p> required <code>exclude</code> <code>bool</code> <p>Whether to exclude items not in the mapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>TypelessSample</code> <code>TypelessSample</code> <p>The new sample with the items remapped.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def remap(\n    self, fromto: Mapping[str, str], exclude: bool = False\n) -&gt; \"TypelessSample\":\n    \"\"\"Return a new sample with the items remapped according to the given mapping.\n    The new sample will always be an instance of `TypelessSample`, because remapping\n    the items always implies that the resulting sample cannot be of the same type as\n    the original sample.\n\n    Args:\n        fromto (Mapping[str, str]): Mapping from old keys to new keys.\n        exclude (bool): Whether to exclude items not in the mapping.\n\n    Returns:\n        TypelessSample: The new sample with the items remapped.\n    \"\"\"\n    if exclude:\n        items = {k: self._get_item(k) for k in self.keys() if k in fromto}\n    else:\n        items = {k: self._get_item(k) for k in self.keys()}\n    for k_from, k_to in fromto.items():\n        if k_from in items:\n            items[k_to] = items.pop(k_from)\n    return TypelessSample(**items)\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.typeless","title":"<code>typeless()</code>","text":"<p>Return a new sample with the same items as the original sample, but as an instance of <code>TypelessSample</code>. This method is useful when needing to explicitly convert a sample to a typeless sample.</p> <p>E.g. when needing to add a new item to a typed sample, it is necessary to drop the type information and convert it to a typeless sample first, otherwise the <code>with_items</code> method (that returns a sample with the same type as the original) would not allow adding items with keys not present in the typed sample definition.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def typeless(self) -&gt; \"TypelessSample\":\n    \"\"\"Return a new sample with the same items as the original sample, but as an\n    instance of `TypelessSample`. This method is useful when needing to explicitly\n    convert a sample to a typeless sample.\n\n    E.g. when needing to add a new item to a typed sample, it is necessary to\n    drop the type information and convert it to a typeless sample first, otherwise\n    the `with_items` method (that returns a sample with the same type as the\n    original) would not allow adding items with keys not present in the typed\n    sample definition.\n    \"\"\"\n    return TypelessSample(**self)\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.with_item","title":"<code>with_item(key, item)</code>","text":"<p>Return a new sample with the given item added or replaced. The new sample is guaranteed to be of the same type as the original sample.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of the item to add or replace.</p> required <code>item</code> <code>Item</code> <p>Item to add or replace.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The new sample with the given item added or replaced.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def with_item(self, key: str, item: Item) -&gt; Self:\n    \"\"\"Return a new sample with the given item added or replaced. The new sample is\n    guaranteed to be of the same type as the original sample.\n\n    Args:\n        key (str): Key of the item to add or replace.\n        item (Item): Item to add or replace.\n\n    Returns:\n        Self: The new sample with the given item added or replaced.\n    \"\"\"\n    return self.with_items(**{key: item})\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.with_items","title":"<code>with_items(**items)</code>  <code>abstractmethod</code>","text":"<p>Return a new sample with the given items added or replaced. The new sample is guaranteed to be of the same type as the original sample.</p> <p>Parameters:</p> Name Type Description Default <code>**items</code> <code>Item</code> <p>Items to add or replace in the sample.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The new sample with the given items added or replaced.</p> Source code in <code>pipewine/sample.py</code> <pre><code>@abstractmethod\ndef with_items(self, **items: Item) -&gt; Self:\n    \"\"\"Return a new sample with the given items added or replaced. The new sample\n    is guaranteed to be of the same type as the original sample.\n\n    Args:\n        **items (Item): Items to add or replace in the sample.\n\n    Returns:\n        Self: The new sample with the given items added or replaced.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.with_only","title":"<code>with_only(*keys)</code>","text":"<p>Return a new sample with only the items with the given keys. The new sample will always be an instance of <code>TypelessSample</code>, because keeping only a subset of the items always implies that the resulting sample cannot be of the same type as the original sample.</p> <p>Parameters:</p> Name Type Description Default <code>*keys</code> <code>str</code> <p>Keys of the items to keep.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>TypelessSample</code> <code>TypelessSample</code> <p>The new sample with only the items with the given keys.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def with_only(self, *keys: str) -&gt; \"TypelessSample\":\n    \"\"\"Return a new sample with only the items with the given keys. The new sample\n    will always be an instance of `TypelessSample`, because keeping only a subset of\n    the items always implies that the resulting sample cannot be of the same type as\n    the original sample.\n\n    Args:\n        *keys (str): Keys of the items to keep.\n\n    Returns:\n        TypelessSample: The new sample with only the items with the given keys.\n    \"\"\"\n    items = {k: self._get_item(k) for k in self.keys() if k in keys}\n    return TypelessSample(**items)\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.with_value","title":"<code>with_value(key, value)</code>","text":"<p>Return a new sample with the value of the item with the given key changed to the given value. The new sample is guaranteed to be of the same type as the original sample.</p> <p>This method differs from <code>with_item</code> in that it only changes the value of the item, preserving its type and parser.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key of the item to change.</p> required <code>value</code> <code>Any</code> <p>New value to set.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The new sample with the modified value.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def with_value(self, key: str, value: Any) -&gt; Self:\n    \"\"\"Return a new sample with the value of the item with the given key changed to\n    the given value. The new sample is guaranteed to be of the same type as the\n    original sample.\n\n    This method differs from `with_item` in that it only changes the value of the\n    item, preserving its type and parser.\n\n    Args:\n        key (str): Key of the item to change.\n        value (Any): New value to set.\n\n    Returns:\n        Self: The new sample with the modified value.\n    \"\"\"\n    return self.with_values(**{key: value})\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.with_values","title":"<code>with_values(**values)</code>","text":"<p>Return a new sample with the values of the items with the given keys changed to the given values. The new sample is guaranteed to be of the same type as the original sample.</p> <p>This method differs from <code>with_items</code> in that it only changes the values of the items, preserving their type and parser.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The new sample with the modified values.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def with_values(self, **values: Any) -&gt; Self:\n    \"\"\"Return a new sample with the values of the items with the given keys changed\n    to the given values. The new sample is guaranteed to be of the same type as the\n    original sample.\n\n    This method differs from `with_items` in that it only changes the values of the\n    items, preserving their type and parser.\n\n    Returns:\n        Self: The new sample with the modified values.\n    \"\"\"\n    dict_values = {k: self._get_item(k).with_value(v) for k, v in values.items()}\n    return self.with_items(**dict_values)\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.Sample.without","title":"<code>without(*keys)</code>","text":"<p>Return a new sample with the items with the given keys removed. The new sample will always be an instance of <code>TypelessSample</code>, because removing an item always implies that the resulting sample cannot be of the same type as the original sample.</p> <p>Parameters:</p> Name Type Description Default <code>*keys</code> <code>str</code> <p>Keys of the items to remove.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>TypelessSample</code> <code>TypelessSample</code> <p>The new sample with the items removed.</p> Source code in <code>pipewine/sample.py</code> <pre><code>def without(self, *keys: str) -&gt; \"TypelessSample\":\n    \"\"\"Return a new sample with the items with the given keys removed. The new\n    sample will always be an instance of `TypelessSample`, because removing an item\n    always implies that the resulting sample cannot be of the same type as the\n    original sample.\n\n    Args:\n        *keys (str): Keys of the items to remove.\n\n    Returns:\n        TypelessSample: The new sample with the items removed.\n    \"\"\"\n    items = {k: self._get_item(k) for k in self.keys() if k not in keys}\n    return TypelessSample(**items)\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.TypedSample","title":"<code>TypedSample</code>","text":"<p>               Bases: <code>Bundle[Item]</code>, <code>Sample</code></p> <p>A <code>TypedSample</code> is a <code>Sample</code> that has a specific type and contains a mapping of items with specific keys.</p> <p>It is useful when needing to define a specific structure for the data samples in a pipeline, with known keys and types for the items. This allows for type checking and auto-completion in IDEs, and makes it easier to reason about the data flow in the pipeline.</p> <p><code>TypedSample</code> is a subclass of <code>Bundle</code>, allowing subclasses to define the items as plain pyhton dataclasses, with type hints for the item values. This makes it easy to define the items and access their values, and provides a convenient way to create new samples with different keys and values.</p> <p>Every <code>TypedSample</code> object can be drop the type information and be converted to a <code>TypelessSample</code> object using the <code>typeless</code> method if needed.</p> <p>Be careful when defining <code>TypedSample</code> subclasses, as the keys of the items might conflict with the methods and attributes of the base class. Avoid using any of the following when defining <code>TypedSample</code> subclasses:</p> <ul> <li>Any key starting with underscore (<code>_</code>).</li> <li><code>keys</code>, <code>values</code>, <code>items</code>, <code>get</code> inherited from the <code>Mapping</code> protocol.</li> <li><code>as_dict</code> and <code>from_dict</code> inherited from the <code>Bundle</code> class.</li> <li><code>with_item</code>, <code>with_items</code>, <code>with_value</code>, <code>with_values</code>, <code>without</code>, <code>with_only</code>,     <code>remap</code>, <code>typeless</code> inherited from the <code>Sample</code> class.</li> </ul> Source code in <code>pipewine/sample.py</code> <pre><code>class TypedSample(Bundle[Item], Sample):\n    \"\"\"A `TypedSample` is a `Sample` that has a specific type and contains a mapping of\n    items with specific keys.\n\n    It is useful when needing to define a specific structure for the data samples in a\n    pipeline, with known keys and types for the items. This allows for type checking and\n    auto-completion in IDEs, and makes it easier to reason about the data flow in the\n    pipeline.\n\n    `TypedSample` is a subclass of `Bundle`, allowing subclasses to define the items as\n    plain pyhton dataclasses, with type hints for the item values. This makes it easy to\n    define the items and access their values, and provides a convenient way to create\n    new samples with different keys and values.\n\n    Every `TypedSample` object can be drop the type information and be converted to a\n    `TypelessSample` object using the `typeless` method if needed.\n\n    Be careful when defining `TypedSample` subclasses, as the keys of the items might\n    conflict with the methods and attributes of the base class. Avoid using any of the\n    following when defining `TypedSample` subclasses:\n\n    - Any key starting with underscore (`_`).\n    - `keys`, `values`, `items`, `get` inherited from the `Mapping` protocol.\n    - `as_dict` and `from_dict` inherited from the `Bundle` class.\n    - `with_item`, `with_items`, `with_value`, `with_values`, `without`, `with_only`,\n        `remap`, `typeless` inherited from the `Sample` class.\n    \"\"\"\n\n    def _get_item(self, key: str) -&gt; Item:\n        return getattr(self, key)\n\n    def _size(self) -&gt; int:\n        return len(self.as_dict())\n\n    def keys(self) -&gt; KeysView[str]:\n        return self.as_dict().keys()\n\n    def with_items(self, **items: Item) -&gt; Self:\n        return type(self).from_dict(**{**self.__dict__, **items})\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.TypelessSample","title":"<code>TypelessSample</code>","text":"<p>               Bases: <code>Sample</code></p> <p>A <code>TypelessSample</code> is a <code>Sample</code> that does not have a specific type and can contain an arbitrary mapping of items with arbitrary keys.</p> <p>It is useful when needing to manipulate samples for which the keys and types of the items change dynamically and are not known at dev time, or when needing to load data quickly without having to define a specific sample type for every step of the pipeline.</p> Source code in <code>pipewine/sample.py</code> <pre><code>class TypelessSample(Sample):\n    \"\"\"A `TypelessSample` is a `Sample` that does not have a specific type and can\n    contain an arbitrary mapping of items with arbitrary keys.\n\n    It is useful when needing to manipulate samples for which the keys and types of the\n    items change dynamically and are not known at dev time, or when needing to load\n    data quickly without having to define a specific sample type for every step of the\n    pipeline.\n    \"\"\"\n\n    def __init__(self, **items: Item) -&gt; None:\n        \"\"\"\n        Args:\n            **items (Item): Items to include in the sample.\n        \"\"\"\n        super().__init__()\n        self._items = items\n\n    def _get_item(self, key: str) -&gt; Item:\n        return self._items[key]\n\n    def _size(self) -&gt; int:\n        return len(self._items)\n\n    def keys(self) -&gt; KeysView[str]:\n        return self._items.keys()\n\n    def with_items(self, **items: Item) -&gt; Self:\n        return self.__class__(**{**self._items, **items})\n</code></pre>"},{"location":"autoapi/pipewine/sample/#pipewine.sample.TypelessSample.__init__","title":"<code>__init__(**items)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>**items</code> <code>Item</code> <p>Items to include in the sample.</p> <code>{}</code> Source code in <code>pipewine/sample.py</code> <pre><code>def __init__(self, **items: Item) -&gt; None:\n    \"\"\"\n    Args:\n        **items (Item): Items to include in the sample.\n    \"\"\"\n    super().__init__()\n    self._items = items\n</code></pre>"},{"location":"autoapi/pipewine/cli/","title":"cli","text":"<p>Package for Pipewine CLI and all related components.</p>"},{"location":"autoapi/pipewine/cli/extension/","title":"extension","text":"<p>Dynamic imports of Python modules from files, class paths, or source code.</p>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension._add_to_sys_path","title":"<code>_add_to_sys_path</code>","text":"<p>               Bases: <code>ContextDecorator</code></p> <p>add_to_sys_path context decorator temporarily adds a path to sys.path</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>class _add_to_sys_path(ContextDecorator):\n    \"\"\"add_to_sys_path context decorator temporarily adds a path to sys.path\"\"\"\n\n    def __init__(self, path: str) -&gt; None:\n        self._new_cwd = path\n\n    def __enter__(self) -&gt; None:\n        sys.path.insert(0, self._new_cwd)\n\n    def __exit__(self, exc_type, exc_value, traceback) -&gt; None:\n        sys.path.pop(0)\n</code></pre>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension._get_module_lock","title":"<code>_get_module_lock(name)</code>","text":"<p>Get or create the module lock for a given module name.</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>def _get_module_lock(name):  # pragma: no cover\n    \"\"\"Get or create the module lock for a given module name.\"\"\"\n    # Acquire/release internally the global import lock to protect _module_locks.\n    with _imp_lock:\n        try:\n            lock = _module_locks[name]()\n        except KeyError:\n            lock = None\n\n        if lock is None:\n            lock = threading.Lock()\n\n            def cb(ref, name=name):\n                with _imp_lock:\n                    # bpo-31070: Check if another thread created a new lock\n                    # after the previous lock was destroyed\n                    # but before the weakref callback was called.\n                    if _module_locks.get(name) is ref:\n                        del _module_locks[name]\n\n            _module_locks[name] = weakref.ref(lock, cb)\n\n        return lock\n</code></pre>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension._import_module_from_class_path","title":"<code>_import_module_from_class_path(module_class_path)</code>","text":"<p>Import a python module from a python class path.</p> <p>Parameters:</p> Name Type Description Default <code>module_class_path</code> <code>str</code> <p>the dot-separated path to the module class.</p> required <p>Returns:</p> Name Type Description <code>ModuleType</code> <code>ModuleType</code> <p>the imported module.</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>def _import_module_from_class_path(module_class_path: str) -&gt; ModuleType:\n    \"\"\"Import a python module from a python class path.\n\n    Args:\n        module_class_path (str): the dot-separated path to the module class.\n\n    Returns:\n        ModuleType: the imported module.\n    \"\"\"\n    return importlib.import_module(module_class_path)\n</code></pre>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension._import_module_from_code","title":"<code>_import_module_from_code(module_code)</code>","text":"<p>Dynamically imports a Python module from its source code.</p> <p>Parameters:</p> Name Type Description Default <code>module_code</code> <code>str</code> <p>the python source code of the module to import.</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>if the module cannot be imported.</p> <p>Returns:</p> Name Type Description <code>ModuleType</code> <code>ModuleType</code> <p>the imported module.</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>def _import_module_from_code(module_code: str) -&gt; ModuleType:\n    \"\"\"Dynamically imports a Python module from its source code.\n\n    Args:\n        module_code (str): the python source code of the module to import.\n\n    Raises:\n        ImportError: if the module cannot be imported.\n\n    Returns:\n        ModuleType: the imported module.\n    \"\"\"\n\n    hash_fn = hashlib.blake2b()\n    hash_fn.update(module_code.encode(\"utf-8\"))\n    name = hash_fn.hexdigest()\n\n    with _ModuleLockManager(name):\n        # check if the module is already imported\n        try:\n            spec = importlib.util.find_spec(name)\n        except Exception:  # pragma: no cover\n            spec = None\n\n        if spec is not None:\n            module = importlib.import_module(name)\n        else:\n            # create a new module from code\n            spec = importlib.util.spec_from_loader(name, loader=None)\n            if spec is None:  # pragma: no cover\n                raise ImportError(f\"Cannot create spec for module `{module_code}`\")\n            module = importlib.util.module_from_spec(spec)\n\n            # compile the code and put everything in the new module\n            exec(module_code, module.__dict__)\n\n            # add the module to sys.modules\n            sys.modules[name] = module\n\n        return module\n</code></pre>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension._import_module_from_file","title":"<code>_import_module_from_file(module_file_path, cwd=None)</code>","text":"<p>Import a python module from a file.</p> <p>Parameters:</p> Name Type Description Default <code>module_file_path</code> <code>Union[str, Path]</code> <p>the path to the <code>.py</code> module file.</p> required <code>cwd</code> <code>Optional[Path]</code> <p>the folder to use for relative module import. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the module cannot be imported.</p> <p>Returns:</p> Name Type Description <code>ModuleType</code> <code>ModuleType</code> <p>the imported module.</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>def _import_module_from_file(\n    module_file_path: str | Path, cwd: Path | None = None\n) -&gt; ModuleType:\n    \"\"\"Import a python module from a file.\n\n    Args:\n        module_file_path (Union[str, Path]): the path to the `.py` module file.\n        cwd (Optional[Path], optional): the folder to use for relative module import.\n            Defaults to None.\n\n    Raises:\n        ImportError: if the module cannot be imported.\n\n    Returns:\n        ModuleType: the imported module.\n    \"\"\"\n    module_path = Path(module_file_path)\n    if not module_path.is_absolute() and cwd is not None:\n        module_path = cwd / module_path\n\n    if not module_path.exists():\n        raise ModuleNotFoundError(f\"Module not found: {module_file_path}\")\n\n    with _add_to_sys_path(str(module_path.parent)):\n        return _import_module_from_class_path(module_path.stem)\n</code></pre>"},{"location":"autoapi/pipewine/cli/extension/#pipewine.cli.extension.import_module","title":"<code>import_module(module_file_or_class_path_or_code, cwd=None)</code>","text":"<p>Import a module from a file, a class path, or its source code.</p> <p>Parameters:</p> Name Type Description Default <code>module_file_or_class_path_or_code</code> <code>str</code> <p>the path to the module file, the python class path, or its python code.</p> required <code>cwd</code> <code>Optional[Path]</code> <p>the current working directory for relative imports. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the module cannot be imported.</p> <p>Returns:</p> Name Type Description <code>ModuleType</code> <code>ModuleType</code> <p>The imported module.</p> Source code in <code>pipewine/cli/extension.py</code> <pre><code>def import_module(\n    module_file_or_class_path_or_code: str | Path, cwd: Path | None = None\n) -&gt; ModuleType:\n    \"\"\"Import a module from a file, a class path, or its source code.\n\n    Args:\n        module_file_or_class_path_or_code (str): the path to the module file, the\n            python class path, or its python code.\n        cwd (Optional[Path], optional): the current working directory for relative\n            imports. Defaults to None.\n\n    Raises:\n        ImportError: If the module cannot be imported.\n\n    Returns:\n        ModuleType: The imported module.\n    \"\"\"\n    module_file_or_class_path_or_code = str(module_file_or_class_path_or_code)\n    err_msgs = [\"\"]\n\n    if module_file_or_class_path_or_code.endswith(\".py\"):\n        try:\n            return _import_module_from_file(module_file_or_class_path_or_code, cwd)\n        except Exception as e:\n            err_msgs.append(f\"from file: {e}\")\n    try:\n        return _import_module_from_class_path(module_file_or_class_path_or_code)\n    except Exception as e:\n        err_msgs.append(f\"from classpath: {e}\")\n\n    try:\n        return _import_module_from_code(module_file_or_class_path_or_code)\n    except Exception as e:\n        err_msgs.append(f\"from code: {e}\")\n\n    raise ImportError(\n        \"Cannot import:\\n\"\n        f\"  `{module_file_or_class_path_or_code}`\\n\"\n        \"Possible causes:\" + \"\\n  \".join(err_msgs)\n    )\n</code></pre>"},{"location":"autoapi/pipewine/cli/main/","title":"main","text":"<p>Main module for Pipewine CLI, containing the main entry point and the main CLI app.</p>"},{"location":"autoapi/pipewine/cli/main/#pipewine.cli.main.pipewine_app","title":"<code>pipewine_app = Typer(invoke_without_command=True, pretty_exceptions_enable=False, add_completion=False, no_args_is_help=True, callback=_main_callback)</code>  <code>module-attribute</code>","text":"<p>Typer app for the main Pipewine CLI.</p>"},{"location":"autoapi/pipewine/cli/main/#pipewine.cli.main.main","title":"<code>main()</code>","text":"<p>Pipewine CLI entry point.</p> Source code in <code>pipewine/cli/main.py</code> <pre><code>def main() -&gt; None:  # pragma: no cover\n    \"\"\"Pipewine CLI entry point.\"\"\"\n    command_names = [x.name for x in pipewine_app.registered_commands]\n    for i, token in enumerate(sys.argv):\n        if token in command_names:\n            break\n        if token in [\"-m\" or \"--module\"]:\n            import_module(sys.argv[i + 1])\n    pipewine_app()\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/","title":"mappers","text":"<p>CLI commands for dataset mappers.</p>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.map_app","title":"<code>map_app = Typer(callback=_op_callback, name='map', help='Run a pipewine dataset mapper.', invoke_without_command=True, no_args_is_help=True)</code>  <code>module-attribute</code>","text":"<p>Typer app for the Pipewine map CLI.</p>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.CacheMapper","title":"<code>CacheMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that replaces all items in a sample with <code>CachedItem</code> instances wrapping the original items.</p> Source code in <code>pipewine/mappers/cache.py</code> <pre><code>class CacheMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that replaces all items in a sample with `CachedItem` instances wrapping\n    the original items.\n    \"\"\"\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        return x.with_items(\n            **{\n                k: v if isinstance(v, CachedItem) else CachedItem(v)\n                for k, v in x.items()\n            }\n        )\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ComposeMapper","title":"<code>ComposeMapper</code>","text":"<p>               Bases: <code>Mapper[T_IN, T_OUT]</code></p> <p>Mapper that composes multiple mappers into a single mapper, calling each mapper in sequence, similar to function composition.</p> <p>When composing multiple mappers, the output type of each mapper must match the input type of the next mapper. This class is hinted in such a way that the type checker can infer the input and output types of the final composed mapper.</p> Source code in <code>pipewine/mappers/compose.py</code> <pre><code>class ComposeMapper[T_IN: Sample, T_OUT: Sample](Mapper[T_IN, T_OUT]):\n    \"\"\"Mapper that composes multiple mappers into a single mapper, calling each\n    mapper in sequence, similar to function composition.\n\n    When composing multiple mappers, the output type of each mapper must match the\n    input type of the next mapper. This class is hinted in such a way that the type\n    checker can infer the input and output types of the final composed mapper.\n    \"\"\"\n\n    def __init__(\n        self,\n        mappers: (\n            Mapper[T_IN, T_OUT]\n            | tuple[Mapper[T_IN, T_OUT]]\n            | tuple[Mapper[T_IN, A], *Ts, Mapper[B, T_OUT]]\n        ),\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            mappers (Mapper[T_IN, T_OUT] | tuple[Mapper, ...]): Mapper or tuple of\n                mappers to compose. If a single mapper is provided, it is treated as a\n                tuple with a single element.\n        \"\"\"\n        super().__init__()\n        if not isinstance(mappers, tuple):\n            mappers_t = (mappers,)\n        else:\n            mappers_t = mappers  # type: ignore\n        self._mappers = mappers_t\n\n    def __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n        temp = x\n        for mapper in self._mappers:\n            temp = cast(Mapper, mapper)(idx, temp)\n        return cast(T_OUT, temp)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ComposeMapper.__init__","title":"<code>__init__(mappers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mappers</code> <code>Mapper[T_IN, T_OUT] | tuple[Mapper, ...]</code> <p>Mapper or tuple of mappers to compose. If a single mapper is provided, it is treated as a tuple with a single element.</p> required Source code in <code>pipewine/mappers/compose.py</code> <pre><code>def __init__(\n    self,\n    mappers: (\n        Mapper[T_IN, T_OUT]\n        | tuple[Mapper[T_IN, T_OUT]]\n        | tuple[Mapper[T_IN, A], *Ts, Mapper[B, T_OUT]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Args:\n        mappers (Mapper[T_IN, T_OUT] | tuple[Mapper, ...]): Mapper or tuple of\n            mappers to compose. If a single mapper is provided, it is treated as a\n            tuple with a single element.\n    \"\"\"\n    super().__init__()\n    if not isinstance(mappers, tuple):\n        mappers_t = (mappers,)\n    else:\n        mappers_t = mappers  # type: ignore\n    self._mappers = mappers_t\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ConvertMapper","title":"<code>ConvertMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that changes the parser of selected items in a sample, allowing for conversion between different data formats, e.g., from JSON to YAML or from PNG to JPEG.</p> Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>class ConvertMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that changes the parser of selected items in a sample, allowing for\n    conversion between different data formats, e.g., from JSON to YAML or from PNG to\n    JPEG.\n    \"\"\"\n\n    def __init__(self, parsers: Mapping[str, Parser]) -&gt; None:\n        \"\"\"\n        Args:\n            parsers (Mapping[str, Parser]): Mapping of item keys to parsers to use for\n                converting the items.\n        \"\"\"\n        super().__init__()\n        self._parsers = parsers\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        to_modify: dict[str, Item] = {}\n        for k, parser in self._parsers.items():\n            if k in x:\n                to_modify[k] = x[k].with_parser(parser)\n        return x.with_items(**to_modify)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ConvertMapper.__init__","title":"<code>__init__(parsers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parsers</code> <code>Mapping[str, Parser]</code> <p>Mapping of item keys to parsers to use for converting the items.</p> required Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>def __init__(self, parsers: Mapping[str, Parser]) -&gt; None:\n    \"\"\"\n    Args:\n        parsers (Mapping[str, Parser]): Mapping of item keys to parsers to use for\n            converting the items.\n    \"\"\"\n    super().__init__()\n    self._parsers = parsers\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.DuplicateItemMapper","title":"<code>DuplicateItemMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Duplicates an item in the sample.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class DuplicateItemMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Duplicates an item in the sample.\"\"\"\n\n    def __init__(self, source_key: str, destination_key: str) -&gt; None:\n        \"\"\"\n        Args:\n            source_key (str): The key of the item to duplicate.\n            destination_key (str): The key of the duplicated item.\n        \"\"\"\n        super().__init__()\n        self._source_key = source_key\n        self._destination_key = destination_key\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.typeless().with_item(self._destination_key, x[self._source_key])\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.DuplicateItemMapper.__init__","title":"<code>__init__(source_key, destination_key)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source_key</code> <code>str</code> <p>The key of the item to duplicate.</p> required <code>destination_key</code> <code>str</code> <p>The key of the duplicated item.</p> required Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, source_key: str, destination_key: str) -&gt; None:\n    \"\"\"\n    Args:\n        source_key (str): The key of the item to duplicate.\n        destination_key (str): The key of the duplicated item.\n    \"\"\"\n    super().__init__()\n    self._source_key = source_key\n    self._destination_key = destination_key\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.FilterKeysMapper","title":"<code>FilterKeysMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Filters sample keys.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class FilterKeysMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Filters sample keys.\"\"\"\n\n    def __init__(self, keys: str | Iterable[str], negate: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            keys (str | Iterable[str]): The keys to keep or remove from the sample.\n            negate (bool, optional): If `True`, the keys are removed from the sample. If\n                `False`, only the keys are kept. Defaults to `False`.\n        \"\"\"\n        super().__init__()\n        self._keys = [keys] if isinstance(keys, str) else keys\n        self._negate = negate\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.without(*self._keys) if self._negate else x.with_only(*self._keys)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.FilterKeysMapper.__init__","title":"<code>__init__(keys, negate=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str | Iterable[str]</code> <p>The keys to keep or remove from the sample.</p> required <code>negate</code> <code>bool</code> <p>If <code>True</code>, the keys are removed from the sample. If <code>False</code>, only the keys are kept. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, keys: str | Iterable[str], negate: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        keys (str | Iterable[str]): The keys to keep or remove from the sample.\n        negate (bool, optional): If `True`, the keys are removed from the sample. If\n            `False`, only the keys are kept. Defaults to `False`.\n    \"\"\"\n    super().__init__()\n    self._keys = [keys] if isinstance(keys, str) else keys\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.FormatKeysMapper","title":"<code>FormatKeysMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Changes key names following a format string.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class FormatKeysMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Changes key names following a format string.\"\"\"\n\n    FMT_CHAR = \"*\"\n\n    def __init__(\n        self, format_string: str = FMT_CHAR, keys: str | Iterable[str] | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            format_string (str, optional): The new sample key format. Any `*` will be\n                replaced with the source key, eg, `my_*_key` on [`image`, `mask`]\n                generates `my_image_key` and `my_mask_key`. If no `*` is found, the\n                string is suffixed to source key, ie, `MyKey` on `image` gives\n                `imageMyKey`. If empty, the source key will not be changed. Defaults to\n                \"*\".\n\n            keys (str | Iterable[str] | None, optional): The keys to apply the new\n                format to. `None` applies to all the keys. Defaults to None.\n        \"\"\"\n        super().__init__()\n        if self.FMT_CHAR not in format_string:\n            format_string = self.FMT_CHAR + format_string\n\n        self._format_string = format_string\n        self._keys = keys\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        keys: Iterable[str]\n        if self._keys is None:\n            keys = x.keys()\n        elif isinstance(self._keys, str):\n            keys = [self._keys]\n        else:\n            keys = self._keys\n        remap = {}\n        for k in keys:\n            remap[k] = self._format_string.replace(self.FMT_CHAR, k)\n        return x.remap(remap)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.FormatKeysMapper.__init__","title":"<code>__init__(format_string=FMT_CHAR, keys=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>format_string</code> <code>str</code> <p>The new sample key format. Any <code>*</code> will be replaced with the source key, eg, <code>my_*_key</code> on [<code>image</code>, <code>mask</code>] generates <code>my_image_key</code> and <code>my_mask_key</code>. If no <code>*</code> is found, the string is suffixed to source key, ie, <code>MyKey</code> on <code>image</code> gives <code>imageMyKey</code>. If empty, the source key will not be changed. Defaults to \"*\".</p> <code>FMT_CHAR</code> <code>keys</code> <code>str | Iterable[str] | None</code> <p>The keys to apply the new format to. <code>None</code> applies to all the keys. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(\n    self, format_string: str = FMT_CHAR, keys: str | Iterable[str] | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        format_string (str, optional): The new sample key format. Any `*` will be\n            replaced with the source key, eg, `my_*_key` on [`image`, `mask`]\n            generates `my_image_key` and `my_mask_key`. If no `*` is found, the\n            string is suffixed to source key, ie, `MyKey` on `image` gives\n            `imageMyKey`. If empty, the source key will not be changed. Defaults to\n            \"*\".\n\n        keys (str | Iterable[str] | None, optional): The keys to apply the new\n            format to. `None` applies to all the keys. Defaults to None.\n    \"\"\"\n    super().__init__()\n    if self.FMT_CHAR not in format_string:\n        format_string = self.FMT_CHAR + format_string\n\n    self._format_string = format_string\n    self._keys = keys\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.HashMapper","title":"<code>HashMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, HashedSample]</code></p> <p>Compute the hash of a sample based on the selected items.</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>class HashMapper(Mapper[Sample, HashedSample]):\n    \"\"\"Compute the hash of a sample based on the selected items.\"\"\"\n\n    def __init__(\n        self, algorithm: str = \"sha256\", keys: str | Sequence[str] | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            algorithm (str, optional): Hash algorithm to use, must be one of the\n                algorithms available in `hashlib.algorithms_available`. Defaults to\n                \"sha256\".\n            keys (str | Sequence[str] | None, optional): Keys of the sample to use for\n                computing the hash. If a string is provided, it is treated as a single\n                key. If a sequence is provided, it is treated as a list of keys. If\n                `None` is provided, all keys in the sample are used. Defaults to `None`.\n\n        Raises:\n            ValueError: If the provided algorithm is not available in `hashlib` or if it\n                requires parameters. Currently, the only two algorithms that require\n                parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".\n        \"\"\"\n        super().__init__()\n        algorithms_with_parameters = [\"shake_128\", \"shake_256\"]\n        if (\n            algorithm not in hashlib.algorithms_available\n            or algorithm in algorithms_with_parameters\n        ):\n            raise ValueError(f\"Invalid algorithm: {algorithm}\")\n        self._algorithm = algorithm\n        self._keys = keys\n\n    def __call__(self, idx: int, x: Sample) -&gt; HashedSample:\n        hash_ = self._compute_sample_hash(x)\n        return HashedSample(hash=MemoryItem(hash_, YAMLParser(type_=str)))\n\n    def _compute_item_hash(self, data: Any) -&gt; str:\n        return hashlib.new(self._algorithm, pickle.dumps(data)).hexdigest()\n\n    def _compute_sample_hash(self, sample: Sample) -&gt; str:\n        keys: Iterable[str]\n        if isinstance(self._keys, str):\n            keys = [self._keys]\n        elif isinstance(self._keys, Sequence):\n            keys = self._keys\n        else:\n            keys = sorted(list(sample.keys()))\n        return \"\".join([self._compute_item_hash(sample[k]()) for k in keys])  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.HashMapper.__init__","title":"<code>__init__(algorithm='sha256', keys=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>str</code> <p>Hash algorithm to use, must be one of the algorithms available in <code>hashlib.algorithms_available</code>. Defaults to \"sha256\".</p> <code>'sha256'</code> <code>keys</code> <code>str | Sequence[str] | None</code> <p>Keys of the sample to use for computing the hash. If a string is provided, it is treated as a single key. If a sequence is provided, it is treated as a list of keys. If <code>None</code> is provided, all keys in the sample are used. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided algorithm is not available in <code>hashlib</code> or if it requires parameters. Currently, the only two algorithms that require parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>def __init__(\n    self, algorithm: str = \"sha256\", keys: str | Sequence[str] | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithm (str, optional): Hash algorithm to use, must be one of the\n            algorithms available in `hashlib.algorithms_available`. Defaults to\n            \"sha256\".\n        keys (str | Sequence[str] | None, optional): Keys of the sample to use for\n            computing the hash. If a string is provided, it is treated as a single\n            key. If a sequence is provided, it is treated as a list of keys. If\n            `None` is provided, all keys in the sample are used. Defaults to `None`.\n\n    Raises:\n        ValueError: If the provided algorithm is not available in `hashlib` or if it\n            requires parameters. Currently, the only two algorithms that require\n            parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".\n    \"\"\"\n    super().__init__()\n    algorithms_with_parameters = [\"shake_128\", \"shake_256\"]\n    if (\n        algorithm not in hashlib.algorithms_available\n        or algorithm in algorithms_with_parameters\n    ):\n        raise ValueError(f\"Invalid algorithm: {algorithm}\")\n    self._algorithm = algorithm\n    self._keys = keys\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.HashedSample","title":"<code>HashedSample</code>","text":"<p>               Bases: <code>TypedSample</code></p> <p>Sample type to represent the hash of a sample.</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>class HashedSample(TypedSample):\n    \"\"\"Sample type to represent the hash of a sample.\"\"\"\n\n    hash: Item[str]\n    \"\"\"The hash of the sample.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.HashedSample.hash","title":"<code>hash</code>  <code>instance-attribute</code>","text":"<p>The hash of the sample.</p>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.Mapper","title":"<code>Mapper</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Pipewine mappers, which are functions that transform individual samples of a dataset.</p> <p>All mapper classes must inherit from this class and implement the <code>__call__</code> method.</p> <p>Mapper classes can be parameterized by the types of the input and output samples, <code>T_IN</code> and <code>T_OUT</code>, respectively. These types must be subclasses of <code>Sample</code>.</p> Source code in <code>pipewine/mappers/base.py</code> <pre><code>class Mapper[T_IN: Sample, T_OUT: Sample](ABC):\n    \"\"\"Base class for all Pipewine mappers, which are functions that transform\n    individual samples of a dataset.\n\n    All mapper classes must inherit from this class and implement the `__call__`\n    method.\n\n    Mapper classes can be parameterized by the types of the input and output\n    samples, `T_IN` and `T_OUT`, respectively. These types must be subclasses of\n    `Sample`.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n        \"\"\"Transform a sample of type `T_IN` into another sample of type `T_OUT`.\n\n        Args:\n            idx (int): The index of the sample in the dataset.\n            x (T_IN): The input sample to be transformed.\n\n        Returns:\n            T_OUT: The transformed output sample.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.Mapper.__call__","title":"<code>__call__(idx, x)</code>  <code>abstractmethod</code>","text":"<p>Transform a sample of type <code>T_IN</code> into another sample of type <code>T_OUT</code>.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the sample in the dataset.</p> required <code>x</code> <code>T_IN</code> <p>The input sample to be transformed.</p> required <p>Returns:</p> Name Type Description <code>T_OUT</code> <code>T_OUT</code> <p>The transformed output sample.</p> Source code in <code>pipewine/mappers/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n    \"\"\"Transform a sample of type `T_IN` into another sample of type `T_OUT`.\n\n    Args:\n        idx (int): The index of the sample in the dataset.\n        x (T_IN): The input sample to be transformed.\n\n    Returns:\n        T_OUT: The transformed output sample.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.RenameMapper","title":"<code>RenameMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Rename some items preserving their content and format.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class RenameMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Rename some items preserving their content and format.\"\"\"\n\n    def __init__(self, renaming: Mapping[str, str], exclude: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            renaming (Mapping[str, str]): Mapping of keys to rename. The keys are the\n                original keys, and the values are the new keys.\n            exclude (bool, optional): If `True`, only the keys in the `renaming` mapping\n                will be renamed. If `False`, all keys except those in the `renaming`\n                mapping will be renamed. Defaults to `False`.\n        \"\"\"\n        super().__init__()\n        self._renaming = renaming\n        self._exclude = exclude\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.remap(self._renaming, exclude=self._exclude)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.RenameMapper.__init__","title":"<code>__init__(renaming, exclude=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>renaming</code> <code>Mapping[str, str]</code> <p>Mapping of keys to rename. The keys are the original keys, and the values are the new keys.</p> required <code>exclude</code> <code>bool</code> <p>If <code>True</code>, only the keys in the <code>renaming</code> mapping will be renamed. If <code>False</code>, all keys except those in the <code>renaming</code> mapping will be renamed. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, renaming: Mapping[str, str], exclude: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        renaming (Mapping[str, str]): Mapping of keys to rename. The keys are the\n            original keys, and the values are the new keys.\n        exclude (bool, optional): If `True`, only the keys in the `renaming` mapping\n            will be renamed. If `False`, all keys except those in the `renaming`\n            mapping will be renamed. Defaults to `False`.\n    \"\"\"\n    super().__init__()\n    self._renaming = renaming\n    self._exclude = exclude\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ShareMapper","title":"<code>ShareMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that changes the sharedness of selected items in a sample, allowing for sharing or unsharing items between samples.</p> Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>class ShareMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that changes the sharedness of selected items in a sample, allowing for\n    sharing or unsharing items between samples.\n    \"\"\"\n\n    def __init__(self, share: Iterable[str], unshare: Iterable[str]) -&gt; None:\n        \"\"\"\n        Args:\n            share (Iterable[str]): Keys of the items to share between samples.\n            unshare (Iterable[str]): Keys of the items to unshare between samples.\n        \"\"\"\n        super().__init__()\n        if set(share) &amp; set(unshare):\n            raise ValueError(\"The keys in 'share' and 'unshare' must be disjoint.\")\n        self._share = share\n        self._unshare = unshare\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        to_modify: dict[str, Item] = {}\n        for k, item in x.items():\n            if not item.is_shared and k in self._share:\n                to_modify[k] = item.with_sharedness(True)\n            elif item.is_shared and k in self._unshare:\n                to_modify[k] = item.with_sharedness(False)\n        return x.with_items(**to_modify)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.ShareMapper.__init__","title":"<code>__init__(share, unshare)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>share</code> <code>Iterable[str]</code> <p>Keys of the items to share between samples.</p> required <code>unshare</code> <code>Iterable[str]</code> <p>Keys of the items to unshare between samples.</p> required Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>def __init__(self, share: Iterable[str], unshare: Iterable[str]) -&gt; None:\n    \"\"\"\n    Args:\n        share (Iterable[str]): Keys of the items to share between samples.\n        unshare (Iterable[str]): Keys of the items to unshare between samples.\n    \"\"\"\n    super().__init__()\n    if set(share) &amp; set(unshare):\n        raise ValueError(\"The keys in 'share' and 'unshare' must be disjoint.\")\n    self._share = share\n    self._unshare = unshare\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.convert","title":"<code>convert(conversion)</code>","text":"<p>Change the format of individual items.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef convert(\n    conversion: Annotated[\n        list[str], Option(..., \"-c\", \"--conversion\", help=conversion_help)\n    ],\n) -&gt; ConvertMapper:\n    \"\"\"Change the format of individual items.\"\"\"\n    parsers: dict[str, Parser] = {}\n    for k_eq_fmt in conversion:\n        k, _, fmt = k_eq_fmt.partition(\"=\")\n        ptype = ParserRegistry.get(fmt)\n        assert ptype is not None\n        parsers[k] = ptype()\n    return ConvertMapper(parsers)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.duplicate","title":"<code>duplicate(source_key, destination_key)</code>","text":"<p>Create a copy of an item with a different name.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef duplicate(\n    source_key: Annotated[\n        str, Option(..., \"-s\", \"--src\", \"--source-key\", help=source_key_help)\n    ],\n    destination_key: Annotated[\n        str, Option(..., \"-d\", \"--dst\", \"--destination-key\", help=destination_key_help)\n    ],\n) -&gt; DuplicateItemMapper:\n    \"\"\"Create a copy of an item with a different name.\"\"\"\n    return DuplicateItemMapper(source_key, destination_key)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.filter_keys","title":"<code>filter_keys(keys, negate=False)</code>","text":"<p>Keep only or remove a subset of items.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef filter_keys(\n    keys: Annotated[list[str], Option(..., \"-k\", \"--keys\", help=keys_help)],\n    negate: Annotated[bool, Option(..., \"-n\", \"--negate\", help=negate_help)] = False,\n) -&gt; FilterKeysMapper:\n    \"\"\"Keep only or remove a subset of items.\"\"\"\n    return FilterKeysMapper(keys, negate=negate)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.format_keys","title":"<code>format_keys(keys, format_string='*')</code>","text":"<p>Rename items according to a custom format rule.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef format_keys(\n    keys: Annotated[list[str], Option(..., \"-k\", \"--keys\", help=keys_help)],\n    format_string: Annotated[\n        str, Option(..., \"-f\", \"--format-string\", help=format_string_help)\n    ] = \"*\",\n) -&gt; FormatKeysMapper:\n    \"\"\"Rename items according to a custom format rule.\"\"\"\n    return FormatKeysMapper(keys=keys, format_string=format_string)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.hash_","title":"<code>hash_(algorithm='sha256', keys=[])</code>","text":"<p>Apply a hashing function to each sample.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli(name=\"hash\")\ndef hash_(\n    algorithm: Annotated[\n        str, Option(..., \"-a\", \"--algorithm\", help=algo_help)\n    ] = \"sha256\",\n    keys: Annotated[list[str], Option(..., \"-k\", \"--keys\", help=keys_help)] = [],\n) -&gt; HashMapper:\n    \"\"\"Apply a hashing function to each sample.\"\"\"\n    return HashMapper(algorithm=algorithm, keys=None if len(keys) == 0 else keys)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.map_cli","title":"<code>map_cli(name=None)</code>","text":"<p>Decorator to generate a CLI command for a dataset mapper.</p> <p>Decorated functions must follow the rules of Typer CLI commands, returning a <code>Mapper</code> object.</p> <p>The decorated function must be correctly annotated with the type of mapper it returns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the command. Defaults to None, in which case the function name is used.</p> <code>None</code> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>def map_cli[T](name: str | None = None) -&gt; Callable[[T], T]:\n    \"\"\"Decorator to generate a CLI command for a dataset mapper.\n\n    Decorated functions must follow the rules of Typer CLI commands, returning a\n    `Mapper` object.\n\n    The decorated function must be correctly annotated with the type of mapper it\n    returns.\n\n    Args:\n        name (str, optional): The name of the command. Defaults to None, in which case\n            the function name is used.\n    \"\"\"\n    return partial(_generate_command, name=name)  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.rename","title":"<code>rename(renaming, exclude=False)</code>","text":"<p>Rename items with a custom mapping from old to new names.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef rename(\n    renaming: Annotated[list[str], Option(..., \"-r\", \"--renaming\", help=renaming_help)],\n    exclude: Annotated[bool, Option(..., \"-e\", \"--exclude\", help=exclude_help)] = False,\n) -&gt; RenameMapper:\n    \"\"\"Rename items with a custom mapping from old to new names.\"\"\"\n    renaming_map: dict[str, str] = {}\n    for old_eq_new in renaming:\n        old, _, new = old_eq_new.partition(\"=\")\n        renaming_map[old] = new\n    return RenameMapper(renaming_map, exclude=exclude)\n</code></pre>"},{"location":"autoapi/pipewine/cli/mappers/#pipewine.cli.mappers.share","title":"<code>share(share=[], unshare=[])</code>","text":"<p>Change the sharedness of individual items.</p> Source code in <code>pipewine/cli/mappers.py</code> <pre><code>@map_cli()\ndef share(\n    share: Annotated[list[str], Option(..., \"-s\", \"--share\", help=share_help)] = [],\n    unshare: Annotated[\n        list[str], Option(..., \"-u\", \"--unshare\", help=unshare_help)\n    ] = [],\n) -&gt; ShareMapper:\n    \"\"\"Change the sharedness of individual items.\"\"\"\n    return ShareMapper(share, unshare)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/","title":"ops","text":"<p>CLI commands for dataset operators.</p>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.op_app","title":"<code>op_app = Typer(callback=_op_callback, name='op', help='Run a pipewine dataset operator.', invoke_without_command=True, no_args_is_help=True)</code>  <code>module-attribute</code>","text":"<p>Typer app for the Pipewine op CLI.</p>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.BatchOp","title":"<code>BatchOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into batches of a given size, except for the last batch which may be smaller.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class BatchOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into batches of a given size, except for the\n    last batch which may be smaller.\n    \"\"\"\n\n    def __init__(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Args:\n            batch_size (int): Size of the batches.\n        \"\"\"\n        super().__init__()\n        self._batch_size = batch_size\n        assert self._batch_size &gt; 0, \"Batch size must be greater than 0.\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        start = 0\n        batches = []\n        while start &lt; len(x):\n            batches.append(x[start : start + self._batch_size])\n            start += self._batch_size\n\n        return batches\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.BatchOp.__init__","title":"<code>__init__(batch_size)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of the batches.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Args:\n        batch_size (int): Size of the batches.\n    \"\"\"\n    super().__init__()\n    self._batch_size = batch_size\n    assert self._batch_size &gt; 0, \"Batch size must be greater than 0.\"\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache","title":"<code>Cache</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Key-value cache abstraction with thread-safe operations on arbitrary keys and values.</p> <p>Subclasses must implement the <code>_clear</code>, <code>_get</code>, and <code>_put</code> methods to define the cache behavior and eviction policy. These methods are automatically made thread-safe by the <code>Cache</code> class, so there is no need to worry about acquiring and releasing locks when implementing them.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class Cache[K, V](ABC):\n    \"\"\"Key-value cache abstraction with thread-safe operations on arbitrary keys and\n    values.\n\n    Subclasses must implement the `_clear`, `_get`, and `_put` methods to define the\n    cache behavior and eviction policy. These methods are automatically made thread-safe\n    by the `Cache` class, so there is no need to worry about acquiring and releasing\n    locks when implementing them.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the locks necessary for thread-safety, always make sure to call\n        this constructor when inheriting from this class.\n        \"\"\"\n        self._lock = RLock()\n\n    @abstractmethod\n    def _clear(self) -&gt; None:\n        \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get(self, key: K) -&gt; V | None:\n        \"\"\"Get the value associated with the given key.\n\n        Args:\n            key (K): Key to look up in the cache.\n\n        Returns:\n            V | None: Value associated with the key, or `None` if the key is not present\n                in the cache.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _put(self, key: K, value: V) -&gt; None:\n        \"\"\"Put a key-value pair in the cache.\n\n        Args:\n            key (K): Key to associate with the value.\n            value (V): Value to store in the cache.\n        \"\"\"\n        pass\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n        with self._lock:\n            self._clear()\n\n    def get(self, key: K) -&gt; V | None:\n        \"\"\"Get the value associated with the given key.\n\n        Args:\n            key (K): Key to look up in the cache.\n\n        Returns:\n            V | None: Value associated with the key, or `None` if the key is not present\n                in the cache.\n        \"\"\"\n        with self._lock:\n            return self._get(key)\n\n    def put(self, key: K, value: V) -&gt; None:\n        \"\"\"Put a key-value pair in the cache.\n\n        Args:\n            key (K): Key to associate with the value.\n            value (V): Value to store in the cache.\n        \"\"\"\n        with self._lock:\n            self._put(key, value)\n\n    def __getstate__(self) -&gt; dict[str, Any]:\n        data = {**self.__dict__}\n        del data[\"_lock\"]\n        return data\n\n    def __setstate__(self, data: dict[str, Any]) -&gt; None:\n        self._lock = RLock()\n        for k, v in data.items():\n            setattr(self, k, v)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the locks necessary for thread-safety, always make sure to call this constructor when inheriting from this class.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the locks necessary for thread-safety, always make sure to call\n    this constructor when inheriting from this class.\n    \"\"\"\n    self._lock = RLock()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache._clear","title":"<code>_clear()</code>  <code>abstractmethod</code>","text":"<p>Clear the cache, removing all key-value pairs.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _clear(self) -&gt; None:\n    \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache._get","title":"<code>_get(key)</code>  <code>abstractmethod</code>","text":"<p>Get the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to look up in the cache.</p> required <p>Returns:</p> Type Description <code>V | None</code> <p>V | None: Value associated with the key, or <code>None</code> if the key is not present in the cache.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _get(self, key: K) -&gt; V | None:\n    \"\"\"Get the value associated with the given key.\n\n    Args:\n        key (K): Key to look up in the cache.\n\n    Returns:\n        V | None: Value associated with the key, or `None` if the key is not present\n            in the cache.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache._put","title":"<code>_put(key, value)</code>  <code>abstractmethod</code>","text":"<p>Put a key-value pair in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to associate with the value.</p> required <code>value</code> <code>V</code> <p>Value to store in the cache.</p> required Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _put(self, key: K, value: V) -&gt; None:\n    \"\"\"Put a key-value pair in the cache.\n\n    Args:\n        key (K): Key to associate with the value.\n        value (V): Value to store in the cache.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache.clear","title":"<code>clear()</code>","text":"<p>Clear the cache, removing all key-value pairs.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n    with self._lock:\n        self._clear()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache.get","title":"<code>get(key)</code>","text":"<p>Get the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to look up in the cache.</p> required <p>Returns:</p> Type Description <code>V | None</code> <p>V | None: Value associated with the key, or <code>None</code> if the key is not present in the cache.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def get(self, key: K) -&gt; V | None:\n    \"\"\"Get the value associated with the given key.\n\n    Args:\n        key (K): Key to look up in the cache.\n\n    Returns:\n        V | None: Value associated with the key, or `None` if the key is not present\n            in the cache.\n    \"\"\"\n    with self._lock:\n        return self._get(key)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.Cache.put","title":"<code>put(key, value)</code>","text":"<p>Put a key-value pair in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to associate with the value.</p> required <code>value</code> <code>V</code> <p>Value to store in the cache.</p> required Source code in <code>pipewine/operators/cache.py</code> <pre><code>def put(self, key: K, value: V) -&gt; None:\n    \"\"\"Put a key-value pair in the cache.\n\n    Args:\n        key (K): Key to associate with the value.\n        value (V): Value to store in the cache.\n    \"\"\"\n    with self._lock:\n        self._put(key, value)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.CacheOp","title":"<code>CacheOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches the results of another operator to avoid recomputation. See the \"Cache\" section in the documentation for more information.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class CacheOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches the results of another operator to avoid recomputation.\n    See the \"Cache\" section in the documentation for more information.\n    \"\"\"\n\n    def __init__(self, cache_type: type[Cache], **cache_params) -&gt; None:\n        \"\"\"\n        Args:\n            cache_type (type[Cache]): Type of cache to use, must be a subclass of\n                `Cache`.\n            cache_params (Any): Additional parameters to pass to the cache constructor.\n        \"\"\"\n        super().__init__()\n        self._cache_mapper: CacheMapper = CacheMapper()\n        self._cache_type = cache_type\n        self._cache_params = cache_params\n\n    def _get_sample[T: Sample](self, dataset: Dataset[T], cache_id: str, idx: int) -&gt; T:\n        cache: Cache[int, T] = InheritedData.data[cache_id]\n        result = cache.get(idx)\n        if result is None:\n            result = self._cache_mapper(idx, dataset[idx])\n            cache.put(idx, result)\n        return result\n\n    def _finalize_cache(self, id_: str) -&gt; None:\n        if id_ in InheritedData.data:  # pragma: no branch\n            del InheritedData.data[id_]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; LazyDataset[T]:\n        cache = self._cache_type(**self._cache_params)\n        id_ = uuid4().hex\n        InheritedData.data[id_] = cache\n        dataset = LazyDataset(len(x), partial(self._get_sample, x, id_))\n        weakref.finalize(dataset, self._finalize_cache, id_=id_)\n        return dataset\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.CacheOp.__init__","title":"<code>__init__(cache_type, **cache_params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cache_type</code> <code>type[Cache]</code> <p>Type of cache to use, must be a subclass of <code>Cache</code>.</p> required <code>cache_params</code> <code>Any</code> <p>Additional parameters to pass to the cache constructor.</p> <code>{}</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, cache_type: type[Cache], **cache_params) -&gt; None:\n    \"\"\"\n    Args:\n        cache_type (type[Cache]): Type of cache to use, must be a subclass of\n            `Cache`.\n        cache_params (Any): Additional parameters to pass to the cache constructor.\n    \"\"\"\n    super().__init__()\n    self._cache_mapper: CacheMapper = CacheMapper()\n    self._cache_type = cache_type\n    self._cache_params = cache_params\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.CatOp","title":"<code>CatOp</code>","text":"<p>               Bases: <code>DatasetOperator[Sequence[Dataset], Dataset]</code></p> <p>Operator that concatenates multiple datasets into a single dataset.</p> Source code in <code>pipewine/operators/merge.py</code> <pre><code>class CatOp(DatasetOperator[Sequence[Dataset], Dataset]):\n    \"\"\"Operator that concatenates multiple datasets into a single dataset.\"\"\"\n\n    def _get_sample[\n        T: Sample\n    ](self, datasets: Sequence[Dataset[T]], index: list[int], i: int) -&gt; T:\n        dataset_idx = bisect(index, i) - 1\n        effective_i = i - index[dataset_idx]\n        return datasets[dataset_idx][effective_i]\n\n    def __call__[T: Sample](self, x: Sequence[Dataset[T]]) -&gt; Dataset[T]:\n        index = [0]\n        for dataset in x:\n            index.append(index[-1] + len(dataset))\n        return LazyDataset(index[-1], partial(self._get_sample, x, index))\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ChunkOp","title":"<code>ChunkOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into a given number of chunks of approximately equal size. The difference in size between any two chunks is guaranteed to be at most 1.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class ChunkOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into a given number of chunks of approximately\n    equal size. The difference in size between any two chunks is guaranteed to be\n    at most 1.\n    \"\"\"\n\n    def __init__(self, chunks: int) -&gt; None:\n        \"\"\"\n        Args:\n            chunks (int): Number of chunks.\n        \"\"\"\n        super().__init__()\n        self._chunks = chunks\n        assert self._chunks &gt; 0, \"Number of chunks must be greater than 0.\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        size_floor = len(x) // self._chunks\n        sizes = [size_floor] * self._chunks\n        remainder = len(x) % self._chunks\n        for i in range(remainder):\n            sizes[i] += 1\n        start = 0\n        chunks = []\n        for size in sizes:\n            chunks.append(x[start : start + size])\n            start += size\n        return chunks\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ChunkOp.__init__","title":"<code>__init__(chunks)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>int</code> <p>Number of chunks.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, chunks: int) -&gt; None:\n    \"\"\"\n    Args:\n        chunks (int): Number of chunks.\n    \"\"\"\n    super().__init__()\n    self._chunks = chunks\n    assert self._chunks &gt; 0, \"Number of chunks must be greater than 0.\"\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.CycleOp","title":"<code>CycleOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that repeats a dataset until a given length is reached.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class CycleOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that repeats a dataset until a given length is reached.\"\"\"\n\n    def __init__(self, n: int) -&gt; None:\n        \"\"\"\n        Args:\n            n (int): Length of the resulting dataset.\n        \"\"\"\n        super().__init__()\n        self._n = n\n\n    def _index_fn(self, orig_len: int, x: int) -&gt; int:\n        return x % orig_len\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        assert len(x) &gt; 0\n        return LazyDataset(\n            self._n, x.get_sample, index_fn=partial(self._index_fn, len(x))\n        )\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.CycleOp.__init__","title":"<code>__init__(n)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Length of the resulting dataset.</p> required Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, n: int) -&gt; None:\n    \"\"\"\n    Args:\n        n (int): Length of the resulting dataset.\n    \"\"\"\n    super().__init__()\n    self._n = n\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.DatasetOperator","title":"<code>DatasetOperator</code>","text":"<p>               Bases: <code>ABC</code>, <code>LoopCallbackMixin</code></p> <p>Base class for all Pipewine dataset operators, which are functions that transform datasets into other datasets.</p> <p>All dataset operator classes must inherit from this class and implement the <code>__call__</code> method, and must provide type hints to enable type inference, required by other pipewine components.</p> <p>Dataset operator classes can be parameterized by the types of the input and output datasets, <code>T_IN</code> and <code>T_OUT</code>, respectively. These generic types are bound to one of the following types: - <code>Dataset</code>: A single dataset. - <code>Sequence[Dataset]</code>: A sequence of datasets. - <code>tuple[Dataset, ...]</code>: A tuple of datasets, with the possibility of having     statically known length and types. - <code>Mapping[str, Dataset]</code>: A mapping of dataset names to datasets. - <code>Bundle[Dataset]</code>: A bundle of datasets, with statically known field names and     types.</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>class DatasetOperator[T_IN: AnyDataset, T_OUT: AnyDataset](ABC, LoopCallbackMixin):\n    \"\"\"Base class for all Pipewine dataset operators, which are functions that transform\n    datasets into other datasets.\n\n    All dataset operator classes must inherit from this class and implement the\n    `__call__` method, and **must** provide type hints to enable type inference,\n    required by other pipewine components.\n\n    Dataset operator classes can be parameterized by the types of the input and output\n    datasets, `T_IN` and `T_OUT`, respectively. These generic types are bound to one\n    of the following types:\n    - `Dataset`: A single dataset.\n    - `Sequence[Dataset]`: A sequence of datasets.\n    - `tuple[Dataset, ...]`: A tuple of datasets, with the possibility of having\n        statically known length and types.\n    - `Mapping[str, Dataset]`: A mapping of dataset names to datasets.\n    - `Bundle[Dataset]`: A bundle of datasets, with statically known field names and\n        types.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, x: T_IN) -&gt; T_OUT:\n        \"\"\"Transform the input dataset (or collection) into another dataset (or\n        collection).\n\n        This method **must** always be correctly annotated with type hints to enable\n        type inference by other pipewine components. Failing to do so will result in\n        type errors when composing operators into workflows, or when registering\n        them to the Pipewine CLI.\n\n        Args:\n            x (T_IN): The input dataset (or collection) to be transformed.\n\n        Returns:\n            T_OUT: The transformed output dataset (or collection).\n        \"\"\"\n        pass\n\n    @property\n    def input_type(self):\n        \"\"\"Infer the origin type of this operator's input, returning the origin of the\n        `__call__` method's `x` parameter.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"x\"])\n\n    @property\n    def output_type(self):\n        \"\"\"Infer the origin type of this operator's input, returning the origin of the\n        `__call__` method's return.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"return\"])\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.DatasetOperator.input_type","title":"<code>input_type</code>  <code>property</code>","text":"<p>Infer the origin type of this operator's input, returning the origin of the <code>__call__</code> method's <code>x</code> parameter.</p>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.DatasetOperator.output_type","title":"<code>output_type</code>  <code>property</code>","text":"<p>Infer the origin type of this operator's input, returning the origin of the <code>__call__</code> method's return.</p>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.DatasetOperator.__call__","title":"<code>__call__(x)</code>  <code>abstractmethod</code>","text":"<p>Transform the input dataset (or collection) into another dataset (or collection).</p> <p>This method must always be correctly annotated with type hints to enable type inference by other pipewine components. Failing to do so will result in type errors when composing operators into workflows, or when registering them to the Pipewine CLI.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>T_IN</code> <p>The input dataset (or collection) to be transformed.</p> required <p>Returns:</p> Name Type Description <code>T_OUT</code> <code>T_OUT</code> <p>The transformed output dataset (or collection).</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, x: T_IN) -&gt; T_OUT:\n    \"\"\"Transform the input dataset (or collection) into another dataset (or\n    collection).\n\n    This method **must** always be correctly annotated with type hints to enable\n    type inference by other pipewine components. Failing to do so will result in\n    type errors when composing operators into workflows, or when registering\n    them to the Pipewine CLI.\n\n    Args:\n        x (T_IN): The input dataset (or collection) to be transformed.\n\n    Returns:\n        T_OUT: The transformed output dataset (or collection).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.FIFOCache","title":"<code>FIFOCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>First-In-First-Out (FIFO) cache that evicts the least recently inserted key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the order of access to the keys is known and the oldest keys are likely to be the least useful.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class FIFOCache[K, V](Cache[K, V]):\n    \"\"\"First-In-First-Out (FIFO) cache that evicts the least recently **inserted**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the order of access to the keys is known and\n    the oldest keys are likely to be the least useful.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: deque[K] = deque()\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            evicted = self._keys.popleft()\n            self._keys.append(key)\n            del self._mp[evicted]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.FIFOCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: deque[K] = deque()\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.FilterOp","title":"<code>FilterOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], Dataset[T]]</code></p> <p>Operator that keeps only or removes samples from a dataset based on a user-defined filter function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class FilterOp[T: Sample](DatasetOperator[Dataset[T], Dataset[T]]):\n    \"\"\"Operator that keeps only or removes samples from a dataset based on a\n    user-defined filter function.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Callable[[int, T], bool],\n        negate: bool = False,\n        grabber: Grabber | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], bool]): Function that takes the index and the sample\n                and returns whether the sample should be kept or removed.\n            negate (bool, optional): Whether to negate the filter function. Defaults to\n                False.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n        self._negate = negate\n\n    def __call__(self, x: Dataset[T]) -&gt; Dataset[T]:\n        new_index = []\n        for i, sample in self.loop(x, self._grabber, name=\"Filtering\"):\n            if self._fn(i, sample) ^ self._negate:\n                new_index.append(i)\n        return LazyDataset(len(new_index), x.get_sample, index_fn=new_index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.FilterOp.__init__","title":"<code>__init__(fn, negate=False, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], bool]</code> <p>Function that takes the index and the sample and returns whether the sample should be kept or removed.</p> required <code>negate</code> <code>bool</code> <p>Whether to negate the filter function. Defaults to False.</p> <code>False</code> <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self,\n    fn: Callable[[int, T], bool],\n    negate: bool = False,\n    grabber: Grabber | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], bool]): Function that takes the index and the sample\n            and returns whether the sample should be kept or removed.\n        negate (bool, optional): Whether to negate the filter function. Defaults to\n            False.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.GroupByOp","title":"<code>GroupByOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], dict[str, Dataset[T]]]</code></p> <p>Operator that groups samples in a dataset based on a user-defined grouping function, returning a mapping of datasets with a key for each unique value returned by the grouping function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class GroupByOp[T: Sample](DatasetOperator[Dataset[T], dict[str, Dataset[T]]]):\n    \"\"\"Operator that groups samples in a dataset based on a user-defined grouping\n    function, returning a mapping of datasets with a key for each unique value\n    returned by the grouping function.\n    \"\"\"\n\n    def __init__(\n        self, fn: Callable[[int, T], str], grabber: Grabber | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], str]): Function that takes the index and the sample\n                and returns a string representing the group to which the sample belongs.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n\n    def __call__(self, x: Dataset[T]) -&gt; dict[str, Dataset[T]]:\n        indexes: dict[str, list[int]] = defaultdict(list)\n        for i, sample in self.loop(x, self._grabber, name=\"Computing index\"):\n            key = self._fn(i, sample)\n            indexes[key].append(i)\n        return {\n            k: LazyDataset(len(index), x.get_sample, index_fn=index.__getitem__)\n            for k, index in indexes.items()\n        }\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.GroupByOp.__init__","title":"<code>__init__(fn, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], str]</code> <p>Function that takes the index and the sample and returns a string representing the group to which the sample belongs.</p> required <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self, fn: Callable[[int, T], str], grabber: Grabber | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], str]): Function that takes the index and the sample\n            and returns a string representing the group to which the sample belongs.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.IdentityOp","title":"<code>IdentityOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Identity operator that accepts a dataset and returns it unchanged. Useful for testing and debugging purposes, or when a no-op is needed in a workflow.</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>class IdentityOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Identity operator that accepts a dataset and returns it unchanged. Useful for\n    testing and debugging purposes, or when a no-op is needed in a workflow.\n    \"\"\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.IndexOp","title":"<code>IndexOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that selects samples from a dataset based on their indices.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class IndexOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that selects samples from a dataset based on their indices.\"\"\"\n\n    def __init__(self, index: Sequence[int], negate: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            index (Sequence[int]): Indices of the samples to select.\n            negate (bool, optional): Whether to negate the selection. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self._index = index\n        self._negate = negate\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        index: Sequence[int]\n        if self._negate:\n            index = list(set(range(len(x))).difference(set(self._index)))\n        else:\n            index = self._index\n        return LazyDataset(len(index), x.get_sample, index_fn=index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.IndexOp.__init__","title":"<code>__init__(index, negate=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>Indices of the samples to select.</p> required <code>negate</code> <code>bool</code> <p>Whether to negate the selection. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, index: Sequence[int], negate: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        index (Sequence[int]): Indices of the samples to select.\n        negate (bool, optional): Whether to negate the selection. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self._index = index\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ItemCacheOp","title":"<code>ItemCacheOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches the items of the samples in a dataset to avoid recomputation. Essentially the same as <code>MapOp(CacheMapper())</code>.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class ItemCacheOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches the items of the samples in a dataset to avoid\n    recomputation. Essentially the same as `MapOp(CacheMapper())`.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the operator with a `MapOp` that uses a `CacheMapper`.\"\"\"\n        super().__init__()\n        self._map_op = MapOp(CacheMapper())\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return self._map_op(x)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ItemCacheOp.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the operator with a <code>MapOp</code> that uses a <code>CacheMapper</code>.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the operator with a `MapOp` that uses a `CacheMapper`.\"\"\"\n    super().__init__()\n    self._map_op = MapOp(CacheMapper())\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.LIFOCache","title":"<code>LIFOCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Last-In-First-Out (LIFO) cache that evicts the most recently inserted key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the data is accessed in long repeated cycles, where the most recently inserted keys are likely to not going to be used again soon.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class LIFOCache[K, V](Cache[K, V]):\n    \"\"\"Last-In-First-Out (LIFO) cache that evicts the most recently **inserted**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the data is accessed in long repeated cycles,\n    where the most recently inserted keys are likely to not going to be used again soon.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: list[K] = []\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            evicted = self._keys[-1]\n            self._keys[-1] = key\n            del self._mp[evicted]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.LIFOCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: list[K] = []\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.LRUCache","title":"<code>LRUCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Least Recently Used (LRU) cache that evicts the least recently accessed key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the most recently accessed keys are likely to be the most useful in the near future.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class LRUCache[K, V](Cache[K, V]):\n    \"\"\"Least Recently Used (LRU) cache that evicts the least recently **accessed**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the most recently accessed keys are likely to\n    be the most useful in the near future.\n    \"\"\"\n\n    _PREV, _NEXT, _KEY, _VALUE = 0, 1, 2, 3\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._maxsize = maxsize\n        self._dll: list = []\n        self._dll[:] = [self._dll, self._dll, None, None]\n        self._mp: dict[K, list] = {}\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._dll[:] = [self._dll, self._dll, None, None]\n\n    def _get(self, key: K) -&gt; V | None:\n        link = self._mp.get(key)\n        if link is not None:\n            link_prev, link_next, _key, value = link\n            link_prev[self._NEXT] = link_next\n            link_next[self._PREV] = link_prev\n            last = self._dll[self._PREV]\n            last[self._NEXT] = self._dll[self._PREV] = link\n            link[self._PREV] = last\n            link[self._NEXT] = self._dll\n            return value\n        return None\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if key in self._mp:\n            self._mp[key][self._VALUE] = value\n            self._get(key)  # Set key as mru\n        elif len(self._mp) &gt;= self._maxsize:\n            oldroot = self._dll\n            oldroot[self._KEY] = key\n            oldroot[self._VALUE] = value\n            self._dll = oldroot[self._NEXT]\n            oldkey = self._dll[self._KEY]\n            oldvalue = self._dll[self._VALUE]\n            self._dll[self._KEY] = self._dll[self._VALUE] = None\n            del self._mp[oldkey]\n            self._mp[key] = oldroot\n        else:\n            last = self._dll[self._PREV]\n            link = [last, self._dll, key, value]\n            last[self._NEXT] = self._dll[self._PREV] = self._mp[key] = link\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.LRUCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._maxsize = maxsize\n    self._dll: list = []\n    self._dll[:] = [self._dll, self._dll, None, None]\n    self._mp: dict[K, list] = {}\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MRUCache","title":"<code>MRUCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Most Recently Used (MRU) cache that evicts the most recently accessed key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the most recently accessed keys are likely not going to be accessed again soon.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MRUCache[K, V](Cache[K, V]):\n    \"\"\"Most Recently Used (MRU) cache that evicts the most recently **accessed**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the most recently accessed keys are likely not\n    going to be accessed again soon.\n    \"\"\"\n\n    _PREV, _NEXT, _KEY, _VALUE = 0, 1, 2, 3\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n\n        super().__init__()\n        self._maxsize = maxsize\n        self._dll: list = []\n        self._dll[:] = [self._dll, self._dll, None, None]\n        self._mp: dict[K, list] = {}\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._dll[:] = [self._dll, self._dll, None, None]\n\n    def _get(self, key: K) -&gt; V | None:\n        link = self._mp.get(key)\n        if link is not None:\n            link_prev, link_next, _key, value = link\n            link_prev[self._NEXT] = link_next\n            link_next[self._PREV] = link_prev\n            last = self._dll[self._PREV]\n            last[self._NEXT] = self._dll[self._PREV] = link\n            link[self._PREV] = last\n            link[self._NEXT] = self._dll\n            return value\n        return None\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if key in self._mp:\n            self._mp[key][self._VALUE] = value\n            self._get(key)  # Set key as mru\n        elif len(self._mp) &gt;= self._maxsize:\n            mru = self._dll[self._PREV]\n            oldkey = mru[self._KEY]\n            oldvalue = mru[self._VALUE]\n            mru[self._KEY] = key\n            mru[self._VALUE] = value\n            del self._mp[oldkey]\n            self._mp[key] = mru\n        else:\n            last = self._dll[self._PREV]\n            link = [last, self._dll, key, value]\n            last[self._NEXT] = self._dll[self._PREV] = self._mp[key] = link\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MRUCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n\n    super().__init__()\n    self._maxsize = maxsize\n    self._dll: list = []\n    self._dll[:] = [self._dll, self._dll, None, None]\n    self._mp: dict[K, list] = {}\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MapOp","title":"<code>MapOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T_IN], Dataset[T_OUT]]</code></p> <p>Operator that applies a <code>Mapper</code> to each sample in a dataset.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class MapOp[T_IN: Sample, T_OUT: Sample](\n    DatasetOperator[Dataset[T_IN], Dataset[T_OUT]]\n):\n    \"\"\"Operator that applies a `Mapper` to each sample in a dataset.\"\"\"\n\n    def __init__(self, mapper: Mapper[T_IN, T_OUT]) -&gt; None:\n        \"\"\"\n        Args:\n            mapper (Mapper[T_IN, T_OUT]): Mapper to apply to each sample.\n        \"\"\"\n        super().__init__()\n        self._mapper = mapper\n\n    def _get_sample(self, x: Dataset[T_IN], idx: int) -&gt; T_OUT:\n        return self._mapper(idx, x[idx])\n\n    def __call__(self, x: Dataset[T_IN]) -&gt; Dataset[T_OUT]:\n        return LazyDataset(len(x), partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MapOp.__init__","title":"<code>__init__(mapper)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mapper</code> <code>Mapper[T_IN, T_OUT]</code> <p>Mapper to apply to each sample.</p> required Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(self, mapper: Mapper[T_IN, T_OUT]) -&gt; None:\n    \"\"\"\n    Args:\n        mapper (Mapper[T_IN, T_OUT]): Mapper to apply to each sample.\n    \"\"\"\n    super().__init__()\n    self._mapper = mapper\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MemoCache","title":"<code>MemoCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Simple cache that stores key-value pairs in a dictionary, with no eviction policy or size limit, useful for memoization of functions with a bounded number of arguments.</p> <p>Avoid using this cache for large datasets or unbounded keys, as it will consume memory indefinitely.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MemoCache[K, V](Cache[K, V]):\n    \"\"\"Simple cache that stores key-value pairs in a dictionary, with no eviction policy\n    or size limit, useful for memoization of functions with a bounded number of\n    arguments.\n\n    Avoid using this cache for large datasets or unbounded keys, as it will consume\n    memory indefinitely.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the cache with an empty dictionary.\"\"\"\n        super().__init__()\n        self._memo: dict[K, V] = {}\n\n    def _clear(self) -&gt; None:\n        self._memo.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._memo.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        self._memo[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MemoCache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the cache with an empty dictionary.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the cache with an empty dictionary.\"\"\"\n    super().__init__()\n    self._memo: dict[K, V] = {}\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MemorizeEverythingOp","title":"<code>MemorizeEverythingOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches all samples in a dataset to avoid recomputation eagerly. This operator will block until all samples are computed and loaded in memory, so it may not be suitable for large datasets, or when memory is a concern.</p> <p>In these cases, consider using a Checkpoint, see the \"Cache\" section in the documentation for more information.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MemorizeEverythingOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches all samples in a dataset to avoid recomputation eagerly.\n    This operator will block until all samples are computed and loaded in memory, so it\n    may not be suitable for large datasets, or when memory is a concern.\n\n    In these cases, consider using a *Checkpoint*, see the \"Cache\" section in the\n    documentation for more information.\n    \"\"\"\n\n    def __init__(self, grabber: Grabber | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            grabber (Grabber | None, optional): Grabber to use for loading the samples\n                from the dataset. Defaults to None, in which case a new `Grabber` is\n                created.\n        \"\"\"\n        super().__init__()\n        self._grabber = grabber or Grabber()\n        self._cache_mapper: CacheMapper = CacheMapper()\n\n    def _get_sample(self, cache_id: str, idx: int) -&gt; Sample:\n        cache: Cache[int, Sample] = InheritedData.data[cache_id]\n        result = cache.get(idx)\n        assert result is not None\n        return result\n\n    def _finalize_cache(self, id_: str) -&gt; None:\n        if id_ in InheritedData.data:  # pragma: no branch\n            del InheritedData.data[id_]\n\n    def __call__(self, x: Dataset) -&gt; Dataset:\n        cache = MemoCache()\n        id_ = uuid4().hex\n        InheritedData.data[id_] = cache\n        for i, sample in self.loop(x, self._grabber, \"Caching\"):\n            cache.put(i, self._cache_mapper(i, sample))\n        dataset = LazyDataset(len(x), partial(self._get_sample, id_))\n        weakref.finalize(dataset, self._finalize_cache, id_=id_)\n        return dataset\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.MemorizeEverythingOp.__init__","title":"<code>__init__(grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>grabber</code> <code>Grabber | None</code> <p>Grabber to use for loading the samples from the dataset. Defaults to None, in which case a new <code>Grabber</code> is created.</p> <code>None</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, grabber: Grabber | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        grabber (Grabber | None, optional): Grabber to use for loading the samples\n            from the dataset. Defaults to None, in which case a new `Grabber` is\n            created.\n    \"\"\"\n    super().__init__()\n    self._grabber = grabber or Grabber()\n    self._cache_mapper: CacheMapper = CacheMapper()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.PadOp","title":"<code>PadOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that pads a dataset to a given length by repeating a specified sample.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class PadOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that pads a dataset to a given length by repeating a specified sample.\"\"\"\n\n    def __init__(self, length: int, pad_with: int = -1) -&gt; None:\n        \"\"\"\n        Args:\n            length (int): Length of the resulting dataset.\n            pad_with (int, optional): Index of the sample to use for padding. Defaults\n                to -1.\n        \"\"\"\n        super().__init__()\n        self._length = length\n        self._pad_with = pad_with\n\n    def _get_sample[T: Sample](self, x: Dataset[T], idx: int) -&gt; T:\n        if idx &lt; len(x):\n            return x[idx]\n        else:\n            return x[self._pad_with]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return LazyDataset(self._length, partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.PadOp.__init__","title":"<code>__init__(length, pad_with=-1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>Length of the resulting dataset.</p> required <code>pad_with</code> <code>int</code> <p>Index of the sample to use for padding. Defaults to -1.</p> <code>-1</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, length: int, pad_with: int = -1) -&gt; None:\n    \"\"\"\n    Args:\n        length (int): Length of the resulting dataset.\n        pad_with (int, optional): Index of the sample to use for padding. Defaults\n            to -1.\n    \"\"\"\n    super().__init__()\n    self._length = length\n    self._pad_with = pad_with\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.RRCache","title":"<code>RRCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Random Replacement (RR) cache that evicts a random key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the order of access to the keys is not known or when no suitable eviction policy is particularly effective.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class RRCache[K, V](Cache[K, V]):\n    \"\"\"Random Replacement (RR) cache that evicts a random key-value pair when the cache\n    is full and a new key-value pair is inserted. This cache is useful for scenarios\n    where the order of access to the keys is not known or when no suitable eviction\n    policy is particularly effective.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: list[K] = []\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            idx = random.randint(0, self._maxsize - 1)\n            prev_k = self._keys[idx]\n            self._keys[idx] = key\n            del self._mp[prev_k]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.RRCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: list[K] = []\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.RepeatOp","title":"<code>RepeatOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that repeats a dataset a given number of times.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class RepeatOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that repeats a dataset a given number of times.\"\"\"\n\n    def __init__(self, times: int, interleave: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            times (int): Number of times to repeat the dataset.\n            interleave (bool, optional): Whether to interleave the repeated samples.\n                Defaults to False.\n        \"\"\"\n        super().__init__()\n        self._times = times\n        self._interleave = interleave\n\n    def _index_fn(self, orig_len: int, x: int) -&gt; int:\n        return x % orig_len\n\n    def _index_fn_interleave(self, x: int) -&gt; int:\n        return x // self._times\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        index_fn = (\n            self._index_fn_interleave\n            if self._interleave\n            else partial(self._index_fn, len(x))\n        )\n        return LazyDataset(len(x) * self._times, x.get_sample, index_fn=index_fn)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.RepeatOp.__init__","title":"<code>__init__(times, interleave=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>times</code> <code>int</code> <p>Number of times to repeat the dataset.</p> required <code>interleave</code> <code>bool</code> <p>Whether to interleave the repeated samples. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, times: int, interleave: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        times (int): Number of times to repeat the dataset.\n        interleave (bool, optional): Whether to interleave the repeated samples.\n            Defaults to False.\n    \"\"\"\n    super().__init__()\n    self._times = times\n    self._interleave = interleave\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ReverseOp","title":"<code>ReverseOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that reverses the order of samples in a dataset.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class ReverseOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that reverses the order of samples in a dataset.\"\"\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x[::-1]\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ShuffleOp","title":"<code>ShuffleOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that shuffles the samples in a dataset in a random order.</p> Source code in <code>pipewine/operators/rand.py</code> <pre><code>class ShuffleOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that shuffles the samples in a dataset in a random order.\"\"\"\n\n    def _index_fn(self, index: list[int], x: int) -&gt; int:\n        return index[x]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        idx = list(range(len(x)))\n        shuffle(idx)\n        return LazyDataset(len(x), x.get_sample, index_fn=partial(self._index_fn, idx))\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SliceOp","title":"<code>SliceOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that slices a dataset based on the start, stop, and step arguments, similar to the plain Python slicing.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class SliceOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that slices a dataset based on the start, stop, and step arguments,\n    similar to the plain Python slicing.\n    \"\"\"\n\n    def __init__(\n        self, start: int | None = None, stop: int | None = None, step: int | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            start (int, optional): Start index of the slice. Defaults to None.\n            stop (int, optional): Stop index of the slice. Defaults to None.\n            step (int, optional): Step of the slice. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self._start = start\n        self._stop = stop\n        self._step = step\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x[slice(self._start or 0, self._stop or len(x), self._step or 1)]\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SliceOp.__init__","title":"<code>__init__(start=None, stop=None, step=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Start index of the slice. Defaults to None.</p> <code>None</code> <code>stop</code> <code>int</code> <p>Stop index of the slice. Defaults to None.</p> <code>None</code> <code>step</code> <code>int</code> <p>Step of the slice. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(\n    self, start: int | None = None, stop: int | None = None, step: int | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        start (int, optional): Start index of the slice. Defaults to None.\n        stop (int, optional): Stop index of the slice. Defaults to None.\n        step (int, optional): Step of the slice. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self._start = start\n    self._stop = stop\n    self._step = step\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SortOp","title":"<code>SortOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], Dataset[T]]</code></p> <p>Operator that sorts samples in a dataset based on a user-defined sorting function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class SortOp[T: Sample](DatasetOperator[Dataset[T], Dataset[T]]):\n    \"\"\"Operator that sorts samples in a dataset based on a user-defined sorting\n    function.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Callable[[int, T], ComparableT],\n        reverse: bool = False,\n        grabber: Grabber | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], ComparableT]): Function that takes the index and the\n                sample and returns a comparable value to use for sorting.\n            reverse (bool, optional): Whether to sort in reverse order. Defaults to False.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n        self._reverse = reverse\n\n    def __call__(self, x: Dataset[T]) -&gt; Dataset[T]:\n        keys: list[tuple[ComparableT, int]] = []\n        for i, sample in self.loop(x, self._grabber, name=\"Computing keys\"):\n            keys.append((self._fn(i, sample), i))\n\n        index = [x[1] for x in sorted(keys, reverse=self._reverse)]\n        return LazyDataset(len(x), x.get_sample, index_fn=index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SortOp.__init__","title":"<code>__init__(fn, reverse=False, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], ComparableT]</code> <p>Function that takes the index and the sample and returns a comparable value to use for sorting.</p> required <code>reverse</code> <code>bool</code> <p>Whether to sort in reverse order. Defaults to False.</p> <code>False</code> <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self,\n    fn: Callable[[int, T], ComparableT],\n    reverse: bool = False,\n    grabber: Grabber | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], ComparableT]): Function that takes the index and the\n            sample and returns a comparable value to use for sorting.\n        reverse (bool, optional): Whether to sort in reverse order. Defaults to False.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n    self._reverse = reverse\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SplitOp","title":"<code>SplitOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into multiple parts of given sizes. The sizes can be either integers or floats representing the proportions of the dataset to be included in each part.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class SplitOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into multiple parts of given sizes. The sizes\n    can be either integers or floats representing the proportions of the dataset to\n    be included in each part.\n    \"\"\"\n\n    def __init__(self, sizes: Sequence[int | float | None]) -&gt; None:\n        \"\"\"\n        Args:\n            sizes (Sequence[int | float | None]): Sizes of the splits. If a size is an\n                integer, it represents the number of samples in the split. If a size is\n                a float, it represents the proportion of the dataset to be included in\n                the split. If a size is None, it represents the remaining samples after\n                all other sizes have been computed. At most one size can be None.\n        \"\"\"\n        super().__init__()\n        self._sizes = sizes\n        all_ints = all(isinstance(x, int) for x in self._sizes if x is not None)\n        all_floats = all(isinstance(x, float) for x in self._sizes if x is not None)\n        assert all_ints or all_floats, \"Sizes must be all int or all float, not mixed.\"\n        self._null_idx = -1\n        self._total: int | float = 0\n        for i, size in enumerate(self._sizes):\n            if size is None:\n                if self._null_idx &gt;= 0:\n                    raise ValueError(\"At most one size can be None.\")\n                self._null_idx = i\n            else:\n                self._total += size\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        sizes: list[int] = []\n        for size in self._sizes:\n            if isinstance(size, float):\n                int_size = int(size * len(x))\n            elif size is None:\n                int_size = len(x) - int(self._total)\n            else:\n                int_size = size\n            sizes.append(int_size)\n\n        start = 0\n        splits = []\n        for size in sizes:\n            splits.append(x[start : start + size])\n            start += size\n        return splits\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.SplitOp.__init__","title":"<code>__init__(sizes)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Sequence[int | float | None]</code> <p>Sizes of the splits. If a size is an integer, it represents the number of samples in the split. If a size is a float, it represents the proportion of the dataset to be included in the split. If a size is None, it represents the remaining samples after all other sizes have been computed. At most one size can be None.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, sizes: Sequence[int | float | None]) -&gt; None:\n    \"\"\"\n    Args:\n        sizes (Sequence[int | float | None]): Sizes of the splits. If a size is an\n            integer, it represents the number of samples in the split. If a size is\n            a float, it represents the proportion of the dataset to be included in\n            the split. If a size is None, it represents the remaining samples after\n            all other sizes have been computed. At most one size can be None.\n    \"\"\"\n    super().__init__()\n    self._sizes = sizes\n    all_ints = all(isinstance(x, int) for x in self._sizes if x is not None)\n    all_floats = all(isinstance(x, float) for x in self._sizes if x is not None)\n    assert all_ints or all_floats, \"Sizes must be all int or all float, not mixed.\"\n    self._null_idx = -1\n    self._total: int | float = 0\n    for i, size in enumerate(self._sizes):\n        if size is None:\n            if self._null_idx &gt;= 0:\n                raise ValueError(\"At most one size can be None.\")\n            self._null_idx = i\n        else:\n            self._total += size\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ZipOp","title":"<code>ZipOp</code>","text":"<p>               Bases: <code>DatasetOperator[Sequence[Dataset], Dataset[T]]</code></p> <p>Operator that zips multiple datasets into a single dataset by merging the items of individual samples.</p> <p>Input datasets must have the same length and the samples must have disjoint items.</p> Source code in <code>pipewine/operators/merge.py</code> <pre><code>class ZipOp[T: Sample](DatasetOperator[Sequence[Dataset], Dataset[T]]):\n    \"\"\"Operator that zips multiple datasets into a single dataset by merging the items\n    of individual samples.\n\n    Input datasets must have the same length and the samples must have disjoint items.\n    \"\"\"\n\n    def __init__(self, out_type: type[T] | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            out_type (type[T] | None, optional): Type of the output samples. Defaults\n                to None (TypelessSample).\n        \"\"\"\n        super().__init__()\n        self._out_type = out_type or TypelessSample\n\n    def _get_sample(self, datasets: Sequence[Dataset[Sample]], idx: int) -&gt; T:\n        data: dict[str, Item] = {}\n        for dataset in datasets:\n            data.update(dataset[idx].items())\n        return self._out_type(**data)  # type: ignore\n\n    def __call__(self, x: Sequence[Dataset[Sample]]) -&gt; Dataset[T]:\n        len0 = len(x[0])\n        assert all(len(dataset) == len0 for dataset in x)\n        return LazyDataset(len(x[0]), partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.ZipOp.__init__","title":"<code>__init__(out_type=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>out_type</code> <code>type[T] | None</code> <p>Type of the output samples. Defaults to None (TypelessSample).</p> <code>None</code> Source code in <code>pipewine/operators/merge.py</code> <pre><code>def __init__(self, out_type: type[T] | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        out_type (type[T] | None, optional): Type of the output samples. Defaults\n            to None (TypelessSample).\n    \"\"\"\n    super().__init__()\n    self._out_type = out_type or TypelessSample\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.batch","title":"<code>batch(batch_size)</code>","text":"<p>Split a dataset into batches of the specified size.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef batch(\n    batch_size: Annotated[int, Option(..., \"--batch-size\", \"-b\", help=batch_size_help)],\n) -&gt; BatchOp:\n    \"\"\"Split a dataset into batches of the specified size.\"\"\"\n    return BatchOp(batch_size)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.cat","title":"<code>cat()</code>","text":"<p>Concatenate two or more datasets into a single dataset.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef cat() -&gt; CatOp:\n    \"\"\"Concatenate two or more datasets into a single dataset.\"\"\"\n    return CatOp()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.chunk","title":"<code>chunk(chunks)</code>","text":"<p>Split a dataset into N chunks.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef chunk(\n    chunks: Annotated[int, Option(..., \"--chunk\", \"-c\", help=\"The number of chunks.\")]\n) -&gt; ChunkOp:\n    \"\"\"Split a dataset into N chunks.\"\"\"\n    return ChunkOp(chunks)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.clone","title":"<code>clone()</code>","text":"<p>Copy a dataset, applying no changes to any sample.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef clone() -&gt; IdentityOp:\n    \"\"\"Copy a dataset, applying no changes to any sample.\"\"\"\n    return IdentityOp()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.cycle","title":"<code>cycle(length)</code>","text":"<p>Repeat the samples until a certain number of samples is reached.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef cycle(\n    length: Annotated[int, Option(..., \"--n\", \"-n\", help=\"Desired number of samples.\")]\n) -&gt; CycleOp:\n    \"\"\"Repeat the samples until a certain number of samples is reached.\"\"\"\n    return CycleOp(length)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.filter_","title":"<code>filter_(grabber, key, compare, target, negate=False)</code>","text":"<p>Keep only the samples that satisfy a certain logical comparison with a target.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli(name=\"filter\")\ndef filter_(\n    grabber: Grabber,\n    key: Annotated[str, Option(..., \"--key\", \"-k\", help=key_help)],\n    compare: Annotated[Compare, Option(..., \"--compare\", \"-c\", help=compare_help)],\n    target: Annotated[str, Option(..., \"--target\", \"-t\", help=target_help)],\n    negate: Annotated[bool, Option(..., \"--negate\", \"-n\", help=negate_help)] = False,\n) -&gt; FilterOp:\n    \"\"\"Keep only the samples that satisfy a certain logical comparison with a target.\"\"\"\n\n    def _filter_fn(idx: int, sample: Sample) -&gt; bool:\n        value = deep_get(sample, key)\n        target_ = (\n            type(value)(target)\n            if type(value) != bool\n            else str(target).lower() in [\"yes\", \"true\", \"y\", \"ok\", \"t\", \"1\"]\n        )\n        if compare == Compare.eq:\n            result = value == target_\n        elif compare == Compare.neq:\n            result = value != target_\n        elif compare == Compare.gt:\n            result = value &gt; target_\n        elif compare == Compare.lt:\n            result = value &lt; target_\n        elif compare == Compare.ge:\n            result = value &gt;= target_\n        else:\n            result = value &lt;= target_\n        return result\n\n    return FilterOp(_filter_fn, negate=negate, grabber=grabber)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.groupby","title":"<code>groupby(grabber, key)</code>","text":"<p>Group together samples with the same value associated to the specified key.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef groupby(\n    grabber: Grabber,\n    key: Annotated[str, Option(..., \"--key\", \"-k\", help=key_help)],\n) -&gt; GroupByOp:\n    \"\"\"Group together samples with the same value associated to the specified key.\"\"\"\n\n    def _groupby_fn(idx: int, sample: Sample) -&gt; str:\n        return str(deep_get(sample, key))\n\n    return GroupByOp(_groupby_fn, grabber=grabber)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.op_cli","title":"<code>op_cli(name=None)</code>","text":"<p>Decorator to generate a CLI command for a dataset operator.</p> <p>Decorated functions must follow the rules of Typer CLI commands, returning a <code>DatasetOperator</code> object.</p> <p>The decorated function must be correctly annotated with the type of operator it returns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the command. Defaults to None, in which case the function name is used.</p> <code>None</code> Source code in <code>pipewine/cli/ops.py</code> <pre><code>def op_cli[T](name: str | None = None) -&gt; Callable[[T], T]:\n    \"\"\"Decorator to generate a CLI command for a dataset operator.\n\n    Decorated functions must follow the rules of Typer CLI commands, returning a\n    `DatasetOperator` object.\n\n    The decorated function must be correctly annotated with the type of operator it\n    returns.\n\n    Args:\n        name (str, optional): The name of the command. Defaults to None, in which case\n            the function name is used.\n    \"\"\"\n\n    return partial(_generate_op_command, name=name)  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.pad","title":"<code>pad(length, pad_with=-1)</code>","text":"<p>Pad a dataset until it reaches a specified length.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef pad(\n    length: Annotated[int, Option(..., \"--length\", \"-l\", help=length_help)],\n    pad_with: Annotated[int, Option(..., \"--pad-width\", \"-p\", help=pad_with_help)] = -1,\n) -&gt; PadOp:\n    \"\"\"Pad a dataset until it reaches a specified length.\"\"\"\n    return PadOp(length, pad_with=pad_with)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.repeat","title":"<code>repeat(times, interleave=False)</code>","text":"<p>Repeat a dataset N times replicating the samples.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef repeat(\n    times: Annotated[int, Option(..., \"--times\", \"-t\", help=times_help)],\n    interleave: Annotated[\n        bool, Option(..., \"--interleave\", \"-I\", help=interleave_help)\n    ] = False,\n) -&gt; RepeatOp:\n    \"\"\"Repeat a dataset N times replicating the samples.\"\"\"\n    return RepeatOp(times, interleave=interleave)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.reverse","title":"<code>reverse()</code>","text":"<p>Reverse the order of the samples.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef reverse() -&gt; ReverseOp:\n    \"\"\"Reverse the order of the samples.\"\"\"\n    return ReverseOp()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.shuffle","title":"<code>shuffle(seed=-1)</code>","text":"<p>Shuffle the samples of a dataset in random order.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef shuffle(\n    seed: Annotated[int, Option(..., \"--seed\", \"-s\", help=\"Random seed.\")] = -1\n) -&gt; ShuffleOp:\n    \"\"\"Shuffle the samples of a dataset in random order.\"\"\"\n    if seed &gt;= 0:\n        random.seed(seed)\n    return ShuffleOp()\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.slice_","title":"<code>slice_(start=None, stop=None, step=None)</code>","text":"<p>Slice a dataset as you would do with any Python sequence.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli(name=\"slice\")\ndef slice_(\n    start: Annotated[int, Option(help=\"Start index.\")] = None,  # type: ignore\n    stop: Annotated[int, Option(help=\"Stop index.\")] = None,  # type: ignore\n    step: Annotated[int, Option(help=\"Slice step size.\")] = None,  # type: ignore\n) -&gt; SliceOp:\n    \"\"\"Slice a dataset as you would do with any Python sequence.\"\"\"\n    return SliceOp(start=start, stop=stop, step=step)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.sort","title":"<code>sort(grabber, key, reverse=False)</code>","text":"<p>Sort samples by non-decreasing values associated with the specified key.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef sort(\n    grabber: Grabber,\n    key: Annotated[str, Option(..., \"--key\", \"-k\", help=key_help)],\n    reverse: Annotated[bool, Option(..., \"--reverse\", \"-r\", help=reverse_help)] = False,\n) -&gt; SortOp:\n    \"\"\"Sort samples by non-decreasing values associated with the specified key.\"\"\"\n\n    def _sort_fn(idx: int, sample: Sample) -&gt; Any:\n        return deep_get(sample, key)\n\n    return SortOp(_sort_fn, reverse=reverse, grabber=grabber)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.split","title":"<code>split(sizes)</code>","text":"<p>Split a dataset into parts with custom size.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli()\ndef split(\n    sizes: Annotated[list[str], Option(..., \"-s\", \"--sizes\", help=splits_help)]\n) -&gt; SplitOp:\n    \"\"\"Split a dataset into parts with custom size.\"\"\"\n    parsed_sizes = []\n    for x in sizes:\n        if x == \"null\":\n            parsed = None\n        elif \".\" in x:\n            parsed = float(x)\n        else:\n            parsed = int(x)\n        parsed_sizes.append(parsed)\n    return SplitOp(parsed_sizes)\n</code></pre>"},{"location":"autoapi/pipewine/cli/ops/#pipewine.cli.ops.zip_","title":"<code>zip_()</code>","text":"<p>Zip two or more datasets of the same length by merging the individual samples.</p> Source code in <code>pipewine/cli/ops.py</code> <pre><code>@op_cli(name=\"zip\")\ndef zip_() -&gt; ZipOp:\n    \"\"\"Zip two or more datasets of the same length by merging the individual samples.\"\"\"\n    return ZipOp()\n</code></pre>"},{"location":"autoapi/pipewine/cli/sinks/","title":"sinks","text":"<p>CLI for dataset sinks.</p>"},{"location":"autoapi/pipewine/cli/sinks/#pipewine.cli.sinks.SinkCLIRegistry","title":"<code>SinkCLIRegistry</code>","text":"<p>Registry for known types of dataset sinks.</p> Source code in <code>pipewine/cli/sinks.py</code> <pre><code>class SinkCLIRegistry:\n    \"\"\"Registry for known types of dataset sinks.\"\"\"\n\n    registered: dict[str, Callable[[str, Grabber], DatasetSink]] = {}\n</code></pre>"},{"location":"autoapi/pipewine/cli/sinks/#pipewine.cli.sinks.sink_cli","title":"<code>sink_cli(name=None)</code>","text":"<p>Decorator to register a type of dataset sink to the CLI.</p> <p>The decorated function must take a string and a grabber and return a dataset sink that can be called with a single dataset.</p> <p>The decorated function must be correctly annotated with the type of dataset sink it returns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the sink. Defaults to None, in which case the function name is used.</p> <code>None</code> Source code in <code>pipewine/cli/sinks.py</code> <pre><code>def sink_cli[\n    T: Callable[[str, Grabber], DatasetSink]\n](name: str | None = None) -&gt; Callable[[T], T]:\n    \"\"\"Decorator to register a type of dataset sink to the CLI.\n\n    The decorated function must take a string and a grabber and return a dataset sink\n    that can be called with a single dataset.\n\n    The decorated function must be correctly annotated with the type of dataset sink it\n    returns.\n\n    Args:\n        name (str, optional): The name of the sink. Defaults to None, in which case\n            the function name is used.\n    \"\"\"\n\n    def inner(fn: T) -&gt; T:\n        fn_name = name or fn.__name__\n        SinkCLIRegistry.registered[fn_name] = fn\n        return fn\n\n    return inner\n</code></pre>"},{"location":"autoapi/pipewine/cli/sinks/#pipewine.cli.sinks.underfolder","title":"<code>underfolder(text, grabber)</code>","text":"<p>PATH[,OVERWRITE=forbid[,COPY_POLICY=hard_link]]</p> <p>PATH: Path to the dataset to write. OVERWRITE: What happens if the destination path is not empty. One of:     - \"forbid\" - Fail if the folder already exists.     - \"allow_if_empty\" - Allow overwrite if the folder exists but it is empty.     - \"allow_new_files\" - Only allow the creation of new files.     - \"overwrite_files\" - If a file already exists, delete it before writing.     - \"overwrite\" - If the folder already exists, delete if before writing. COPY_POLICY: What happens if the library detects replication of existing data. One of:     - \"rewrite\" - Do as if no copy was detected. Serialize the data and write.     - \"replicate\" - Avoid the serialization but copy the original file contents.     - \"symbolic_link\" - Create a symlink to the original file.     - \"hard_link\" - Create a link to the same inode of the original file.</p> Source code in <code>pipewine/cli/sinks.py</code> <pre><code>@sink_cli()\ndef underfolder(text: str, grabber: Grabber) -&gt; UnderfolderSink:\n    \"\"\"PATH[,OVERWRITE=forbid[,COPY_POLICY=hard_link]]\n\n    PATH: Path to the dataset to write.\n    OVERWRITE: What happens if the destination path is not empty. One of:\n        - \"forbid\" - Fail if the folder already exists.\n        - \"allow_if_empty\" - Allow overwrite if the folder exists but it is empty.\n        - \"allow_new_files\" - Only allow the creation of new files.\n        - \"overwrite_files\" - If a file already exists, delete it before writing.\n        - \"overwrite\" - If the folder already exists, delete if before writing.\n    COPY_POLICY: What happens if the library detects replication of existing data. One of:\n        - \"rewrite\" - Do as if no copy was detected. Serialize the data and write.\n        - \"replicate\" - Avoid the serialization but copy the original file contents.\n        - \"symbolic_link\" - Create a symlink to the original file.\n        - \"hard_link\" - Create a link to the same inode of the original file.\n    \"\"\"\n    path, ow_policy, copy_policy = _split_and_parse_underfolder_text(text)\n    return UnderfolderSink(\n        Path(path), grabber=grabber, overwrite_policy=ow_policy, copy_policy=copy_policy\n    )\n</code></pre>"},{"location":"autoapi/pipewine/cli/sources/","title":"sources","text":"<p>CLI for dataset sources.</p>"},{"location":"autoapi/pipewine/cli/sources/#pipewine.cli.sources.SourceCLIRegistry","title":"<code>SourceCLIRegistry</code>","text":"<p>Registry for known types of dataset sources.</p> Source code in <code>pipewine/cli/sources.py</code> <pre><code>class SourceCLIRegistry:\n    \"\"\"Registry for known types of dataset sources.\"\"\"\n\n    registered: dict[str, Callable[[str, Grabber, type[Sample]], DatasetSource]] = {}\n</code></pre>"},{"location":"autoapi/pipewine/cli/sources/#pipewine.cli.sources.images_folder","title":"<code>images_folder(text, grabber, sample_type)</code>","text":"<p>PATH[,recursive]: Path to the folder where the images are stored.</p> Source code in <code>pipewine/cli/sources.py</code> <pre><code>@source_cli()\ndef images_folder(\n    text: str, grabber: Grabber, sample_type: type[Sample]\n) -&gt; ImagesFolderSource:\n    \"\"\"PATH[,recursive]: Path to the folder where the images are stored.\"\"\"\n    path, _, recursive = text.rpartition(\",\")\n    if recursive == \"recursive\":\n        return ImagesFolderSource(Path(path), recursive=True)\n    else:\n        return ImagesFolderSource(Path(text))\n</code></pre>"},{"location":"autoapi/pipewine/cli/sources/#pipewine.cli.sources.source_cli","title":"<code>source_cli(name=None)</code>","text":"<p>Decorator to register a type of dataset source to the CLI.</p> <p>The decorated function must take a string, a grabber, and a sample type and return a dataset source that can be called with a single dataset.</p> <p>The decorated function must be correctly annotated with the type of dataset source it returns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the source. Defaults to None, in which case the function name is used.</p> <code>None</code> Source code in <code>pipewine/cli/sources.py</code> <pre><code>def source_cli[T: Callable[[str, Grabber, type[Sample]], DatasetSource]](\n    name: str | None = None,\n) -&gt; Callable[[T], T]:\n    \"\"\"Decorator to register a type of dataset source to the CLI.\n\n    The decorated function must take a string, a grabber, and a sample type and return a\n    dataset source that can be called with a single dataset.\n\n    The decorated function must be correctly annotated with the type of dataset source it\n    returns.\n\n    Args:\n        name (str, optional): The name of the source. Defaults to None, in which case\n            the function name is used.\n    \"\"\"\n\n    def inner(fn: T) -&gt; T:\n        fn_name = name or fn.__name__\n        SourceCLIRegistry.registered[fn_name] = fn\n        return fn\n\n    return inner\n</code></pre>"},{"location":"autoapi/pipewine/cli/sources/#pipewine.cli.sources.underfolder","title":"<code>underfolder(text, grabber, sample_type)</code>","text":"<p>PATH: Path to the dataset folder.</p> Source code in <code>pipewine/cli/sources.py</code> <pre><code>@source_cli()\ndef underfolder(\n    text: str, grabber: Grabber, sample_type: type[Sample]\n) -&gt; UnderfolderSource:\n    \"\"\"PATH: Path to the dataset folder.\"\"\"\n    return UnderfolderSource(Path(text), sample_type=sample_type)\n</code></pre>"},{"location":"autoapi/pipewine/cli/utils/","title":"utils","text":"<p>CLI utilities for Pipewine.</p>"},{"location":"autoapi/pipewine/cli/utils/#pipewine.cli.utils.deep_get","title":"<code>deep_get(sample, key)</code>","text":"<p>Get a value from a nested dictionary using a dot-separated key.</p> Source code in <code>pipewine/cli/utils.py</code> <pre><code>def deep_get(sample: Sample, key: str) -&gt; Any:\n    \"\"\"Get a value from a nested dictionary using a dot-separated key.\"\"\"\n    sep = \".\"\n    sub_keys = key.split(sep)\n    item_key, other_keys = sub_keys[0], deque(sub_keys[1:])\n    current = sample[item_key]()\n    while len(other_keys) &gt; 0:\n        current_key = other_keys.popleft()\n        if isinstance(current, Sequence):\n            current = current[int(current_key)]\n        else:\n            current = current[current_key]\n    return current\n</code></pre>"},{"location":"autoapi/pipewine/cli/utils/#pipewine.cli.utils.parse_grabber","title":"<code>parse_grabber(value)</code>","text":"<p>Parse a grabber from a string.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>A string representing the grabber.</p> required <p>Returns:</p> Name Type Description <code>Grabber</code> <code>Grabber</code> <p>The parsed grabber</p> Source code in <code>pipewine/cli/utils.py</code> <pre><code>def parse_grabber(value: str) -&gt; Grabber:\n    \"\"\"Parse a grabber from a string.\n\n    Args:\n        value (str): A string representing the grabber.\n\n    Returns:\n        Grabber: The parsed grabber\n    \"\"\"\n    sep = \",\"\n    if sep in value:\n        worker_str, _, prefetch_str = value.partition(sep)\n        return Grabber(num_workers=int(worker_str), prefetch=int(prefetch_str))\n    else:\n        return Grabber(num_workers=int(value))\n</code></pre>"},{"location":"autoapi/pipewine/cli/utils/#pipewine.cli.utils.parse_sink","title":"<code>parse_sink(format_, text, grabber)</code>","text":"<p>Parse a dataset sink from a format string and a text string.</p> <p>Parameters:</p> Name Type Description Default <code>format_</code> <code>str</code> <p>A string representing the format of the dataset sink.</p> required <code>text</code> <code>str</code> <p>The text to parse into a dataset sink.</p> required <code>grabber</code> <code>Grabber</code> <p>The grabber to use with the dataset sink.</p> required <p>Returns:</p> Name Type Description <code>DatasetSink</code> <code>DatasetSink</code> <p>The parsed dataset sink.</p> Source code in <code>pipewine/cli/utils.py</code> <pre><code>def parse_sink(format_: str, text: str, grabber: Grabber) -&gt; DatasetSink:\n    \"\"\"Parse a dataset sink from a format string and a text string.\n\n    Args:\n        format_ (str): A string representing the format of the dataset sink.\n        text (str): The text to parse into a dataset sink.\n        grabber (Grabber): The grabber to use with the dataset sink.\n\n    Returns:\n        DatasetSink: The parsed dataset sink.\n    \"\"\"\n    return _parse_source_or_sink(format_, text, SinkCLIRegistry, grabber)\n</code></pre>"},{"location":"autoapi/pipewine/cli/utils/#pipewine.cli.utils.parse_source","title":"<code>parse_source(format_, text, grabber, sample_type)</code>","text":"<p>Parse a dataset source from a format string and a text string.</p> <p>Parameters:</p> Name Type Description Default <code>format_</code> <code>str</code> <p>A string representing the format of the dataset source.</p> required <code>text</code> <code>str</code> <p>The text to parse into a dataset source.</p> required <code>grabber</code> <code>Grabber</code> <p>The grabber to use with the dataset source.</p> required <code>sample_type</code> <code>type[Sample]</code> <p>The type of sample to use with the dataset source.</p> required <p>Returns:</p> Name Type Description <code>DatasetSource</code> <code>DatasetSource</code> <p>The parsed dataset source.</p> Source code in <code>pipewine/cli/utils.py</code> <pre><code>def parse_source(\n    format_: str,\n    text: str,\n    grabber: Grabber,\n    sample_type: type[Sample],\n) -&gt; DatasetSource:\n    \"\"\"Parse a dataset source from a format string and a text string.\n\n    Args:\n        format_ (str): A string representing the format of the dataset source.\n        text (str): The text to parse into a dataset source.\n        grabber (Grabber): The grabber to use with the dataset source.\n        sample_type (type[Sample]): The type of sample to use with the dataset source.\n\n    Returns:\n        DatasetSource: The parsed dataset source.\n    \"\"\"\n    return _parse_source_or_sink(format_, text, SourceCLIRegistry, grabber, sample_type)\n</code></pre>"},{"location":"autoapi/pipewine/cli/utils/#pipewine.cli.utils.run_cli_workflow","title":"<code>run_cli_workflow(workflow, tui=True)</code>","text":"<p>Wrapper around the <code>run_workflow</code> function that handles exceptions and prints a panel with the workflow status.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>The workflow to run.</p> required <code>tui</code> <code>bool</code> <p>Whether to use a text-based user interface. Defaults to True</p> <code>True</code> Source code in <code>pipewine/cli/utils.py</code> <pre><code>def run_cli_workflow(workflow: Workflow, tui: bool = True) -&gt; None:\n    \"\"\"Wrapper around the `run_workflow` function that handles exceptions and prints a\n    panel with the workflow status.\n\n    Args:\n        workflow (Workflow): The workflow to run.\n        tui (bool, optional): Whether to use a text-based user interface. Defaults to\n            True\n    \"\"\"\n    start_time = datetime.now()\n    try:\n        run_workflow(workflow, tracker=CursesTracker() if tui else None)\n    except KeyboardInterrupt:\n        _print_workflow_panel(start_time, \"Workflow canceled.\", \"bold bright_black\")\n        exit(1)\n    except Exception:\n        _print_workflow_panel(\n            start_time, \"Workflow failed.\", \"bold red\", body=traceback.format_exc()\n        )\n        exit(1)\n\n    _print_workflow_panel(start_time, \"Workflow completed successfully.\", \"bold green\")\n</code></pre>"},{"location":"autoapi/pipewine/cli/workflows/","title":"workflows","text":"<p>CLI for running workflows.</p>"},{"location":"autoapi/pipewine/cli/workflows/#pipewine.cli.workflows.wf_app","title":"<code>wf_app = Typer(callback=_wf_callback, name='wf', help='Run a pipewine workflow.', invoke_without_command=True, no_args_is_help=True)</code>  <code>module-attribute</code>","text":"<p>The main CLI application for workflows.</p>"},{"location":"autoapi/pipewine/cli/workflows/#pipewine.cli.workflows.wf_cli","title":"<code>wf_cli(name=None)</code>","text":"<p>Decorator to generate a CLI command for a dataset workflow.</p> <p>Decorated functions must follwo the rules of Typer CLI commands, returning a <code>Workflow</code> object to be run.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the command. Defaults to None, in which case the function name is used.</p> <code>None</code> Source code in <code>pipewine/cli/workflows.py</code> <pre><code>def wf_cli[T](name: str | None = None) -&gt; Callable[[T], T]:\n    \"\"\"Decorator to generate a CLI command for a dataset workflow.\n\n    Decorated functions must follwo the rules of Typer CLI commands, returning a\n    `Workflow` object to be run.\n\n    Args:\n        name (str, optional): The name of the command. Defaults to None, in which case\n            the function name is used.\n    \"\"\"\n    return functools.partial(_generate_wf_command, name=name)  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/mappers/","title":"mappers","text":"<p>Package for all Pipewine built-in mappers.</p>"},{"location":"autoapi/pipewine/mappers/base/","title":"base","text":"<p><code>Mapper</code> base class definition.</p>"},{"location":"autoapi/pipewine/mappers/base/#pipewine.mappers.base.Mapper","title":"<code>Mapper</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all Pipewine mappers, which are functions that transform individual samples of a dataset.</p> <p>All mapper classes must inherit from this class and implement the <code>__call__</code> method.</p> <p>Mapper classes can be parameterized by the types of the input and output samples, <code>T_IN</code> and <code>T_OUT</code>, respectively. These types must be subclasses of <code>Sample</code>.</p> Source code in <code>pipewine/mappers/base.py</code> <pre><code>class Mapper[T_IN: Sample, T_OUT: Sample](ABC):\n    \"\"\"Base class for all Pipewine mappers, which are functions that transform\n    individual samples of a dataset.\n\n    All mapper classes must inherit from this class and implement the `__call__`\n    method.\n\n    Mapper classes can be parameterized by the types of the input and output\n    samples, `T_IN` and `T_OUT`, respectively. These types must be subclasses of\n    `Sample`.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n        \"\"\"Transform a sample of type `T_IN` into another sample of type `T_OUT`.\n\n        Args:\n            idx (int): The index of the sample in the dataset.\n            x (T_IN): The input sample to be transformed.\n\n        Returns:\n            T_OUT: The transformed output sample.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/mappers/base/#pipewine.mappers.base.Mapper.__call__","title":"<code>__call__(idx, x)</code>  <code>abstractmethod</code>","text":"<p>Transform a sample of type <code>T_IN</code> into another sample of type <code>T_OUT</code>.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>The index of the sample in the dataset.</p> required <code>x</code> <code>T_IN</code> <p>The input sample to be transformed.</p> required <p>Returns:</p> Name Type Description <code>T_OUT</code> <code>T_OUT</code> <p>The transformed output sample.</p> Source code in <code>pipewine/mappers/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n    \"\"\"Transform a sample of type `T_IN` into another sample of type `T_OUT`.\n\n    Args:\n        idx (int): The index of the sample in the dataset.\n        x (T_IN): The input sample to be transformed.\n\n    Returns:\n        T_OUT: The transformed output sample.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/mappers/cache/","title":"cache","text":"<p>Mappers related to item caching.</p>"},{"location":"autoapi/pipewine/mappers/cache/#pipewine.mappers.cache.CacheMapper","title":"<code>CacheMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that replaces all items in a sample with <code>CachedItem</code> instances wrapping the original items.</p> Source code in <code>pipewine/mappers/cache.py</code> <pre><code>class CacheMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that replaces all items in a sample with `CachedItem` instances wrapping\n    the original items.\n    \"\"\"\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        return x.with_items(\n            **{\n                k: v if isinstance(v, CachedItem) else CachedItem(v)\n                for k, v in x.items()\n            }\n        )\n</code></pre>"},{"location":"autoapi/pipewine/mappers/compose/","title":"compose","text":"<p>Mappers that wrap or compose other mappers.</p>"},{"location":"autoapi/pipewine/mappers/compose/#pipewine.mappers.compose.ComposeMapper","title":"<code>ComposeMapper</code>","text":"<p>               Bases: <code>Mapper[T_IN, T_OUT]</code></p> <p>Mapper that composes multiple mappers into a single mapper, calling each mapper in sequence, similar to function composition.</p> <p>When composing multiple mappers, the output type of each mapper must match the input type of the next mapper. This class is hinted in such a way that the type checker can infer the input and output types of the final composed mapper.</p> Source code in <code>pipewine/mappers/compose.py</code> <pre><code>class ComposeMapper[T_IN: Sample, T_OUT: Sample](Mapper[T_IN, T_OUT]):\n    \"\"\"Mapper that composes multiple mappers into a single mapper, calling each\n    mapper in sequence, similar to function composition.\n\n    When composing multiple mappers, the output type of each mapper must match the\n    input type of the next mapper. This class is hinted in such a way that the type\n    checker can infer the input and output types of the final composed mapper.\n    \"\"\"\n\n    def __init__(\n        self,\n        mappers: (\n            Mapper[T_IN, T_OUT]\n            | tuple[Mapper[T_IN, T_OUT]]\n            | tuple[Mapper[T_IN, A], *Ts, Mapper[B, T_OUT]]\n        ),\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            mappers (Mapper[T_IN, T_OUT] | tuple[Mapper, ...]): Mapper or tuple of\n                mappers to compose. If a single mapper is provided, it is treated as a\n                tuple with a single element.\n        \"\"\"\n        super().__init__()\n        if not isinstance(mappers, tuple):\n            mappers_t = (mappers,)\n        else:\n            mappers_t = mappers  # type: ignore\n        self._mappers = mappers_t\n\n    def __call__(self, idx: int, x: T_IN) -&gt; T_OUT:\n        temp = x\n        for mapper in self._mappers:\n            temp = cast(Mapper, mapper)(idx, temp)\n        return cast(T_OUT, temp)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/compose/#pipewine.mappers.compose.ComposeMapper.__init__","title":"<code>__init__(mappers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mappers</code> <code>Mapper[T_IN, T_OUT] | tuple[Mapper, ...]</code> <p>Mapper or tuple of mappers to compose. If a single mapper is provided, it is treated as a tuple with a single element.</p> required Source code in <code>pipewine/mappers/compose.py</code> <pre><code>def __init__(\n    self,\n    mappers: (\n        Mapper[T_IN, T_OUT]\n        | tuple[Mapper[T_IN, T_OUT]]\n        | tuple[Mapper[T_IN, A], *Ts, Mapper[B, T_OUT]]\n    ),\n) -&gt; None:\n    \"\"\"\n    Args:\n        mappers (Mapper[T_IN, T_OUT] | tuple[Mapper, ...]): Mapper or tuple of\n            mappers to compose. If a single mapper is provided, it is treated as a\n            tuple with a single element.\n    \"\"\"\n    super().__init__()\n    if not isinstance(mappers, tuple):\n        mappers_t = (mappers,)\n    else:\n        mappers_t = mappers  # type: ignore\n    self._mappers = mappers_t\n</code></pre>"},{"location":"autoapi/pipewine/mappers/crypto/","title":"crypto","text":"<p>Mappers for cryptographic operations.</p>"},{"location":"autoapi/pipewine/mappers/crypto/#pipewine.mappers.crypto.HashMapper","title":"<code>HashMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, HashedSample]</code></p> <p>Compute the hash of a sample based on the selected items.</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>class HashMapper(Mapper[Sample, HashedSample]):\n    \"\"\"Compute the hash of a sample based on the selected items.\"\"\"\n\n    def __init__(\n        self, algorithm: str = \"sha256\", keys: str | Sequence[str] | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            algorithm (str, optional): Hash algorithm to use, must be one of the\n                algorithms available in `hashlib.algorithms_available`. Defaults to\n                \"sha256\".\n            keys (str | Sequence[str] | None, optional): Keys of the sample to use for\n                computing the hash. If a string is provided, it is treated as a single\n                key. If a sequence is provided, it is treated as a list of keys. If\n                `None` is provided, all keys in the sample are used. Defaults to `None`.\n\n        Raises:\n            ValueError: If the provided algorithm is not available in `hashlib` or if it\n                requires parameters. Currently, the only two algorithms that require\n                parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".\n        \"\"\"\n        super().__init__()\n        algorithms_with_parameters = [\"shake_128\", \"shake_256\"]\n        if (\n            algorithm not in hashlib.algorithms_available\n            or algorithm in algorithms_with_parameters\n        ):\n            raise ValueError(f\"Invalid algorithm: {algorithm}\")\n        self._algorithm = algorithm\n        self._keys = keys\n\n    def __call__(self, idx: int, x: Sample) -&gt; HashedSample:\n        hash_ = self._compute_sample_hash(x)\n        return HashedSample(hash=MemoryItem(hash_, YAMLParser(type_=str)))\n\n    def _compute_item_hash(self, data: Any) -&gt; str:\n        return hashlib.new(self._algorithm, pickle.dumps(data)).hexdigest()\n\n    def _compute_sample_hash(self, sample: Sample) -&gt; str:\n        keys: Iterable[str]\n        if isinstance(self._keys, str):\n            keys = [self._keys]\n        elif isinstance(self._keys, Sequence):\n            keys = self._keys\n        else:\n            keys = sorted(list(sample.keys()))\n        return \"\".join([self._compute_item_hash(sample[k]()) for k in keys])  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/mappers/crypto/#pipewine.mappers.crypto.HashMapper.__init__","title":"<code>__init__(algorithm='sha256', keys=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>algorithm</code> <code>str</code> <p>Hash algorithm to use, must be one of the algorithms available in <code>hashlib.algorithms_available</code>. Defaults to \"sha256\".</p> <code>'sha256'</code> <code>keys</code> <code>str | Sequence[str] | None</code> <p>Keys of the sample to use for computing the hash. If a string is provided, it is treated as a single key. If a sequence is provided, it is treated as a list of keys. If <code>None</code> is provided, all keys in the sample are used. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided algorithm is not available in <code>hashlib</code> or if it requires parameters. Currently, the only two algorithms that require parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>def __init__(\n    self, algorithm: str = \"sha256\", keys: str | Sequence[str] | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        algorithm (str, optional): Hash algorithm to use, must be one of the\n            algorithms available in `hashlib.algorithms_available`. Defaults to\n            \"sha256\".\n        keys (str | Sequence[str] | None, optional): Keys of the sample to use for\n            computing the hash. If a string is provided, it is treated as a single\n            key. If a sequence is provided, it is treated as a list of keys. If\n            `None` is provided, all keys in the sample are used. Defaults to `None`.\n\n    Raises:\n        ValueError: If the provided algorithm is not available in `hashlib` or if it\n            requires parameters. Currently, the only two algorithms that require\n            parameters (and thus are not supported) are \"shake_128\" and \"shake_256\".\n    \"\"\"\n    super().__init__()\n    algorithms_with_parameters = [\"shake_128\", \"shake_256\"]\n    if (\n        algorithm not in hashlib.algorithms_available\n        or algorithm in algorithms_with_parameters\n    ):\n        raise ValueError(f\"Invalid algorithm: {algorithm}\")\n    self._algorithm = algorithm\n    self._keys = keys\n</code></pre>"},{"location":"autoapi/pipewine/mappers/crypto/#pipewine.mappers.crypto.HashedSample","title":"<code>HashedSample</code>","text":"<p>               Bases: <code>TypedSample</code></p> <p>Sample type to represent the hash of a sample.</p> Source code in <code>pipewine/mappers/crypto.py</code> <pre><code>class HashedSample(TypedSample):\n    \"\"\"Sample type to represent the hash of a sample.\"\"\"\n\n    hash: Item[str]\n    \"\"\"The hash of the sample.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/mappers/crypto/#pipewine.mappers.crypto.HashedSample.hash","title":"<code>hash</code>  <code>instance-attribute</code>","text":"<p>The hash of the sample.</p>"},{"location":"autoapi/pipewine/mappers/item_transform/","title":"item_transform","text":"<p>Mappers for manipulating the format and sharedness of items in a sample.</p>"},{"location":"autoapi/pipewine/mappers/item_transform/#pipewine.mappers.item_transform.ConvertMapper","title":"<code>ConvertMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that changes the parser of selected items in a sample, allowing for conversion between different data formats, e.g., from JSON to YAML or from PNG to JPEG.</p> Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>class ConvertMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that changes the parser of selected items in a sample, allowing for\n    conversion between different data formats, e.g., from JSON to YAML or from PNG to\n    JPEG.\n    \"\"\"\n\n    def __init__(self, parsers: Mapping[str, Parser]) -&gt; None:\n        \"\"\"\n        Args:\n            parsers (Mapping[str, Parser]): Mapping of item keys to parsers to use for\n                converting the items.\n        \"\"\"\n        super().__init__()\n        self._parsers = parsers\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        to_modify: dict[str, Item] = {}\n        for k, parser in self._parsers.items():\n            if k in x:\n                to_modify[k] = x[k].with_parser(parser)\n        return x.with_items(**to_modify)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/item_transform/#pipewine.mappers.item_transform.ConvertMapper.__init__","title":"<code>__init__(parsers)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parsers</code> <code>Mapping[str, Parser]</code> <p>Mapping of item keys to parsers to use for converting the items.</p> required Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>def __init__(self, parsers: Mapping[str, Parser]) -&gt; None:\n    \"\"\"\n    Args:\n        parsers (Mapping[str, Parser]): Mapping of item keys to parsers to use for\n            converting the items.\n    \"\"\"\n    super().__init__()\n    self._parsers = parsers\n</code></pre>"},{"location":"autoapi/pipewine/mappers/item_transform/#pipewine.mappers.item_transform.ShareMapper","title":"<code>ShareMapper</code>","text":"<p>               Bases: <code>Mapper[T, T]</code></p> <p>Mapper that changes the sharedness of selected items in a sample, allowing for sharing or unsharing items between samples.</p> Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>class ShareMapper[T: Sample](Mapper[T, T]):\n    \"\"\"Mapper that changes the sharedness of selected items in a sample, allowing for\n    sharing or unsharing items between samples.\n    \"\"\"\n\n    def __init__(self, share: Iterable[str], unshare: Iterable[str]) -&gt; None:\n        \"\"\"\n        Args:\n            share (Iterable[str]): Keys of the items to share between samples.\n            unshare (Iterable[str]): Keys of the items to unshare between samples.\n        \"\"\"\n        super().__init__()\n        if set(share) &amp; set(unshare):\n            raise ValueError(\"The keys in 'share' and 'unshare' must be disjoint.\")\n        self._share = share\n        self._unshare = unshare\n\n    def __call__(self, idx: int, x: T) -&gt; T:\n        to_modify: dict[str, Item] = {}\n        for k, item in x.items():\n            if not item.is_shared and k in self._share:\n                to_modify[k] = item.with_sharedness(True)\n            elif item.is_shared and k in self._unshare:\n                to_modify[k] = item.with_sharedness(False)\n        return x.with_items(**to_modify)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/item_transform/#pipewine.mappers.item_transform.ShareMapper.__init__","title":"<code>__init__(share, unshare)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>share</code> <code>Iterable[str]</code> <p>Keys of the items to share between samples.</p> required <code>unshare</code> <code>Iterable[str]</code> <p>Keys of the items to unshare between samples.</p> required Source code in <code>pipewine/mappers/item_transform.py</code> <pre><code>def __init__(self, share: Iterable[str], unshare: Iterable[str]) -&gt; None:\n    \"\"\"\n    Args:\n        share (Iterable[str]): Keys of the items to share between samples.\n        unshare (Iterable[str]): Keys of the items to unshare between samples.\n    \"\"\"\n    super().__init__()\n    if set(share) &amp; set(unshare):\n        raise ValueError(\"The keys in 'share' and 'unshare' must be disjoint.\")\n    self._share = share\n    self._unshare = unshare\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/","title":"key_transform","text":"<p>Mapper classes to manipulate sample keys by adding, removing, or renaming them.</p>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.DuplicateItemMapper","title":"<code>DuplicateItemMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Duplicates an item in the sample.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class DuplicateItemMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Duplicates an item in the sample.\"\"\"\n\n    def __init__(self, source_key: str, destination_key: str) -&gt; None:\n        \"\"\"\n        Args:\n            source_key (str): The key of the item to duplicate.\n            destination_key (str): The key of the duplicated item.\n        \"\"\"\n        super().__init__()\n        self._source_key = source_key\n        self._destination_key = destination_key\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.typeless().with_item(self._destination_key, x[self._source_key])\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.DuplicateItemMapper.__init__","title":"<code>__init__(source_key, destination_key)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source_key</code> <code>str</code> <p>The key of the item to duplicate.</p> required <code>destination_key</code> <code>str</code> <p>The key of the duplicated item.</p> required Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, source_key: str, destination_key: str) -&gt; None:\n    \"\"\"\n    Args:\n        source_key (str): The key of the item to duplicate.\n        destination_key (str): The key of the duplicated item.\n    \"\"\"\n    super().__init__()\n    self._source_key = source_key\n    self._destination_key = destination_key\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.FilterKeysMapper","title":"<code>FilterKeysMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Filters sample keys.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class FilterKeysMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Filters sample keys.\"\"\"\n\n    def __init__(self, keys: str | Iterable[str], negate: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            keys (str | Iterable[str]): The keys to keep or remove from the sample.\n            negate (bool, optional): If `True`, the keys are removed from the sample. If\n                `False`, only the keys are kept. Defaults to `False`.\n        \"\"\"\n        super().__init__()\n        self._keys = [keys] if isinstance(keys, str) else keys\n        self._negate = negate\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.without(*self._keys) if self._negate else x.with_only(*self._keys)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.FilterKeysMapper.__init__","title":"<code>__init__(keys, negate=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>keys</code> <code>str | Iterable[str]</code> <p>The keys to keep or remove from the sample.</p> required <code>negate</code> <code>bool</code> <p>If <code>True</code>, the keys are removed from the sample. If <code>False</code>, only the keys are kept. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, keys: str | Iterable[str], negate: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        keys (str | Iterable[str]): The keys to keep or remove from the sample.\n        negate (bool, optional): If `True`, the keys are removed from the sample. If\n            `False`, only the keys are kept. Defaults to `False`.\n    \"\"\"\n    super().__init__()\n    self._keys = [keys] if isinstance(keys, str) else keys\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.FormatKeysMapper","title":"<code>FormatKeysMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Changes key names following a format string.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class FormatKeysMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Changes key names following a format string.\"\"\"\n\n    FMT_CHAR = \"*\"\n\n    def __init__(\n        self, format_string: str = FMT_CHAR, keys: str | Iterable[str] | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            format_string (str, optional): The new sample key format. Any `*` will be\n                replaced with the source key, eg, `my_*_key` on [`image`, `mask`]\n                generates `my_image_key` and `my_mask_key`. If no `*` is found, the\n                string is suffixed to source key, ie, `MyKey` on `image` gives\n                `imageMyKey`. If empty, the source key will not be changed. Defaults to\n                \"*\".\n\n            keys (str | Iterable[str] | None, optional): The keys to apply the new\n                format to. `None` applies to all the keys. Defaults to None.\n        \"\"\"\n        super().__init__()\n        if self.FMT_CHAR not in format_string:\n            format_string = self.FMT_CHAR + format_string\n\n        self._format_string = format_string\n        self._keys = keys\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        keys: Iterable[str]\n        if self._keys is None:\n            keys = x.keys()\n        elif isinstance(self._keys, str):\n            keys = [self._keys]\n        else:\n            keys = self._keys\n        remap = {}\n        for k in keys:\n            remap[k] = self._format_string.replace(self.FMT_CHAR, k)\n        return x.remap(remap)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.FormatKeysMapper.__init__","title":"<code>__init__(format_string=FMT_CHAR, keys=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>format_string</code> <code>str</code> <p>The new sample key format. Any <code>*</code> will be replaced with the source key, eg, <code>my_*_key</code> on [<code>image</code>, <code>mask</code>] generates <code>my_image_key</code> and <code>my_mask_key</code>. If no <code>*</code> is found, the string is suffixed to source key, ie, <code>MyKey</code> on <code>image</code> gives <code>imageMyKey</code>. If empty, the source key will not be changed. Defaults to \"*\".</p> <code>FMT_CHAR</code> <code>keys</code> <code>str | Iterable[str] | None</code> <p>The keys to apply the new format to. <code>None</code> applies to all the keys. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(\n    self, format_string: str = FMT_CHAR, keys: str | Iterable[str] | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        format_string (str, optional): The new sample key format. Any `*` will be\n            replaced with the source key, eg, `my_*_key` on [`image`, `mask`]\n            generates `my_image_key` and `my_mask_key`. If no `*` is found, the\n            string is suffixed to source key, ie, `MyKey` on `image` gives\n            `imageMyKey`. If empty, the source key will not be changed. Defaults to\n            \"*\".\n\n        keys (str | Iterable[str] | None, optional): The keys to apply the new\n            format to. `None` applies to all the keys. Defaults to None.\n    \"\"\"\n    super().__init__()\n    if self.FMT_CHAR not in format_string:\n        format_string = self.FMT_CHAR + format_string\n\n    self._format_string = format_string\n    self._keys = keys\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.RenameMapper","title":"<code>RenameMapper</code>","text":"<p>               Bases: <code>Mapper[Sample, TypelessSample]</code></p> <p>Rename some items preserving their content and format.</p> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>class RenameMapper(Mapper[Sample, TypelessSample]):\n    \"\"\"Rename some items preserving their content and format.\"\"\"\n\n    def __init__(self, renaming: Mapping[str, str], exclude: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            renaming (Mapping[str, str]): Mapping of keys to rename. The keys are the\n                original keys, and the values are the new keys.\n            exclude (bool, optional): If `True`, only the keys in the `renaming` mapping\n                will be renamed. If `False`, all keys except those in the `renaming`\n                mapping will be renamed. Defaults to `False`.\n        \"\"\"\n        super().__init__()\n        self._renaming = renaming\n        self._exclude = exclude\n\n    def __call__(self, idx: int, x: Sample) -&gt; TypelessSample:\n        return x.remap(self._renaming, exclude=self._exclude)\n</code></pre>"},{"location":"autoapi/pipewine/mappers/key_transform/#pipewine.mappers.key_transform.RenameMapper.__init__","title":"<code>__init__(renaming, exclude=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>renaming</code> <code>Mapping[str, str]</code> <p>Mapping of keys to rename. The keys are the original keys, and the values are the new keys.</p> required <code>exclude</code> <code>bool</code> <p>If <code>True</code>, only the keys in the <code>renaming</code> mapping will be renamed. If <code>False</code>, all keys except those in the <code>renaming</code> mapping will be renamed. Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>pipewine/mappers/key_transform.py</code> <pre><code>def __init__(self, renaming: Mapping[str, str], exclude: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        renaming (Mapping[str, str]): Mapping of keys to rename. The keys are the\n            original keys, and the values are the new keys.\n        exclude (bool, optional): If `True`, only the keys in the `renaming` mapping\n            will be renamed. If `False`, all keys except those in the `renaming`\n            mapping will be renamed. Defaults to `False`.\n    \"\"\"\n    super().__init__()\n    self._renaming = renaming\n    self._exclude = exclude\n</code></pre>"},{"location":"autoapi/pipewine/operators/","title":"operators","text":"<p>Package for all Pipewine built-in dataset operators.</p>"},{"location":"autoapi/pipewine/operators/base/","title":"base","text":"<p><code>DatasetOperator</code> base class definition and basic operators.</p>"},{"location":"autoapi/pipewine/operators/base/#pipewine.operators.base.DatasetOperator","title":"<code>DatasetOperator</code>","text":"<p>               Bases: <code>ABC</code>, <code>LoopCallbackMixin</code></p> <p>Base class for all Pipewine dataset operators, which are functions that transform datasets into other datasets.</p> <p>All dataset operator classes must inherit from this class and implement the <code>__call__</code> method, and must provide type hints to enable type inference, required by other pipewine components.</p> <p>Dataset operator classes can be parameterized by the types of the input and output datasets, <code>T_IN</code> and <code>T_OUT</code>, respectively. These generic types are bound to one of the following types: - <code>Dataset</code>: A single dataset. - <code>Sequence[Dataset]</code>: A sequence of datasets. - <code>tuple[Dataset, ...]</code>: A tuple of datasets, with the possibility of having     statically known length and types. - <code>Mapping[str, Dataset]</code>: A mapping of dataset names to datasets. - <code>Bundle[Dataset]</code>: A bundle of datasets, with statically known field names and     types.</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>class DatasetOperator[T_IN: AnyDataset, T_OUT: AnyDataset](ABC, LoopCallbackMixin):\n    \"\"\"Base class for all Pipewine dataset operators, which are functions that transform\n    datasets into other datasets.\n\n    All dataset operator classes must inherit from this class and implement the\n    `__call__` method, and **must** provide type hints to enable type inference,\n    required by other pipewine components.\n\n    Dataset operator classes can be parameterized by the types of the input and output\n    datasets, `T_IN` and `T_OUT`, respectively. These generic types are bound to one\n    of the following types:\n    - `Dataset`: A single dataset.\n    - `Sequence[Dataset]`: A sequence of datasets.\n    - `tuple[Dataset, ...]`: A tuple of datasets, with the possibility of having\n        statically known length and types.\n    - `Mapping[str, Dataset]`: A mapping of dataset names to datasets.\n    - `Bundle[Dataset]`: A bundle of datasets, with statically known field names and\n        types.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, x: T_IN) -&gt; T_OUT:\n        \"\"\"Transform the input dataset (or collection) into another dataset (or\n        collection).\n\n        This method **must** always be correctly annotated with type hints to enable\n        type inference by other pipewine components. Failing to do so will result in\n        type errors when composing operators into workflows, or when registering\n        them to the Pipewine CLI.\n\n        Args:\n            x (T_IN): The input dataset (or collection) to be transformed.\n\n        Returns:\n            T_OUT: The transformed output dataset (or collection).\n        \"\"\"\n        pass\n\n    @property\n    def input_type(self):\n        \"\"\"Infer the origin type of this operator's input, returning the origin of the\n        `__call__` method's `x` parameter.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"x\"])\n\n    @property\n    def output_type(self):\n        \"\"\"Infer the origin type of this operator's input, returning the origin of the\n        `__call__` method's return.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"return\"])\n</code></pre>"},{"location":"autoapi/pipewine/operators/base/#pipewine.operators.base.DatasetOperator.input_type","title":"<code>input_type</code>  <code>property</code>","text":"<p>Infer the origin type of this operator's input, returning the origin of the <code>__call__</code> method's <code>x</code> parameter.</p>"},{"location":"autoapi/pipewine/operators/base/#pipewine.operators.base.DatasetOperator.output_type","title":"<code>output_type</code>  <code>property</code>","text":"<p>Infer the origin type of this operator's input, returning the origin of the <code>__call__</code> method's return.</p>"},{"location":"autoapi/pipewine/operators/base/#pipewine.operators.base.DatasetOperator.__call__","title":"<code>__call__(x)</code>  <code>abstractmethod</code>","text":"<p>Transform the input dataset (or collection) into another dataset (or collection).</p> <p>This method must always be correctly annotated with type hints to enable type inference by other pipewine components. Failing to do so will result in type errors when composing operators into workflows, or when registering them to the Pipewine CLI.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>T_IN</code> <p>The input dataset (or collection) to be transformed.</p> required <p>Returns:</p> Name Type Description <code>T_OUT</code> <code>T_OUT</code> <p>The transformed output dataset (or collection).</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, x: T_IN) -&gt; T_OUT:\n    \"\"\"Transform the input dataset (or collection) into another dataset (or\n    collection).\n\n    This method **must** always be correctly annotated with type hints to enable\n    type inference by other pipewine components. Failing to do so will result in\n    type errors when composing operators into workflows, or when registering\n    them to the Pipewine CLI.\n\n    Args:\n        x (T_IN): The input dataset (or collection) to be transformed.\n\n    Returns:\n        T_OUT: The transformed output dataset (or collection).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/operators/base/#pipewine.operators.base.IdentityOp","title":"<code>IdentityOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Identity operator that accepts a dataset and returns it unchanged. Useful for testing and debugging purposes, or when a no-op is needed in a workflow.</p> Source code in <code>pipewine/operators/base.py</code> <pre><code>class IdentityOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Identity operator that accepts a dataset and returns it unchanged. Useful for\n    testing and debugging purposes, or when a no-op is needed in a workflow.\n    \"\"\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/","title":"cache","text":"<p>Operators for caching the results of other operators to avoid recomputation.</p>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache","title":"<code>Cache</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Key-value cache abstraction with thread-safe operations on arbitrary keys and values.</p> <p>Subclasses must implement the <code>_clear</code>, <code>_get</code>, and <code>_put</code> methods to define the cache behavior and eviction policy. These methods are automatically made thread-safe by the <code>Cache</code> class, so there is no need to worry about acquiring and releasing locks when implementing them.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class Cache[K, V](ABC):\n    \"\"\"Key-value cache abstraction with thread-safe operations on arbitrary keys and\n    values.\n\n    Subclasses must implement the `_clear`, `_get`, and `_put` methods to define the\n    cache behavior and eviction policy. These methods are automatically made thread-safe\n    by the `Cache` class, so there is no need to worry about acquiring and releasing\n    locks when implementing them.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the locks necessary for thread-safety, always make sure to call\n        this constructor when inheriting from this class.\n        \"\"\"\n        self._lock = RLock()\n\n    @abstractmethod\n    def _clear(self) -&gt; None:\n        \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get(self, key: K) -&gt; V | None:\n        \"\"\"Get the value associated with the given key.\n\n        Args:\n            key (K): Key to look up in the cache.\n\n        Returns:\n            V | None: Value associated with the key, or `None` if the key is not present\n                in the cache.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def _put(self, key: K, value: V) -&gt; None:\n        \"\"\"Put a key-value pair in the cache.\n\n        Args:\n            key (K): Key to associate with the value.\n            value (V): Value to store in the cache.\n        \"\"\"\n        pass\n\n    def clear(self) -&gt; None:\n        \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n        with self._lock:\n            self._clear()\n\n    def get(self, key: K) -&gt; V | None:\n        \"\"\"Get the value associated with the given key.\n\n        Args:\n            key (K): Key to look up in the cache.\n\n        Returns:\n            V | None: Value associated with the key, or `None` if the key is not present\n                in the cache.\n        \"\"\"\n        with self._lock:\n            return self._get(key)\n\n    def put(self, key: K, value: V) -&gt; None:\n        \"\"\"Put a key-value pair in the cache.\n\n        Args:\n            key (K): Key to associate with the value.\n            value (V): Value to store in the cache.\n        \"\"\"\n        with self._lock:\n            self._put(key, value)\n\n    def __getstate__(self) -&gt; dict[str, Any]:\n        data = {**self.__dict__}\n        del data[\"_lock\"]\n        return data\n\n    def __setstate__(self, data: dict[str, Any]) -&gt; None:\n        self._lock = RLock()\n        for k, v in data.items():\n            setattr(self, k, v)\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the locks necessary for thread-safety, always make sure to call this constructor when inheriting from this class.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the locks necessary for thread-safety, always make sure to call\n    this constructor when inheriting from this class.\n    \"\"\"\n    self._lock = RLock()\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache._clear","title":"<code>_clear()</code>  <code>abstractmethod</code>","text":"<p>Clear the cache, removing all key-value pairs.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _clear(self) -&gt; None:\n    \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache._get","title":"<code>_get(key)</code>  <code>abstractmethod</code>","text":"<p>Get the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to look up in the cache.</p> required <p>Returns:</p> Type Description <code>V | None</code> <p>V | None: Value associated with the key, or <code>None</code> if the key is not present in the cache.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _get(self, key: K) -&gt; V | None:\n    \"\"\"Get the value associated with the given key.\n\n    Args:\n        key (K): Key to look up in the cache.\n\n    Returns:\n        V | None: Value associated with the key, or `None` if the key is not present\n            in the cache.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache._put","title":"<code>_put(key, value)</code>  <code>abstractmethod</code>","text":"<p>Put a key-value pair in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to associate with the value.</p> required <code>value</code> <code>V</code> <p>Value to store in the cache.</p> required Source code in <code>pipewine/operators/cache.py</code> <pre><code>@abstractmethod\ndef _put(self, key: K, value: V) -&gt; None:\n    \"\"\"Put a key-value pair in the cache.\n\n    Args:\n        key (K): Key to associate with the value.\n        value (V): Value to store in the cache.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache.clear","title":"<code>clear()</code>","text":"<p>Clear the cache, removing all key-value pairs.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def clear(self) -&gt; None:\n    \"\"\"Clear the cache, removing all key-value pairs.\"\"\"\n    with self._lock:\n        self._clear()\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache.get","title":"<code>get(key)</code>","text":"<p>Get the value associated with the given key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to look up in the cache.</p> required <p>Returns:</p> Type Description <code>V | None</code> <p>V | None: Value associated with the key, or <code>None</code> if the key is not present in the cache.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def get(self, key: K) -&gt; V | None:\n    \"\"\"Get the value associated with the given key.\n\n    Args:\n        key (K): Key to look up in the cache.\n\n    Returns:\n        V | None: Value associated with the key, or `None` if the key is not present\n            in the cache.\n    \"\"\"\n    with self._lock:\n        return self._get(key)\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.Cache.put","title":"<code>put(key, value)</code>","text":"<p>Put a key-value pair in the cache.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>K</code> <p>Key to associate with the value.</p> required <code>value</code> <code>V</code> <p>Value to store in the cache.</p> required Source code in <code>pipewine/operators/cache.py</code> <pre><code>def put(self, key: K, value: V) -&gt; None:\n    \"\"\"Put a key-value pair in the cache.\n\n    Args:\n        key (K): Key to associate with the value.\n        value (V): Value to store in the cache.\n    \"\"\"\n    with self._lock:\n        self._put(key, value)\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.CacheOp","title":"<code>CacheOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches the results of another operator to avoid recomputation. See the \"Cache\" section in the documentation for more information.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class CacheOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches the results of another operator to avoid recomputation.\n    See the \"Cache\" section in the documentation for more information.\n    \"\"\"\n\n    def __init__(self, cache_type: type[Cache], **cache_params) -&gt; None:\n        \"\"\"\n        Args:\n            cache_type (type[Cache]): Type of cache to use, must be a subclass of\n                `Cache`.\n            cache_params (Any): Additional parameters to pass to the cache constructor.\n        \"\"\"\n        super().__init__()\n        self._cache_mapper: CacheMapper = CacheMapper()\n        self._cache_type = cache_type\n        self._cache_params = cache_params\n\n    def _get_sample[T: Sample](self, dataset: Dataset[T], cache_id: str, idx: int) -&gt; T:\n        cache: Cache[int, T] = InheritedData.data[cache_id]\n        result = cache.get(idx)\n        if result is None:\n            result = self._cache_mapper(idx, dataset[idx])\n            cache.put(idx, result)\n        return result\n\n    def _finalize_cache(self, id_: str) -&gt; None:\n        if id_ in InheritedData.data:  # pragma: no branch\n            del InheritedData.data[id_]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; LazyDataset[T]:\n        cache = self._cache_type(**self._cache_params)\n        id_ = uuid4().hex\n        InheritedData.data[id_] = cache\n        dataset = LazyDataset(len(x), partial(self._get_sample, x, id_))\n        weakref.finalize(dataset, self._finalize_cache, id_=id_)\n        return dataset\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.CacheOp.__init__","title":"<code>__init__(cache_type, **cache_params)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cache_type</code> <code>type[Cache]</code> <p>Type of cache to use, must be a subclass of <code>Cache</code>.</p> required <code>cache_params</code> <code>Any</code> <p>Additional parameters to pass to the cache constructor.</p> <code>{}</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, cache_type: type[Cache], **cache_params) -&gt; None:\n    \"\"\"\n    Args:\n        cache_type (type[Cache]): Type of cache to use, must be a subclass of\n            `Cache`.\n        cache_params (Any): Additional parameters to pass to the cache constructor.\n    \"\"\"\n    super().__init__()\n    self._cache_mapper: CacheMapper = CacheMapper()\n    self._cache_type = cache_type\n    self._cache_params = cache_params\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.FIFOCache","title":"<code>FIFOCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>First-In-First-Out (FIFO) cache that evicts the least recently inserted key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the order of access to the keys is known and the oldest keys are likely to be the least useful.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class FIFOCache[K, V](Cache[K, V]):\n    \"\"\"First-In-First-Out (FIFO) cache that evicts the least recently **inserted**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the order of access to the keys is known and\n    the oldest keys are likely to be the least useful.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: deque[K] = deque()\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            evicted = self._keys.popleft()\n            self._keys.append(key)\n            del self._mp[evicted]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.FIFOCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: deque[K] = deque()\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.ItemCacheOp","title":"<code>ItemCacheOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches the items of the samples in a dataset to avoid recomputation. Essentially the same as <code>MapOp(CacheMapper())</code>.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class ItemCacheOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches the items of the samples in a dataset to avoid\n    recomputation. Essentially the same as `MapOp(CacheMapper())`.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the operator with a `MapOp` that uses a `CacheMapper`.\"\"\"\n        super().__init__()\n        self._map_op = MapOp(CacheMapper())\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return self._map_op(x)\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.ItemCacheOp.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the operator with a <code>MapOp</code> that uses a <code>CacheMapper</code>.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the operator with a `MapOp` that uses a `CacheMapper`.\"\"\"\n    super().__init__()\n    self._map_op = MapOp(CacheMapper())\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.LIFOCache","title":"<code>LIFOCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Last-In-First-Out (LIFO) cache that evicts the most recently inserted key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the data is accessed in long repeated cycles, where the most recently inserted keys are likely to not going to be used again soon.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class LIFOCache[K, V](Cache[K, V]):\n    \"\"\"Last-In-First-Out (LIFO) cache that evicts the most recently **inserted**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the data is accessed in long repeated cycles,\n    where the most recently inserted keys are likely to not going to be used again soon.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: list[K] = []\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            evicted = self._keys[-1]\n            self._keys[-1] = key\n            del self._mp[evicted]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.LIFOCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: list[K] = []\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.LRUCache","title":"<code>LRUCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Least Recently Used (LRU) cache that evicts the least recently accessed key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the most recently accessed keys are likely to be the most useful in the near future.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class LRUCache[K, V](Cache[K, V]):\n    \"\"\"Least Recently Used (LRU) cache that evicts the least recently **accessed**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the most recently accessed keys are likely to\n    be the most useful in the near future.\n    \"\"\"\n\n    _PREV, _NEXT, _KEY, _VALUE = 0, 1, 2, 3\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n        super().__init__()\n        self._maxsize = maxsize\n        self._dll: list = []\n        self._dll[:] = [self._dll, self._dll, None, None]\n        self._mp: dict[K, list] = {}\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._dll[:] = [self._dll, self._dll, None, None]\n\n    def _get(self, key: K) -&gt; V | None:\n        link = self._mp.get(key)\n        if link is not None:\n            link_prev, link_next, _key, value = link\n            link_prev[self._NEXT] = link_next\n            link_next[self._PREV] = link_prev\n            last = self._dll[self._PREV]\n            last[self._NEXT] = self._dll[self._PREV] = link\n            link[self._PREV] = last\n            link[self._NEXT] = self._dll\n            return value\n        return None\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if key in self._mp:\n            self._mp[key][self._VALUE] = value\n            self._get(key)  # Set key as mru\n        elif len(self._mp) &gt;= self._maxsize:\n            oldroot = self._dll\n            oldroot[self._KEY] = key\n            oldroot[self._VALUE] = value\n            self._dll = oldroot[self._NEXT]\n            oldkey = self._dll[self._KEY]\n            oldvalue = self._dll[self._VALUE]\n            self._dll[self._KEY] = self._dll[self._VALUE] = None\n            del self._mp[oldkey]\n            self._mp[key] = oldroot\n        else:\n            last = self._dll[self._PREV]\n            link = [last, self._dll, key, value]\n            last[self._NEXT] = self._dll[self._PREV] = self._mp[key] = link\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.LRUCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n    super().__init__()\n    self._maxsize = maxsize\n    self._dll: list = []\n    self._dll[:] = [self._dll, self._dll, None, None]\n    self._mp: dict[K, list] = {}\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MRUCache","title":"<code>MRUCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Most Recently Used (MRU) cache that evicts the most recently accessed key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the most recently accessed keys are likely not going to be accessed again soon.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MRUCache[K, V](Cache[K, V]):\n    \"\"\"Most Recently Used (MRU) cache that evicts the most recently **accessed**\n    key-value pair when the cache is full and a new key-value pair is inserted. This\n    cache is useful for scenarios where the most recently accessed keys are likely not\n    going to be accessed again soon.\n    \"\"\"\n\n    _PREV, _NEXT, _KEY, _VALUE = 0, 1, 2, 3\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n\n        super().__init__()\n        self._maxsize = maxsize\n        self._dll: list = []\n        self._dll[:] = [self._dll, self._dll, None, None]\n        self._mp: dict[K, list] = {}\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._dll[:] = [self._dll, self._dll, None, None]\n\n    def _get(self, key: K) -&gt; V | None:\n        link = self._mp.get(key)\n        if link is not None:\n            link_prev, link_next, _key, value = link\n            link_prev[self._NEXT] = link_next\n            link_next[self._PREV] = link_prev\n            last = self._dll[self._PREV]\n            last[self._NEXT] = self._dll[self._PREV] = link\n            link[self._PREV] = last\n            link[self._NEXT] = self._dll\n            return value\n        return None\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if key in self._mp:\n            self._mp[key][self._VALUE] = value\n            self._get(key)  # Set key as mru\n        elif len(self._mp) &gt;= self._maxsize:\n            mru = self._dll[self._PREV]\n            oldkey = mru[self._KEY]\n            oldvalue = mru[self._VALUE]\n            mru[self._KEY] = key\n            mru[self._VALUE] = value\n            del self._mp[oldkey]\n            self._mp[key] = mru\n        else:\n            last = self._dll[self._PREV]\n            link = [last, self._dll, key, value]\n            last[self._NEXT] = self._dll[self._PREV] = self._mp[key] = link\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MRUCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n\n    super().__init__()\n    self._maxsize = maxsize\n    self._dll: list = []\n    self._dll[:] = [self._dll, self._dll, None, None]\n    self._mp: dict[K, list] = {}\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MemoCache","title":"<code>MemoCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Simple cache that stores key-value pairs in a dictionary, with no eviction policy or size limit, useful for memoization of functions with a bounded number of arguments.</p> <p>Avoid using this cache for large datasets or unbounded keys, as it will consume memory indefinitely.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MemoCache[K, V](Cache[K, V]):\n    \"\"\"Simple cache that stores key-value pairs in a dictionary, with no eviction policy\n    or size limit, useful for memoization of functions with a bounded number of\n    arguments.\n\n    Avoid using this cache for large datasets or unbounded keys, as it will consume\n    memory indefinitely.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the cache with an empty dictionary.\"\"\"\n        super().__init__()\n        self._memo: dict[K, V] = {}\n\n    def _clear(self) -&gt; None:\n        self._memo.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._memo.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        self._memo[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MemoCache.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the cache with an empty dictionary.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the cache with an empty dictionary.\"\"\"\n    super().__init__()\n    self._memo: dict[K, V] = {}\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MemorizeEverythingOp","title":"<code>MemorizeEverythingOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that caches all samples in a dataset to avoid recomputation eagerly. This operator will block until all samples are computed and loaded in memory, so it may not be suitable for large datasets, or when memory is a concern.</p> <p>In these cases, consider using a Checkpoint, see the \"Cache\" section in the documentation for more information.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class MemorizeEverythingOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that caches all samples in a dataset to avoid recomputation eagerly.\n    This operator will block until all samples are computed and loaded in memory, so it\n    may not be suitable for large datasets, or when memory is a concern.\n\n    In these cases, consider using a *Checkpoint*, see the \"Cache\" section in the\n    documentation for more information.\n    \"\"\"\n\n    def __init__(self, grabber: Grabber | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            grabber (Grabber | None, optional): Grabber to use for loading the samples\n                from the dataset. Defaults to None, in which case a new `Grabber` is\n                created.\n        \"\"\"\n        super().__init__()\n        self._grabber = grabber or Grabber()\n        self._cache_mapper: CacheMapper = CacheMapper()\n\n    def _get_sample(self, cache_id: str, idx: int) -&gt; Sample:\n        cache: Cache[int, Sample] = InheritedData.data[cache_id]\n        result = cache.get(idx)\n        assert result is not None\n        return result\n\n    def _finalize_cache(self, id_: str) -&gt; None:\n        if id_ in InheritedData.data:  # pragma: no branch\n            del InheritedData.data[id_]\n\n    def __call__(self, x: Dataset) -&gt; Dataset:\n        cache = MemoCache()\n        id_ = uuid4().hex\n        InheritedData.data[id_] = cache\n        for i, sample in self.loop(x, self._grabber, \"Caching\"):\n            cache.put(i, self._cache_mapper(i, sample))\n        dataset = LazyDataset(len(x), partial(self._get_sample, id_))\n        weakref.finalize(dataset, self._finalize_cache, id_=id_)\n        return dataset\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.MemorizeEverythingOp.__init__","title":"<code>__init__(grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>grabber</code> <code>Grabber | None</code> <p>Grabber to use for loading the samples from the dataset. Defaults to None, in which case a new <code>Grabber</code> is created.</p> <code>None</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, grabber: Grabber | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        grabber (Grabber | None, optional): Grabber to use for loading the samples\n            from the dataset. Defaults to None, in which case a new `Grabber` is\n            created.\n    \"\"\"\n    super().__init__()\n    self._grabber = grabber or Grabber()\n    self._cache_mapper: CacheMapper = CacheMapper()\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.RRCache","title":"<code>RRCache</code>","text":"<p>               Bases: <code>Cache[K, V]</code></p> <p>Random Replacement (RR) cache that evicts a random key-value pair when the cache is full and a new key-value pair is inserted. This cache is useful for scenarios where the order of access to the keys is not known or when no suitable eviction policy is particularly effective.</p> Source code in <code>pipewine/operators/cache.py</code> <pre><code>class RRCache[K, V](Cache[K, V]):\n    \"\"\"Random Replacement (RR) cache that evicts a random key-value pair when the cache\n    is full and a new key-value pair is inserted. This cache is useful for scenarios\n    where the order of access to the keys is not known or when no suitable eviction\n    policy is particularly effective.\n    \"\"\"\n\n    def __init__(self, maxsize: int = 32) -&gt; None:\n        \"\"\"\n        Args:\n            maxsize (int, optional): Maximum number of key-value pairs to store in the\n                cache. Defaults to 32.\n        \"\"\"\n\n        super().__init__()\n        self._mp: dict[K, V] = {}\n        self._keys: list[K] = []\n        self._maxsize = maxsize\n\n    def _clear(self) -&gt; None:\n        self._mp.clear()\n        self._keys.clear()\n\n    def _get(self, key: K) -&gt; V | None:\n        return self._mp.get(key)\n\n    def _put(self, key: K, value: V) -&gt; None:\n        if len(self._keys) &lt; self._maxsize:\n            self._keys.append(key)\n        else:\n            idx = random.randint(0, self._maxsize - 1)\n            prev_k = self._keys[idx]\n            self._keys[idx] = key\n            del self._mp[prev_k]\n        self._mp[key] = value\n</code></pre>"},{"location":"autoapi/pipewine/operators/cache/#pipewine.operators.cache.RRCache.__init__","title":"<code>__init__(maxsize=32)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>maxsize</code> <code>int</code> <p>Maximum number of key-value pairs to store in the cache. Defaults to 32.</p> <code>32</code> Source code in <code>pipewine/operators/cache.py</code> <pre><code>def __init__(self, maxsize: int = 32) -&gt; None:\n    \"\"\"\n    Args:\n        maxsize (int, optional): Maximum number of key-value pairs to store in the\n            cache. Defaults to 32.\n    \"\"\"\n\n    super().__init__()\n    self._mp: dict[K, V] = {}\n    self._keys: list[K] = []\n    self._maxsize = maxsize\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/","title":"functional","text":"<p>Operators that change behavior based on user-defined functions.</p>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.ComparableT","title":"<code>ComparableT = SupportsDunderLT[Any] | SupportsDunderGT[Any]</code>  <code>module-attribute</code>","text":"<p>Type alias for types that support the less-than and greater-than dunder methods.</p>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.FilterOp","title":"<code>FilterOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], Dataset[T]]</code></p> <p>Operator that keeps only or removes samples from a dataset based on a user-defined filter function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class FilterOp[T: Sample](DatasetOperator[Dataset[T], Dataset[T]]):\n    \"\"\"Operator that keeps only or removes samples from a dataset based on a\n    user-defined filter function.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Callable[[int, T], bool],\n        negate: bool = False,\n        grabber: Grabber | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], bool]): Function that takes the index and the sample\n                and returns whether the sample should be kept or removed.\n            negate (bool, optional): Whether to negate the filter function. Defaults to\n                False.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n        self._negate = negate\n\n    def __call__(self, x: Dataset[T]) -&gt; Dataset[T]:\n        new_index = []\n        for i, sample in self.loop(x, self._grabber, name=\"Filtering\"):\n            if self._fn(i, sample) ^ self._negate:\n                new_index.append(i)\n        return LazyDataset(len(new_index), x.get_sample, index_fn=new_index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.FilterOp.__init__","title":"<code>__init__(fn, negate=False, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], bool]</code> <p>Function that takes the index and the sample and returns whether the sample should be kept or removed.</p> required <code>negate</code> <code>bool</code> <p>Whether to negate the filter function. Defaults to False.</p> <code>False</code> <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self,\n    fn: Callable[[int, T], bool],\n    negate: bool = False,\n    grabber: Grabber | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], bool]): Function that takes the index and the sample\n            and returns whether the sample should be kept or removed.\n        negate (bool, optional): Whether to negate the filter function. Defaults to\n            False.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.GroupByOp","title":"<code>GroupByOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], dict[str, Dataset[T]]]</code></p> <p>Operator that groups samples in a dataset based on a user-defined grouping function, returning a mapping of datasets with a key for each unique value returned by the grouping function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class GroupByOp[T: Sample](DatasetOperator[Dataset[T], dict[str, Dataset[T]]]):\n    \"\"\"Operator that groups samples in a dataset based on a user-defined grouping\n    function, returning a mapping of datasets with a key for each unique value\n    returned by the grouping function.\n    \"\"\"\n\n    def __init__(\n        self, fn: Callable[[int, T], str], grabber: Grabber | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], str]): Function that takes the index and the sample\n                and returns a string representing the group to which the sample belongs.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n\n    def __call__(self, x: Dataset[T]) -&gt; dict[str, Dataset[T]]:\n        indexes: dict[str, list[int]] = defaultdict(list)\n        for i, sample in self.loop(x, self._grabber, name=\"Computing index\"):\n            key = self._fn(i, sample)\n            indexes[key].append(i)\n        return {\n            k: LazyDataset(len(index), x.get_sample, index_fn=index.__getitem__)\n            for k, index in indexes.items()\n        }\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.GroupByOp.__init__","title":"<code>__init__(fn, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], str]</code> <p>Function that takes the index and the sample and returns a string representing the group to which the sample belongs.</p> required <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self, fn: Callable[[int, T], str], grabber: Grabber | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], str]): Function that takes the index and the sample\n            and returns a string representing the group to which the sample belongs.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.MapOp","title":"<code>MapOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T_IN], Dataset[T_OUT]]</code></p> <p>Operator that applies a <code>Mapper</code> to each sample in a dataset.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class MapOp[T_IN: Sample, T_OUT: Sample](\n    DatasetOperator[Dataset[T_IN], Dataset[T_OUT]]\n):\n    \"\"\"Operator that applies a `Mapper` to each sample in a dataset.\"\"\"\n\n    def __init__(self, mapper: Mapper[T_IN, T_OUT]) -&gt; None:\n        \"\"\"\n        Args:\n            mapper (Mapper[T_IN, T_OUT]): Mapper to apply to each sample.\n        \"\"\"\n        super().__init__()\n        self._mapper = mapper\n\n    def _get_sample(self, x: Dataset[T_IN], idx: int) -&gt; T_OUT:\n        return self._mapper(idx, x[idx])\n\n    def __call__(self, x: Dataset[T_IN]) -&gt; Dataset[T_OUT]:\n        return LazyDataset(len(x), partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.MapOp.__init__","title":"<code>__init__(mapper)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mapper</code> <code>Mapper[T_IN, T_OUT]</code> <p>Mapper to apply to each sample.</p> required Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(self, mapper: Mapper[T_IN, T_OUT]) -&gt; None:\n    \"\"\"\n    Args:\n        mapper (Mapper[T_IN, T_OUT]): Mapper to apply to each sample.\n    \"\"\"\n    super().__init__()\n    self._mapper = mapper\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.SortOp","title":"<code>SortOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset[T], Dataset[T]]</code></p> <p>Operator that sorts samples in a dataset based on a user-defined sorting function.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class SortOp[T: Sample](DatasetOperator[Dataset[T], Dataset[T]]):\n    \"\"\"Operator that sorts samples in a dataset based on a user-defined sorting\n    function.\n    \"\"\"\n\n    def __init__(\n        self,\n        fn: Callable[[int, T], ComparableT],\n        reverse: bool = False,\n        grabber: Grabber | None = None,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            fn (Callable[[int, T], ComparableT]): Function that takes the index and the\n                sample and returns a comparable value to use for sorting.\n            reverse (bool, optional): Whether to sort in reverse order. Defaults to False.\n            grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n                to None.\n        \"\"\"\n        super().__init__()\n        self._fn = fn\n        self._grabber = grabber or Grabber()\n        self._reverse = reverse\n\n    def __call__(self, x: Dataset[T]) -&gt; Dataset[T]:\n        keys: list[tuple[ComparableT, int]] = []\n        for i, sample in self.loop(x, self._grabber, name=\"Computing keys\"):\n            keys.append((self._fn(i, sample), i))\n\n        index = [x[1] for x in sorted(keys, reverse=self._reverse)]\n        return LazyDataset(len(x), x.get_sample, index_fn=index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.SortOp.__init__","title":"<code>__init__(fn, reverse=False, grabber=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[[int, T], ComparableT]</code> <p>Function that takes the index and the sample and returns a comparable value to use for sorting.</p> required <code>reverse</code> <code>bool</code> <p>Whether to sort in reverse order. Defaults to False.</p> <code>False</code> <code>grabber</code> <code>Grabber</code> <p>Grabber to use for grabbing samples. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/functional.py</code> <pre><code>def __init__(\n    self,\n    fn: Callable[[int, T], ComparableT],\n    reverse: bool = False,\n    grabber: Grabber | None = None,\n) -&gt; None:\n    \"\"\"\n    Args:\n        fn (Callable[[int, T], ComparableT]): Function that takes the index and the\n            sample and returns a comparable value to use for sorting.\n        reverse (bool, optional): Whether to sort in reverse order. Defaults to False.\n        grabber (Grabber, optional): Grabber to use for grabbing samples. Defaults\n            to None.\n    \"\"\"\n    super().__init__()\n    self._fn = fn\n    self._grabber = grabber or Grabber()\n    self._reverse = reverse\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.SupportsDunderGT","title":"<code>SupportsDunderGT</code>","text":"<p>               Bases: <code>Protocol[_T_contravariant]</code></p> <p>Protocol for types that support the greater-than dunder method.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class SupportsDunderGT(Protocol[_T_contravariant]):\n    \"\"\"Protocol for types that support the greater-than dunder method.\"\"\"\n\n    def __gt__(self, other: _T_contravariant, /) -&gt; bool: ...\n</code></pre>"},{"location":"autoapi/pipewine/operators/functional/#pipewine.operators.functional.SupportsDunderLT","title":"<code>SupportsDunderLT</code>","text":"<p>               Bases: <code>Protocol[_T_contravariant]</code></p> <p>Protocol for types that support the less-than dunder method.</p> Source code in <code>pipewine/operators/functional.py</code> <pre><code>class SupportsDunderLT(Protocol[_T_contravariant]):\n    \"\"\"Protocol for types that support the less-than dunder method.\"\"\"\n\n    def __lt__(self, other: _T_contravariant, /) -&gt; bool: ...\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/","title":"iter","text":"<p>Operators for manipulating the length and order of datasets.</p>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.CycleOp","title":"<code>CycleOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that repeats a dataset until a given length is reached.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class CycleOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that repeats a dataset until a given length is reached.\"\"\"\n\n    def __init__(self, n: int) -&gt; None:\n        \"\"\"\n        Args:\n            n (int): Length of the resulting dataset.\n        \"\"\"\n        super().__init__()\n        self._n = n\n\n    def _index_fn(self, orig_len: int, x: int) -&gt; int:\n        return x % orig_len\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        assert len(x) &gt; 0\n        return LazyDataset(\n            self._n, x.get_sample, index_fn=partial(self._index_fn, len(x))\n        )\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.CycleOp.__init__","title":"<code>__init__(n)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Length of the resulting dataset.</p> required Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, n: int) -&gt; None:\n    \"\"\"\n    Args:\n        n (int): Length of the resulting dataset.\n    \"\"\"\n    super().__init__()\n    self._n = n\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.IndexOp","title":"<code>IndexOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that selects samples from a dataset based on their indices.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class IndexOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that selects samples from a dataset based on their indices.\"\"\"\n\n    def __init__(self, index: Sequence[int], negate: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            index (Sequence[int]): Indices of the samples to select.\n            negate (bool, optional): Whether to negate the selection. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self._index = index\n        self._negate = negate\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        index: Sequence[int]\n        if self._negate:\n            index = list(set(range(len(x))).difference(set(self._index)))\n        else:\n            index = self._index\n        return LazyDataset(len(index), x.get_sample, index_fn=index.__getitem__)\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.IndexOp.__init__","title":"<code>__init__(index, negate=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>index</code> <code>Sequence[int]</code> <p>Indices of the samples to select.</p> required <code>negate</code> <code>bool</code> <p>Whether to negate the selection. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, index: Sequence[int], negate: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        index (Sequence[int]): Indices of the samples to select.\n        negate (bool, optional): Whether to negate the selection. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self._index = index\n    self._negate = negate\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.PadOp","title":"<code>PadOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that pads a dataset to a given length by repeating a specified sample.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class PadOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that pads a dataset to a given length by repeating a specified sample.\"\"\"\n\n    def __init__(self, length: int, pad_with: int = -1) -&gt; None:\n        \"\"\"\n        Args:\n            length (int): Length of the resulting dataset.\n            pad_with (int, optional): Index of the sample to use for padding. Defaults\n                to -1.\n        \"\"\"\n        super().__init__()\n        self._length = length\n        self._pad_with = pad_with\n\n    def _get_sample[T: Sample](self, x: Dataset[T], idx: int) -&gt; T:\n        if idx &lt; len(x):\n            return x[idx]\n        else:\n            return x[self._pad_with]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return LazyDataset(self._length, partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.PadOp.__init__","title":"<code>__init__(length, pad_with=-1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>Length of the resulting dataset.</p> required <code>pad_with</code> <code>int</code> <p>Index of the sample to use for padding. Defaults to -1.</p> <code>-1</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, length: int, pad_with: int = -1) -&gt; None:\n    \"\"\"\n    Args:\n        length (int): Length of the resulting dataset.\n        pad_with (int, optional): Index of the sample to use for padding. Defaults\n            to -1.\n    \"\"\"\n    super().__init__()\n    self._length = length\n    self._pad_with = pad_with\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.RepeatOp","title":"<code>RepeatOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that repeats a dataset a given number of times.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class RepeatOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that repeats a dataset a given number of times.\"\"\"\n\n    def __init__(self, times: int, interleave: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            times (int): Number of times to repeat the dataset.\n            interleave (bool, optional): Whether to interleave the repeated samples.\n                Defaults to False.\n        \"\"\"\n        super().__init__()\n        self._times = times\n        self._interleave = interleave\n\n    def _index_fn(self, orig_len: int, x: int) -&gt; int:\n        return x % orig_len\n\n    def _index_fn_interleave(self, x: int) -&gt; int:\n        return x // self._times\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        index_fn = (\n            self._index_fn_interleave\n            if self._interleave\n            else partial(self._index_fn, len(x))\n        )\n        return LazyDataset(len(x) * self._times, x.get_sample, index_fn=index_fn)\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.RepeatOp.__init__","title":"<code>__init__(times, interleave=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>times</code> <code>int</code> <p>Number of times to repeat the dataset.</p> required <code>interleave</code> <code>bool</code> <p>Whether to interleave the repeated samples. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(self, times: int, interleave: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        times (int): Number of times to repeat the dataset.\n        interleave (bool, optional): Whether to interleave the repeated samples.\n            Defaults to False.\n    \"\"\"\n    super().__init__()\n    self._times = times\n    self._interleave = interleave\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.ReverseOp","title":"<code>ReverseOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that reverses the order of samples in a dataset.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class ReverseOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that reverses the order of samples in a dataset.\"\"\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x[::-1]\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.SliceOp","title":"<code>SliceOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that slices a dataset based on the start, stop, and step arguments, similar to the plain Python slicing.</p> Source code in <code>pipewine/operators/iter.py</code> <pre><code>class SliceOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that slices a dataset based on the start, stop, and step arguments,\n    similar to the plain Python slicing.\n    \"\"\"\n\n    def __init__(\n        self, start: int | None = None, stop: int | None = None, step: int | None = None\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            start (int, optional): Start index of the slice. Defaults to None.\n            stop (int, optional): Stop index of the slice. Defaults to None.\n            step (int, optional): Step of the slice. Defaults to None.\n        \"\"\"\n        super().__init__()\n        self._start = start\n        self._stop = stop\n        self._step = step\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        return x[slice(self._start or 0, self._stop or len(x), self._step or 1)]\n</code></pre>"},{"location":"autoapi/pipewine/operators/iter/#pipewine.operators.iter.SliceOp.__init__","title":"<code>__init__(start=None, stop=None, step=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Start index of the slice. Defaults to None.</p> <code>None</code> <code>stop</code> <code>int</code> <p>Stop index of the slice. Defaults to None.</p> <code>None</code> <code>step</code> <code>int</code> <p>Step of the slice. Defaults to None.</p> <code>None</code> Source code in <code>pipewine/operators/iter.py</code> <pre><code>def __init__(\n    self, start: int | None = None, stop: int | None = None, step: int | None = None\n) -&gt; None:\n    \"\"\"\n    Args:\n        start (int, optional): Start index of the slice. Defaults to None.\n        stop (int, optional): Stop index of the slice. Defaults to None.\n        step (int, optional): Step of the slice. Defaults to None.\n    \"\"\"\n    super().__init__()\n    self._start = start\n    self._stop = stop\n    self._step = step\n</code></pre>"},{"location":"autoapi/pipewine/operators/merge/","title":"merge","text":"<p>Operators for merging datasets.</p>"},{"location":"autoapi/pipewine/operators/merge/#pipewine.operators.merge.CatOp","title":"<code>CatOp</code>","text":"<p>               Bases: <code>DatasetOperator[Sequence[Dataset], Dataset]</code></p> <p>Operator that concatenates multiple datasets into a single dataset.</p> Source code in <code>pipewine/operators/merge.py</code> <pre><code>class CatOp(DatasetOperator[Sequence[Dataset], Dataset]):\n    \"\"\"Operator that concatenates multiple datasets into a single dataset.\"\"\"\n\n    def _get_sample[\n        T: Sample\n    ](self, datasets: Sequence[Dataset[T]], index: list[int], i: int) -&gt; T:\n        dataset_idx = bisect(index, i) - 1\n        effective_i = i - index[dataset_idx]\n        return datasets[dataset_idx][effective_i]\n\n    def __call__[T: Sample](self, x: Sequence[Dataset[T]]) -&gt; Dataset[T]:\n        index = [0]\n        for dataset in x:\n            index.append(index[-1] + len(dataset))\n        return LazyDataset(index[-1], partial(self._get_sample, x, index))\n</code></pre>"},{"location":"autoapi/pipewine/operators/merge/#pipewine.operators.merge.ZipOp","title":"<code>ZipOp</code>","text":"<p>               Bases: <code>DatasetOperator[Sequence[Dataset], Dataset[T]]</code></p> <p>Operator that zips multiple datasets into a single dataset by merging the items of individual samples.</p> <p>Input datasets must have the same length and the samples must have disjoint items.</p> Source code in <code>pipewine/operators/merge.py</code> <pre><code>class ZipOp[T: Sample](DatasetOperator[Sequence[Dataset], Dataset[T]]):\n    \"\"\"Operator that zips multiple datasets into a single dataset by merging the items\n    of individual samples.\n\n    Input datasets must have the same length and the samples must have disjoint items.\n    \"\"\"\n\n    def __init__(self, out_type: type[T] | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            out_type (type[T] | None, optional): Type of the output samples. Defaults\n                to None (TypelessSample).\n        \"\"\"\n        super().__init__()\n        self._out_type = out_type or TypelessSample\n\n    def _get_sample(self, datasets: Sequence[Dataset[Sample]], idx: int) -&gt; T:\n        data: dict[str, Item] = {}\n        for dataset in datasets:\n            data.update(dataset[idx].items())\n        return self._out_type(**data)  # type: ignore\n\n    def __call__(self, x: Sequence[Dataset[Sample]]) -&gt; Dataset[T]:\n        len0 = len(x[0])\n        assert all(len(dataset) == len0 for dataset in x)\n        return LazyDataset(len(x[0]), partial(self._get_sample, x))\n</code></pre>"},{"location":"autoapi/pipewine/operators/merge/#pipewine.operators.merge.ZipOp.__init__","title":"<code>__init__(out_type=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>out_type</code> <code>type[T] | None</code> <p>Type of the output samples. Defaults to None (TypelessSample).</p> <code>None</code> Source code in <code>pipewine/operators/merge.py</code> <pre><code>def __init__(self, out_type: type[T] | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        out_type (type[T] | None, optional): Type of the output samples. Defaults\n            to None (TypelessSample).\n    \"\"\"\n    super().__init__()\n    self._out_type = out_type or TypelessSample\n</code></pre>"},{"location":"autoapi/pipewine/operators/rand/","title":"rand","text":"<p>Operators with random behavior.</p>"},{"location":"autoapi/pipewine/operators/rand/#pipewine.operators.rand.ShuffleOp","title":"<code>ShuffleOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Dataset]</code></p> <p>Operator that shuffles the samples in a dataset in a random order.</p> Source code in <code>pipewine/operators/rand.py</code> <pre><code>class ShuffleOp(DatasetOperator[Dataset, Dataset]):\n    \"\"\"Operator that shuffles the samples in a dataset in a random order.\"\"\"\n\n    def _index_fn(self, index: list[int], x: int) -&gt; int:\n        return index[x]\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        idx = list(range(len(x)))\n        shuffle(idx)\n        return LazyDataset(len(x), x.get_sample, index_fn=partial(self._index_fn, idx))\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/","title":"split","text":"<p>Operators for splitting datasets into multiple parts.</p>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.BatchOp","title":"<code>BatchOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into batches of a given size, except for the last batch which may be smaller.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class BatchOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into batches of a given size, except for the\n    last batch which may be smaller.\n    \"\"\"\n\n    def __init__(self, batch_size: int) -&gt; None:\n        \"\"\"\n        Args:\n            batch_size (int): Size of the batches.\n        \"\"\"\n        super().__init__()\n        self._batch_size = batch_size\n        assert self._batch_size &gt; 0, \"Batch size must be greater than 0.\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        start = 0\n        batches = []\n        while start &lt; len(x):\n            batches.append(x[start : start + self._batch_size])\n            start += self._batch_size\n\n        return batches\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.BatchOp.__init__","title":"<code>__init__(batch_size)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Size of the batches.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, batch_size: int) -&gt; None:\n    \"\"\"\n    Args:\n        batch_size (int): Size of the batches.\n    \"\"\"\n    super().__init__()\n    self._batch_size = batch_size\n    assert self._batch_size &gt; 0, \"Batch size must be greater than 0.\"\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.ChunkOp","title":"<code>ChunkOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into a given number of chunks of approximately equal size. The difference in size between any two chunks is guaranteed to be at most 1.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class ChunkOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into a given number of chunks of approximately\n    equal size. The difference in size between any two chunks is guaranteed to be\n    at most 1.\n    \"\"\"\n\n    def __init__(self, chunks: int) -&gt; None:\n        \"\"\"\n        Args:\n            chunks (int): Number of chunks.\n        \"\"\"\n        super().__init__()\n        self._chunks = chunks\n        assert self._chunks &gt; 0, \"Number of chunks must be greater than 0.\"\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        size_floor = len(x) // self._chunks\n        sizes = [size_floor] * self._chunks\n        remainder = len(x) % self._chunks\n        for i in range(remainder):\n            sizes[i] += 1\n        start = 0\n        chunks = []\n        for size in sizes:\n            chunks.append(x[start : start + size])\n            start += size\n        return chunks\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.ChunkOp.__init__","title":"<code>__init__(chunks)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>int</code> <p>Number of chunks.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, chunks: int) -&gt; None:\n    \"\"\"\n    Args:\n        chunks (int): Number of chunks.\n    \"\"\"\n    super().__init__()\n    self._chunks = chunks\n    assert self._chunks &gt; 0, \"Number of chunks must be greater than 0.\"\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.SplitOp","title":"<code>SplitOp</code>","text":"<p>               Bases: <code>DatasetOperator[Dataset, Sequence[Dataset]]</code></p> <p>Operator that splits a dataset into multiple parts of given sizes. The sizes can be either integers or floats representing the proportions of the dataset to be included in each part.</p> Source code in <code>pipewine/operators/split.py</code> <pre><code>class SplitOp(DatasetOperator[Dataset, Sequence[Dataset]]):\n    \"\"\"Operator that splits a dataset into multiple parts of given sizes. The sizes\n    can be either integers or floats representing the proportions of the dataset to\n    be included in each part.\n    \"\"\"\n\n    def __init__(self, sizes: Sequence[int | float | None]) -&gt; None:\n        \"\"\"\n        Args:\n            sizes (Sequence[int | float | None]): Sizes of the splits. If a size is an\n                integer, it represents the number of samples in the split. If a size is\n                a float, it represents the proportion of the dataset to be included in\n                the split. If a size is None, it represents the remaining samples after\n                all other sizes have been computed. At most one size can be None.\n        \"\"\"\n        super().__init__()\n        self._sizes = sizes\n        all_ints = all(isinstance(x, int) for x in self._sizes if x is not None)\n        all_floats = all(isinstance(x, float) for x in self._sizes if x is not None)\n        assert all_ints or all_floats, \"Sizes must be all int or all float, not mixed.\"\n        self._null_idx = -1\n        self._total: int | float = 0\n        for i, size in enumerate(self._sizes):\n            if size is None:\n                if self._null_idx &gt;= 0:\n                    raise ValueError(\"At most one size can be None.\")\n                self._null_idx = i\n            else:\n                self._total += size\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Sequence[Dataset[T]]:\n        sizes: list[int] = []\n        for size in self._sizes:\n            if isinstance(size, float):\n                int_size = int(size * len(x))\n            elif size is None:\n                int_size = len(x) - int(self._total)\n            else:\n                int_size = size\n            sizes.append(int_size)\n\n        start = 0\n        splits = []\n        for size in sizes:\n            splits.append(x[start : start + size])\n            start += size\n        return splits\n</code></pre>"},{"location":"autoapi/pipewine/operators/split/#pipewine.operators.split.SplitOp.__init__","title":"<code>__init__(sizes)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>Sequence[int | float | None]</code> <p>Sizes of the splits. If a size is an integer, it represents the number of samples in the split. If a size is a float, it represents the proportion of the dataset to be included in the split. If a size is None, it represents the remaining samples after all other sizes have been computed. At most one size can be None.</p> required Source code in <code>pipewine/operators/split.py</code> <pre><code>def __init__(self, sizes: Sequence[int | float | None]) -&gt; None:\n    \"\"\"\n    Args:\n        sizes (Sequence[int | float | None]): Sizes of the splits. If a size is an\n            integer, it represents the number of samples in the split. If a size is\n            a float, it represents the proportion of the dataset to be included in\n            the split. If a size is None, it represents the remaining samples after\n            all other sizes have been computed. At most one size can be None.\n    \"\"\"\n    super().__init__()\n    self._sizes = sizes\n    all_ints = all(isinstance(x, int) for x in self._sizes if x is not None)\n    all_floats = all(isinstance(x, float) for x in self._sizes if x is not None)\n    assert all_ints or all_floats, \"Sizes must be all int or all float, not mixed.\"\n    self._null_idx = -1\n    self._total: int | float = 0\n    for i, size in enumerate(self._sizes):\n        if size is None:\n            if self._null_idx &gt;= 0:\n                raise ValueError(\"At most one size can be None.\")\n            self._null_idx = i\n        else:\n            self._total += size\n</code></pre>"},{"location":"autoapi/pipewine/parsers/","title":"parsers","text":"<p>Package for all Pipewine built-in parsers.</p>"},{"location":"autoapi/pipewine/parsers/base/","title":"base","text":"<p>Base classes for Pipewine parsers.</p>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser","title":"<code>Parser</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for Pipewine parsers, which are used to parse data from bytes and vice versa. Parser classes are parameterized by the type <code>T</code> of the data they parse.</p> <p>Subclasses should implement the <code>parse</code>, <code>dump</code>, and <code>extensions</code> methods.</p> <p>All subclasses of this class are automatically registered in the <code>ParserRegistry</code> on import.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>class Parser[T: Any](ABC, metaclass=_ParserMeta):\n    \"\"\"Base class for Pipewine parsers, which are used to parse data from bytes and\n    vice versa. Parser classes are parameterized by the type `T` of the data they parse.\n\n    Subclasses should implement the `parse`, `dump`, and `extensions` methods.\n\n    All subclasses of this class are automatically registered in the `ParserRegistry`\n    on import.\n    \"\"\"\n\n    def __init__(self, type_: type[T] | None = None):\n        \"\"\"Initialize the parser, make sure to call the super constructor in subclasses.\n\n        Args:\n            type_ (type[T] | None, optional): Optional concrete type `T` of the returned\n                data, if known. Used by some parsers to enforce the return type of the\n                parsed data using information only available at runtime. Defaults to\n                None, in which case the type of the parsed data is not enforced and can\n                be any subclass of `T`.\n        \"\"\"\n        super().__init__()\n        self._type = type_\n\n    @property\n    def type_(self) -&gt; type[T] | None:\n        \"\"\"Get the concrete type `T` of the parsed data, if known.\"\"\"\n        return self._type\n\n    @abstractmethod\n    def parse(self, data: bytes) -&gt; T:\n        \"\"\"Parse data from bytes. Implementations can access the `type_` attribute, if\n        set, to enforce the return type of the parsed data.\n\n        Args:\n            data (bytes): A byte string containing the data to parse.\n\n        Returns:\n            T: The parsed data.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def dump(self, data: T) -&gt; bytes:\n        \"\"\"Dump data to bytes.\n\n        Args:\n            data (T): The data to dump.\n\n        Returns:\n            bytes: A byte string containing the dumped data.\n        \"\"\"\n        pass\n\n    @classmethod\n    @abstractmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        \"\"\"Get the file extensions associated with the parser.\"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser.type_","title":"<code>type_</code>  <code>property</code>","text":"<p>Get the concrete type <code>T</code> of the parsed data, if known.</p>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser.__init__","title":"<code>__init__(type_=None)</code>","text":"<p>Initialize the parser, make sure to call the super constructor in subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>type_</code> <code>type[T] | None</code> <p>Optional concrete type <code>T</code> of the returned data, if known. Used by some parsers to enforce the return type of the parsed data using information only available at runtime. Defaults to None, in which case the type of the parsed data is not enforced and can be any subclass of <code>T</code>.</p> <code>None</code> Source code in <code>pipewine/parsers/base.py</code> <pre><code>def __init__(self, type_: type[T] | None = None):\n    \"\"\"Initialize the parser, make sure to call the super constructor in subclasses.\n\n    Args:\n        type_ (type[T] | None, optional): Optional concrete type `T` of the returned\n            data, if known. Used by some parsers to enforce the return type of the\n            parsed data using information only available at runtime. Defaults to\n            None, in which case the type of the parsed data is not enforced and can\n            be any subclass of `T`.\n    \"\"\"\n    super().__init__()\n    self._type = type_\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser.dump","title":"<code>dump(data)</code>  <code>abstractmethod</code>","text":"<p>Dump data to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>T</code> <p>The data to dump.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>A byte string containing the dumped data.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>@abstractmethod\ndef dump(self, data: T) -&gt; bytes:\n    \"\"\"Dump data to bytes.\n\n    Args:\n        data (T): The data to dump.\n\n    Returns:\n        bytes: A byte string containing the dumped data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser.extensions","title":"<code>extensions()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Get the file extensions associated with the parser.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>@classmethod\n@abstractmethod\ndef extensions(cls) -&gt; Iterable[str]:\n    \"\"\"Get the file extensions associated with the parser.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.Parser.parse","title":"<code>parse(data)</code>  <code>abstractmethod</code>","text":"<p>Parse data from bytes. Implementations can access the <code>type_</code> attribute, if set, to enforce the return type of the parsed data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>bytes</code> <p>A byte string containing the data to parse.</p> required <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The parsed data.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>@abstractmethod\ndef parse(self, data: bytes) -&gt; T:\n    \"\"\"Parse data from bytes. Implementations can access the `type_` attribute, if\n    set, to enforce the return type of the parsed data.\n\n    Args:\n        data (bytes): A byte string containing the data to parse.\n\n    Returns:\n        T: The parsed data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.ParserRegistry","title":"<code>ParserRegistry</code>","text":"<p>Container for currently registered parsers, allowing other parts of the code to retrieve the most appropriate parser class for a given file format.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>class ParserRegistry:\n    \"\"\"Container for currently registered parsers, allowing other parts of the code to\n    retrieve the most appropriate parser class for a given file format.\n    \"\"\"\n\n    _registered_parsers: dict[str, type[\"Parser\"]] = {}\n\n    @classmethod\n    def get(cls, key: str) -&gt; type[\"Parser\"] | None:\n        \"\"\"Get the parser class for a given file format.\n\n        Args:\n            key (str): File format extension.\n\n        Returns:\n            type[Parser]: Parser class for the given file format, or None if no parser\n                is currently registered for the format.\n        \"\"\"\n        return cls._registered_parsers.get(key)\n\n    @classmethod\n    def keys(cls) -&gt; KeysView[str]:\n        \"\"\"Get the keys of the currently registered parsers.\"\"\"\n        return cls._registered_parsers.keys()\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.ParserRegistry.get","title":"<code>get(key)</code>  <code>classmethod</code>","text":"<p>Get the parser class for a given file format.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>File format extension.</p> required <p>Returns:</p> Type Description <code>type[Parser] | None</code> <p>type[Parser]: Parser class for the given file format, or None if no parser is currently registered for the format.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>@classmethod\ndef get(cls, key: str) -&gt; type[\"Parser\"] | None:\n    \"\"\"Get the parser class for a given file format.\n\n    Args:\n        key (str): File format extension.\n\n    Returns:\n        type[Parser]: Parser class for the given file format, or None if no parser\n            is currently registered for the format.\n    \"\"\"\n    return cls._registered_parsers.get(key)\n</code></pre>"},{"location":"autoapi/pipewine/parsers/base/#pipewine.parsers.base.ParserRegistry.keys","title":"<code>keys()</code>  <code>classmethod</code>","text":"<p>Get the keys of the currently registered parsers.</p> Source code in <code>pipewine/parsers/base.py</code> <pre><code>@classmethod\ndef keys(cls) -&gt; KeysView[str]:\n    \"\"\"Get the keys of the currently registered parsers.\"\"\"\n    return cls._registered_parsers.keys()\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/","title":"image_parser","text":"<p>Parsers for image data.</p>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.BmpParser","title":"<code>BmpParser</code>","text":"<p>               Bases: <code>ImageParser</code></p> <p>Parser for BMP image data.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>class BmpParser(ImageParser):\n    \"\"\"Parser for BMP image data.\"\"\"\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"bmp\"]\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.ImageParser","title":"<code>ImageParser</code>","text":"<p>               Bases: <code>Parser[ndarray]</code></p> <p>Base class for image data parsers that use the <code>imageio</code> library.</p> <p>Subclasses should only need to implement the <code>extensions</code> method to specify the file extensions that the parser can handle.</p> <p>Optionally, the <code>_save_options</code> method can be implemented to provide additional options to the <code>imwrite</code> function when dumping data, e.g., compression level for PNG files.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>class ImageParser(Parser[np.ndarray]):\n    \"\"\"Base class for image data parsers that use the `imageio` library.\n\n    Subclasses should only need to implement the `extensions` method to specify the\n    file extensions that the parser can handle.\n\n    Optionally, the `_save_options` method can be implemented to provide additional\n    options to the `imwrite` function when dumping data, e.g., compression level for PNG\n    files.\n    \"\"\"\n\n    def parse(self, data: bytes) -&gt; np.ndarray:\n        return np.array(iio.imread(data, extension=\".\" + next(iter(self.extensions()))))\n\n    def dump(self, data: np.ndarray) -&gt; bytes:\n        ext = next(iter(self.extensions()))\n        return iio.imwrite(\n            \"&lt;bytes&gt;\",\n            data,\n            extension=\".\" + ext,\n            **self._save_options(),\n        )\n\n    def _save_options(self) -&gt; Mapping[str, Any]:\n        \"\"\"Additional options to pass to the `imwrite` function when dumping data.\"\"\"\n        return {}\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.ImageParser._save_options","title":"<code>_save_options()</code>","text":"<p>Additional options to pass to the <code>imwrite</code> function when dumping data.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>def _save_options(self) -&gt; Mapping[str, Any]:\n    \"\"\"Additional options to pass to the `imwrite` function when dumping data.\"\"\"\n    return {}\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.JpegParser","title":"<code>JpegParser</code>","text":"<p>               Bases: <code>ImageParser</code></p> <p>Parser for JPEG image data.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>class JpegParser(ImageParser):\n    \"\"\"Parser for JPEG image data.\"\"\"\n\n    def _save_options(self) -&gt; Mapping[str, Any]:\n        return {\"quality\": 80}\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"jpeg\", \"jpg\", \"jfif\", \"jpe\"]\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.PngParser","title":"<code>PngParser</code>","text":"<p>               Bases: <code>ImageParser</code></p> <p>Parser for PNG image data.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>class PngParser(ImageParser):\n    \"\"\"Parser for PNG image data.\"\"\"\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"png\"]\n\n    def _save_options(self) -&gt; Mapping[str, Any]:\n        return {\"compress_level\": 4}\n</code></pre>"},{"location":"autoapi/pipewine/parsers/image_parser/#pipewine.parsers.image_parser.TiffParser","title":"<code>TiffParser</code>","text":"<p>               Bases: <code>ImageParser</code></p> <p>Parser for TIFF image data.</p> Source code in <code>pipewine/parsers/image_parser.py</code> <pre><code>class TiffParser(ImageParser):\n    \"\"\"Parser for TIFF image data.\"\"\"\n\n    def _save_options(self) -&gt; Mapping[str, Any]:\n        return {\"compression\": \"zlib\", \"photometric\": True}\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"tiff\", \"tif\"]\n\n    def parse(self, data: bytes) -&gt; np.ndarray:\n        return np.array(tifffile.imread(io.BytesIO(data)))\n\n    def dump(self, data: np.ndarray) -&gt; bytes:\n        buffer = io.BytesIO()\n        tifffile.imwrite(buffer, data, **self._save_options())\n        buffer.seek(0)\n        return buffer.read()\n</code></pre>"},{"location":"autoapi/pipewine/parsers/metadata_parser/","title":"metadata_parser","text":"<p>Parsers for metadata files.</p>"},{"location":"autoapi/pipewine/parsers/metadata_parser/#pipewine.parsers.metadata_parser.JSONParser","title":"<code>JSONParser</code>","text":"<p>               Bases: <code>Parser[T]</code></p> <p>Parser for JSON data. Can parse and dump basic types (str, int, float, bool, dict, list) as well as Pydantic models.</p> Source code in <code>pipewine/parsers/metadata_parser.py</code> <pre><code>class JSONParser[T: str | int | float | bool | dict | list | PydanticLike](Parser[T]):\n    \"\"\"Parser for JSON data. Can parse and dump basic types (str, int, float, bool,\n    dict, list) as well as Pydantic models.\n    \"\"\"\n\n    def parse(self, data: bytes) -&gt; T:\n        json_data = json.loads(data.decode())\n        if self._type is None:\n            return json_data\n        elif issubclass(self._type, (str, int, float, bool, dict, list)):\n            return self._type(json_data)  # type: ignore\n        else:\n            return self._type.model_validate(json_data)\n\n    def dump(self, data: T) -&gt; bytes:\n        if isinstance(data, (str, int, float, bool, dict, list)):\n            json_data = data  # type: ignore\n        else:\n            json_data = data.model_dump()\n        return json.dumps(json_data).encode()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"json\"]\n</code></pre>"},{"location":"autoapi/pipewine/parsers/metadata_parser/#pipewine.parsers.metadata_parser.PydanticLike","title":"<code>PydanticLike</code>","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for classes that behave like Pydantic models.</p> Source code in <code>pipewine/parsers/metadata_parser.py</code> <pre><code>class PydanticLike(Protocol):\n    \"\"\"Protocol for classes that behave like Pydantic models.\"\"\"\n\n    @classmethod\n    def model_validate(cls, obj: Any) -&gt; Self: ...\n    def model_dump(self) -&gt; dict: ...\n</code></pre>"},{"location":"autoapi/pipewine/parsers/metadata_parser/#pipewine.parsers.metadata_parser.YAMLParser","title":"<code>YAMLParser</code>","text":"<p>               Bases: <code>Parser[T]</code></p> <p>Parser for YAML data. Can parse and dump basic types (str, int, float, bool, dict, list) as well as Pydantic models.</p> Source code in <code>pipewine/parsers/metadata_parser.py</code> <pre><code>class YAMLParser[T: str | int | float | bool | dict | list | PydanticLike](Parser[T]):\n    \"\"\"Parser for YAML data. Can parse and dump basic types (str, int, float, bool,\n    dict, list) as well as Pydantic models.\n    \"\"\"\n\n    def parse(self, data: bytes) -&gt; T:\n        yaml_data = yaml.safe_load(data.decode())\n        if self._type is None:\n            return yaml_data\n        elif issubclass(self._type, (str, int, float, bool, dict, list)):\n            return self._type(yaml_data)  # type: ignore\n        else:\n            return self._type.model_validate(yaml_data)\n\n    def dump(self, data: T) -&gt; bytes:\n        if isinstance(data, (str, int, float, bool, dict, list)):\n            yaml_data = data\n        else:\n            yaml_data = data.model_dump()\n        return yaml.safe_dump(yaml_data).encode()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"yaml\", \"yml\"]\n</code></pre>"},{"location":"autoapi/pipewine/parsers/numpy_parser/","title":"numpy_parser","text":"<p>Parser for arbitrary NumPy arrays.</p>"},{"location":"autoapi/pipewine/parsers/numpy_parser/#pipewine.parsers.numpy_parser.NumpyNpyParser","title":"<code>NumpyNpyParser</code>","text":"<p>               Bases: <code>Parser[ndarray]</code></p> <p>Parser for NumPy arrays saved in the <code>.npy</code> format.</p> Source code in <code>pipewine/parsers/numpy_parser.py</code> <pre><code>class NumpyNpyParser(Parser[np.ndarray]):\n    \"\"\"Parser for NumPy arrays saved in the `.npy` format.\"\"\"\n\n    def parse(self, data: bytes) -&gt; np.ndarray:\n        buffer = io.BytesIO(data)\n        return np.load(buffer)\n\n    def dump(self, data: np.ndarray) -&gt; bytes:\n        buffer = io.BytesIO()\n        np.save(buffer, data)\n        buffer.seek(0)\n        return buffer.read()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"npy\"]\n</code></pre>"},{"location":"autoapi/pipewine/parsers/pickle_parser/","title":"pickle_parser","text":"<p>Parsers for arbitrary python objects using the <code>pickle</code> library.</p>"},{"location":"autoapi/pipewine/parsers/pickle_parser/#pipewine.parsers.pickle_parser.PickleParser","title":"<code>PickleParser</code>","text":"<p>               Bases: <code>Parser[T]</code></p> <p>Parser for arbitrary python objects using the <code>pickle</code> library.</p> Source code in <code>pipewine/parsers/pickle_parser.py</code> <pre><code>class PickleParser[T](Parser[T]):\n    \"\"\"Parser for arbitrary python objects using the `pickle` library.\"\"\"\n\n    def parse(self, data: bytes) -&gt; T:\n        return pickle.loads(data)\n\n    def dump(self, data: T) -&gt; bytes:\n        return pickle.dumps(data)\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        return [\"pkl\"]\n</code></pre>"},{"location":"autoapi/pipewine/sinks/","title":"sinks","text":"<p>Package for all Pipewine built-in dataset sinks.</p>"},{"location":"autoapi/pipewine/sinks/base/","title":"base","text":"<p>Base class for all dataset sinks.</p>"},{"location":"autoapi/pipewine/sinks/base/#pipewine.sinks.base.DatasetSink","title":"<code>DatasetSink</code>","text":"<p>               Bases: <code>ABC</code>, <code>LoopCallbackMixin</code></p> <p>Base class for all dataset sinks, used to consume datasets, typically at the end of a pipeline by writing them to disk or to some other storage.</p> <p>Subclasses should implement the <code>__call__</code> method, and must provide type hints to enable type inference, required by other pipewine components.</p> <p>Dataset sink classes can be parameterized by the type of the dataset they consume, <code>T</code>, which can be one of the following types: - <code>Dataset</code>: A single dataset. - <code>Sequence[Dataset]</code>: A sequence of datasets. - <code>tuple[Dataset, ...]</code>: A tuple of datasets, with the possibility of having     statically known length and types. - <code>Mapping[str, Dataset]</code>: A mapping of dataset names to datasets. - <code>Bundle[Dataset]</code>: A bundle of datasets, with statically known field names and     types.</p> Source code in <code>pipewine/sinks/base.py</code> <pre><code>class DatasetSink[T: AnyDataset](ABC, LoopCallbackMixin):\n    \"\"\"Base class for all dataset sinks, used to consume datasets, typically at the end\n    of a pipeline by writing them to disk or to some other storage.\n\n    Subclasses should implement the `__call__` method, and **must** provide type hints\n    to enable type inference, required by other pipewine components.\n\n    Dataset sink classes can be parameterized by the type of the dataset they consume,\n    `T`, which can be one of the following types:\n    - `Dataset`: A single dataset.\n    - `Sequence[Dataset]`: A sequence of datasets.\n    - `tuple[Dataset, ...]`: A tuple of datasets, with the possibility of having\n        statically known length and types.\n    - `Mapping[str, Dataset]`: A mapping of dataset names to datasets.\n    - `Bundle[Dataset]`: A bundle of datasets, with statically known field names and\n        types.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, data: T) -&gt; None:\n        \"\"\"Consume the input dataset (or collection).\n\n        This method **must** always be correctly annotated with type hints to enable\n        type inference by other pipewine components. Failing to do so will result in\n        type errors when composing operators into workflows, or when registering\n        them to the Pipewine CLI.\n\n        Args:\n            data (T): The input dataset (or collection) to be consumed.\n        \"\"\"\n        pass\n\n    @property\n    def input_type(self):\n        \"\"\"Infer the origin type of this sink's input, returning the origin of the\n        `__call__` method's `data` parameter.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"data\"])\n</code></pre>"},{"location":"autoapi/pipewine/sinks/base/#pipewine.sinks.base.DatasetSink.input_type","title":"<code>input_type</code>  <code>property</code>","text":"<p>Infer the origin type of this sink's input, returning the origin of the <code>__call__</code> method's <code>data</code> parameter.</p>"},{"location":"autoapi/pipewine/sinks/base/#pipewine.sinks.base.DatasetSink.__call__","title":"<code>__call__(data)</code>  <code>abstractmethod</code>","text":"<p>Consume the input dataset (or collection).</p> <p>This method must always be correctly annotated with type hints to enable type inference by other pipewine components. Failing to do so will result in type errors when composing operators into workflows, or when registering them to the Pipewine CLI.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>T</code> <p>The input dataset (or collection) to be consumed.</p> required Source code in <code>pipewine/sinks/base.py</code> <pre><code>@abstractmethod\ndef __call__(self, data: T) -&gt; None:\n    \"\"\"Consume the input dataset (or collection).\n\n    This method **must** always be correctly annotated with type hints to enable\n    type inference by other pipewine components. Failing to do so will result in\n    type errors when composing operators into workflows, or when registering\n    them to the Pipewine CLI.\n\n    Args:\n        data (T): The input dataset (or collection) to be consumed.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sinks/fs_utils/","title":"fs_utils","text":"<p>Utilities for file system operations</p>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.CopyPolicy","title":"<code>CopyPolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>How to handle cases in which a replication of existing data is detected. In these cases, some operations may be avoided, trading off data consistency for speed.</p> Source code in <code>pipewine/sinks/fs_utils.py</code> <pre><code>class CopyPolicy(str, Enum):\n    \"\"\"How to handle cases in which a replication of existing data is detected. In these\n    cases, some operations may be avoided, trading off data consistency for speed.\n    \"\"\"\n\n    REWRITE = \"REWRITE\"\n    \"\"\"Do not copy anything, ever, even if the data is untouched. Treat every write\n    alike: serialize the object, encode it and write to a new file. This is the slowest\n    option but also the safest.\"\"\"\n\n    REPLICATE = \"REPLICATE\"\n    \"\"\"Avoid the serialization (which can be expensive), and simply copy the existing \n    file, replicating all of its content. This option is significantly faster than \n    re-serializing everything every time, but may cause data corruption in case of\n    read-write-read races. E.g. pipewine reads the file, some other process modifies it,\n    then pipewine copies the modified file assuming it was not modified. Pipewine\n    datasets should not be modified inplace, and pipewine never does so unless \n    explicitly configured to do so, making this option relatively safe to use.\"\"\"\n\n    SYMBOLIC_LINK = \"SYMBOLIC_LINK\"\n    \"\"\"Symbolic links (a.k.a. soft links) are simply a reference to another file in any\n    of the mounted file systems. They are virtually inexpensive, but do not actually\n    contain any replicated data. They are prone to cause data loss, because every\n    time the original file is modified/deleted/renamed/moved, the symbolic link will \n    point to a modified/deleted file. A symbolic link can point to a file on another \n    file system. This option is extremely unsafe, use at your own risk.\"\"\"\n\n    HARD_LINK = \"HARD_LINK\"\n    \"\"\"Hard links are similar to symbolic links in terms of speed and safety, the\n    difference is that they point to the same inode of the linked file.\n\n    Soft link:  `MY_LINK` --&gt; `MY_FILE` --&gt; `INODE`\n\n    Hard link:  `MY_FILE` --&gt; `INODE` &lt;-- `MY_LINK`\n\n    While this does not offer any protection against modifications of the original file \n    (because the data is not actually replicated), it prevents links from breaking in\n    case the original file is moved/renamed or even deleted. Hard link are also faster\n    than symbolic links, making this option preferable. The only limitation is that\n    hard links can only be created when both link and linked files exist on the same\n    filesystem.\n    \"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.CopyPolicy.HARD_LINK","title":"<code>HARD_LINK = 'HARD_LINK'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Hard links are similar to symbolic links in terms of speed and safety, the difference is that they point to the same inode of the linked file.</p> <p>Soft link:  <code>MY_LINK</code> --&gt; <code>MY_FILE</code> --&gt; <code>INODE</code></p> <p>Hard link:  <code>MY_FILE</code> --&gt; <code>INODE</code> &lt;-- <code>MY_LINK</code></p> <p>While this does not offer any protection against modifications of the original file  (because the data is not actually replicated), it prevents links from breaking in case the original file is moved/renamed or even deleted. Hard link are also faster than symbolic links, making this option preferable. The only limitation is that hard links can only be created when both link and linked files exist on the same filesystem.</p>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.CopyPolicy.REPLICATE","title":"<code>REPLICATE = 'REPLICATE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Avoid the serialization (which can be expensive), and simply copy the existing  file, replicating all of its content. This option is significantly faster than  re-serializing everything every time, but may cause data corruption in case of read-write-read races. E.g. pipewine reads the file, some other process modifies it, then pipewine copies the modified file assuming it was not modified. Pipewine datasets should not be modified inplace, and pipewine never does so unless  explicitly configured to do so, making this option relatively safe to use.</p>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.CopyPolicy.REWRITE","title":"<code>REWRITE = 'REWRITE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Do not copy anything, ever, even if the data is untouched. Treat every write alike: serialize the object, encode it and write to a new file. This is the slowest option but also the safest.</p>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.CopyPolicy.SYMBOLIC_LINK","title":"<code>SYMBOLIC_LINK = 'SYMBOLIC_LINK'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Symbolic links (a.k.a. soft links) are simply a reference to another file in any of the mounted file systems. They are virtually inexpensive, but do not actually contain any replicated data. They are prone to cause data loss, because every time the original file is modified/deleted/renamed/moved, the symbolic link will  point to a modified/deleted file. A symbolic link can point to a file on another  file system. This option is extremely unsafe, use at your own risk.</p>"},{"location":"autoapi/pipewine/sinks/fs_utils/#pipewine.sinks.fs_utils.write_item_to_file","title":"<code>write_item_to_file(item, file, copy_policy=CopyPolicy.HARD_LINK)</code>","text":"<p>Write an item to a file, using the specified copy policy.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Item</code> <p>The item to write to the file.</p> required <code>file</code> <code>Path</code> <p>The path to the file to write to.</p> required <code>copy_policy</code> <code>CopyPolicy</code> <p>The copy policy to use when Pipewine infers that the item is a copy of an existing file. Defaults to CopyPolicy.HARD_LINK.</p> <code>HARD_LINK</code> <p>Raises:</p> Type Description <code>IOError</code> <p>If the item cannot be written to the file.</p> Source code in <code>pipewine/sinks/fs_utils.py</code> <pre><code>def write_item_to_file(\n    item: Item, file: Path, copy_policy: CopyPolicy = CopyPolicy.HARD_LINK\n) -&gt; None:\n    \"\"\"Write an item to a file, using the specified copy policy.\n\n    Args:\n        item (Item): The item to write to the file.\n        file (Path): The path to the file to write to.\n        copy_policy (CopyPolicy, optional): The copy policy to use when Pipewine\n            infers that the item is a copy of an existing file. Defaults to\n            CopyPolicy.HARD_LINK.\n\n    Raises:\n        IOError: If the item cannot be written to the file.\n    \"\"\"\n    if isinstance(item, CachedItem):\n        item = item.source_recursive\n\n    errors: list[tuple] = []\n    if (\n        isinstance(item, StoredItem)\n        and isinstance(item.reader, LocalFileReader)\n        and item.reader.path.is_file()\n    ):\n        src = item.reader.path\n        if copy_policy == CopyPolicy.HARD_LINK:\n            if _try_copy(os.link, str(src), str(file), errors):\n                return\n            else:\n                copy_policy = CopyPolicy.REPLICATE\n\n        if copy_policy == CopyPolicy.SYMBOLIC_LINK:\n            if _try_copy(os.symlink, str(src), str(file), errors):\n                return\n            else:\n                copy_policy = CopyPolicy.REPLICATE\n\n        if copy_policy == CopyPolicy.REPLICATE:\n            if _try_copy(shutil.copy, str(src), str(file), errors):\n                return\n            else:\n                copy_policy = CopyPolicy.REWRITE\n\n    data = item.parser.dump(item())\n    try:\n        with open(file, \"wb\") as fp:\n            fp.write(data)\n    except Exception:\n        msg = f\"Failed to write to file {file}. Failed attempts: \\n\"\n        substr = []\n        for err in errors:\n            modname, fname = err[0].__module__, err[0].__name__\n            substr.append(\n                f\" - Calling {modname}.{fname}({err[1]}, {err[2]}) -&gt; {err[3]}\"\n            )\n        msg += \"\\n\".join(substr)\n        raise IOError(msg)\n</code></pre>"},{"location":"autoapi/pipewine/sinks/underfolder/","title":"underfolder","text":"<p>Sink that writes the dataset to file system using Pipelime \"Underfolder\" format.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy","title":"<code>OverwritePolicy</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>How to handle cases where pipewine needs to overwrite existing data.</p> Source code in <code>pipewine/sinks/underfolder.py</code> <pre><code>class OverwritePolicy(str, Enum):\n    \"\"\"How to handle cases where pipewine needs to overwrite existing data.\"\"\"\n\n    FORBID = \"FORBID\"\n    \"\"\"Most strict policy: always fail in case there is something saved in the \n    destination path, even an empty folder. Ensures no data loss, but may crash the \n    program unwantedly.\n    \"\"\"\n\n    ALLOW_IF_EMPTY = \"ALLOW_IF_EMPTY\"\n    \"\"\"Allow the overwrite only in case the destination path is an empty folder.\"\"\"\n\n    ALLOW_NEW_FILES = \"ALLOW_NEW_FILES\"\n    \"\"\"Only allow the creation of new files without deleting/modifying existing ones.\n    Prevents data loss at the individual file level, but may render the dataset\n    unreadable or change its format.\n    \"\"\"\n\n    OVERWRITE_FILES = \"OVERWRITE_FILES\"\n    \"\"\"Delete only conflicting files before writing. This may result in data loss and\n    make the dataset unreadable or change its format. Use at your own risk.\n    \"\"\"\n\n    OVERWRITE = \"OVERWRITE\"\n    \"\"\"Weakest policy: completely delete and rewrite the folder. This will result in\n    major data loss but ensures that the final dataset is readable and with the expected\n    format. Use at your own risk.\n    \"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy.ALLOW_IF_EMPTY","title":"<code>ALLOW_IF_EMPTY = 'ALLOW_IF_EMPTY'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Allow the overwrite only in case the destination path is an empty folder.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy.ALLOW_NEW_FILES","title":"<code>ALLOW_NEW_FILES = 'ALLOW_NEW_FILES'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Only allow the creation of new files without deleting/modifying existing ones. Prevents data loss at the individual file level, but may render the dataset unreadable or change its format.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy.FORBID","title":"<code>FORBID = 'FORBID'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Most strict policy: always fail in case there is something saved in the  destination path, even an empty folder. Ensures no data loss, but may crash the  program unwantedly.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy.OVERWRITE","title":"<code>OVERWRITE = 'OVERWRITE'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Weakest policy: completely delete and rewrite the folder. This will result in major data loss but ensures that the final dataset is readable and with the expected format. Use at your own risk.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.OverwritePolicy.OVERWRITE_FILES","title":"<code>OVERWRITE_FILES = 'OVERWRITE_FILES'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Delete only conflicting files before writing. This may result in data loss and make the dataset unreadable or change its format. Use at your own risk.</p>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.UnderfolderSink","title":"<code>UnderfolderSink</code>","text":"<p>               Bases: <code>DatasetSink[Dataset]</code></p> <p>Sink that writes the dataset to file system using Pipewine \"Underfolder\" format.</p> Source code in <code>pipewine/sinks/underfolder.py</code> <pre><code>class UnderfolderSink(DatasetSink[Dataset]):\n    \"\"\"Sink that writes the dataset to file system using Pipewine \"Underfolder\" format.\"\"\"\n\n    def __init__(\n        self,\n        folder: Path,\n        grabber: Grabber | None = None,\n        overwrite_policy: OverwritePolicy = OverwritePolicy.FORBID,\n        copy_policy: CopyPolicy = CopyPolicy.HARD_LINK,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            folder (Path): Path to the folder where the dataset will be saved.\n            grabber (Grabber, optional): The grabber to use when writing the dataset.\n                Defaults to None, in which case a new grabber is created.\n            overwrite_policy (OverwritePolicy, optional): How to handle cases where\n                pipewine needs to overwrite existing data. Defaults to\n                OverwritePolicy.FORBID.\n            copy_policy (CopyPolicy, optional): The copy policy to use when Pipewine\n                infers that the item is a copy of an existing file. Defaults to\n                CopyPolicy.HARD_LINK.\n        \"\"\"\n        super().__init__()\n        self._folder = folder\n        self._grabber = grabber or Grabber()\n        self._overwrite_policy = overwrite_policy\n        self._copy_policy = copy_policy\n\n    def __call__[T: Sample](self, data: Dataset[T]) -&gt; None:\n        if len(data) == 0:\n            return\n        if self._folder.exists():\n            if self._overwrite_policy == OverwritePolicy.FORBID:\n                raise FileExistsError(\n                    f\"Folder {self._folder} already exists and policy \"\n                    f\"{self._overwrite_policy} is used. Either change the destination \"\n                    \"path or set a weaker policy.\"\n                )\n\n            elif self._overwrite_policy == OverwritePolicy.OVERWRITE:\n                shutil.rmtree(self._folder, ignore_errors=True)\n\n        self._folder.mkdir(parents=True, exist_ok=True)\n\n        with os.scandir(self._folder) as it:\n            if any(it) and self._overwrite_policy == OverwritePolicy.ALLOW_IF_EMPTY:\n                raise FileExistsError(\n                    f\"Folder {self._folder} is not empty and policy \"\n                    f\"{self._overwrite_policy} is used. Either change the destination\"\n                    \"path or set a weaker policy.\"\n                )\n\n        inner_folder = self._folder / \"data\"\n        inner_folder.mkdir(parents=True, exist_ok=True)\n        best_zfill = len(str(len(data) - 1))\n\n        writer: _WriterMapper[T] = _WriterMapper(\n            self._folder,\n            inner_folder,\n            best_zfill,\n            self._overwrite_policy,\n            self._copy_policy,\n        )\n        data = MapOp(writer)(data)\n\n        for _ in self.loop(data, self._grabber, name=\"Writing\"):\n            pass\n</code></pre>"},{"location":"autoapi/pipewine/sinks/underfolder/#pipewine.sinks.underfolder.UnderfolderSink.__init__","title":"<code>__init__(folder, grabber=None, overwrite_policy=OverwritePolicy.FORBID, copy_policy=CopyPolicy.HARD_LINK)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>folder</code> <code>Path</code> <p>Path to the folder where the dataset will be saved.</p> required <code>grabber</code> <code>Grabber</code> <p>The grabber to use when writing the dataset. Defaults to None, in which case a new grabber is created.</p> <code>None</code> <code>overwrite_policy</code> <code>OverwritePolicy</code> <p>How to handle cases where pipewine needs to overwrite existing data. Defaults to OverwritePolicy.FORBID.</p> <code>FORBID</code> <code>copy_policy</code> <code>CopyPolicy</code> <p>The copy policy to use when Pipewine infers that the item is a copy of an existing file. Defaults to CopyPolicy.HARD_LINK.</p> <code>HARD_LINK</code> Source code in <code>pipewine/sinks/underfolder.py</code> <pre><code>def __init__(\n    self,\n    folder: Path,\n    grabber: Grabber | None = None,\n    overwrite_policy: OverwritePolicy = OverwritePolicy.FORBID,\n    copy_policy: CopyPolicy = CopyPolicy.HARD_LINK,\n) -&gt; None:\n    \"\"\"\n    Args:\n        folder (Path): Path to the folder where the dataset will be saved.\n        grabber (Grabber, optional): The grabber to use when writing the dataset.\n            Defaults to None, in which case a new grabber is created.\n        overwrite_policy (OverwritePolicy, optional): How to handle cases where\n            pipewine needs to overwrite existing data. Defaults to\n            OverwritePolicy.FORBID.\n        copy_policy (CopyPolicy, optional): The copy policy to use when Pipewine\n            infers that the item is a copy of an existing file. Defaults to\n            CopyPolicy.HARD_LINK.\n    \"\"\"\n    super().__init__()\n    self._folder = folder\n    self._grabber = grabber or Grabber()\n    self._overwrite_policy = overwrite_policy\n    self._copy_policy = copy_policy\n</code></pre>"},{"location":"autoapi/pipewine/sources/","title":"sources","text":"<p>Package for all Pipewine built-in dataset sources.</p>"},{"location":"autoapi/pipewine/sources/base/","title":"base","text":"<p>Base classes for defining data sources.</p>"},{"location":"autoapi/pipewine/sources/base/#pipewine.sources.base.DatasetSource","title":"<code>DatasetSource</code>","text":"<p>               Bases: <code>ABC</code>, <code>LoopCallbackMixin</code></p> <p>Base class for all dataset sources, used to produce datasets, typically at the beginning of a pipeline by reading them from disk or from some other storage.</p> <p>Subclasses should implement the <code>__call__</code> method, and must provide type hints to enable type inference, required by other pipewine components.</p> <p>Dataset source classes can be parameterized by the type of the dataset they produce, <code>T</code>, which can be one of the following types: - <code>Dataset</code>: A single dataset. - <code>Sequence[Dataset]</code>: A sequence of datasets. - <code>tuple[Dataset, ...]</code>: A tuple of datasets, with the possibility of having     statically known length and types. - <code>Mapping[str, Dataset]</code>: A mapping of dataset names to datasets. - <code>Bundle[Dataset]</code>: A bundle of datasets, with statically known field names and     types.</p> Source code in <code>pipewine/sources/base.py</code> <pre><code>class DatasetSource[T: AnyDataset](ABC, LoopCallbackMixin):\n    \"\"\"Base class for all dataset sources, used to produce datasets, typically at the\n    beginning of a pipeline by reading them from disk or from some other storage.\n\n    Subclasses should implement the `__call__` method, and **must** provide type hints\n    to enable type inference, required by other pipewine components.\n\n    Dataset source classes can be parameterized by the type of the dataset they produce,\n    `T`, which can be one of the following types:\n    - `Dataset`: A single dataset.\n    - `Sequence[Dataset]`: A sequence of datasets.\n    - `tuple[Dataset, ...]`: A tuple of datasets, with the possibility of having\n        statically known length and types.\n    - `Mapping[str, Dataset]`: A mapping of dataset names to datasets.\n    - `Bundle[Dataset]`: A bundle of datasets, with statically known field names and\n        types.\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self) -&gt; T:\n        \"\"\"Produce a dataset (or collection).\n\n        This method **must** always be correctly annotated with type hints to enable\n        type inference by other pipewine components. Failing to do so will result in\n        type errors when composing operators into workflows, or when registering\n        them to the Pipewine CLI.\n\n        Returns:\n            T: The produced dataset (or collection).\n        \"\"\"\n        pass\n\n    @property\n    def output_type(self):\n        \"\"\"Infer the origin type of this source's output, returning the origin of the\n        `__call__` method's return.\n        \"\"\"\n        return origin_type(get_annotations(self.__call__, eval_str=True)[\"return\"])\n</code></pre>"},{"location":"autoapi/pipewine/sources/base/#pipewine.sources.base.DatasetSource.output_type","title":"<code>output_type</code>  <code>property</code>","text":"<p>Infer the origin type of this source's output, returning the origin of the <code>__call__</code> method's return.</p>"},{"location":"autoapi/pipewine/sources/base/#pipewine.sources.base.DatasetSource.__call__","title":"<code>__call__()</code>  <code>abstractmethod</code>","text":"<p>Produce a dataset (or collection).</p> <p>This method must always be correctly annotated with type hints to enable type inference by other pipewine components. Failing to do so will result in type errors when composing operators into workflows, or when registering them to the Pipewine CLI.</p> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The produced dataset (or collection).</p> Source code in <code>pipewine/sources/base.py</code> <pre><code>@abstractmethod\ndef __call__(self) -&gt; T:\n    \"\"\"Produce a dataset (or collection).\n\n    This method **must** always be correctly annotated with type hints to enable\n    type inference by other pipewine components. Failing to do so will result in\n    type errors when composing operators into workflows, or when registering\n    them to the Pipewine CLI.\n\n    Returns:\n        T: The produced dataset (or collection).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/sources/images_folder/","title":"images_folder","text":"<p>Source that reads the dataset from a folder of images.</p>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImageSample","title":"<code>ImageSample</code>","text":"<p>               Bases: <code>TypedSample</code></p> <p>Sample that contains an image and its label.</p> Source code in <code>pipewine/sources/images_folder.py</code> <pre><code>class ImageSample(TypedSample):\n    \"\"\"Sample that contains an image and its label.\"\"\"\n\n    image: Item[np.ndarray]\n    \"\"\"The image data.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImageSample.image","title":"<code>image</code>  <code>instance-attribute</code>","text":"<p>The image data.</p>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImagesFolderSource","title":"<code>ImagesFolderSource</code>","text":"<p>               Bases: <code>DatasetSource[Dataset[ImageSample]]</code></p> <p>Source that reads the dataset from a folder of images.</p> Source code in <code>pipewine/sources/images_folder.py</code> <pre><code>class ImagesFolderSource(DatasetSource[Dataset[ImageSample]]):\n    \"\"\"Source that reads the dataset from a folder of images.\"\"\"\n\n    def __init__(self, folder: Path, recursive: bool = False) -&gt; None:\n        \"\"\"\n        Args:\n            folder (Path): Path to the folder where the dataset is stored.\n            recursive (bool, optional): Whether to search for images recursively in the\n                folder. Defaults to False.\n        \"\"\"\n        super().__init__()\n        self._folder = folder\n        self._recursive = recursive\n        self._found_files = []\n\n    @property\n    def folder(self) -&gt; Path:\n        \"\"\"Path to the folder where the dataset is stored.\"\"\"\n        return self._folder\n\n    @property\n    def is_recursive(self) -&gt; bool:\n        \"\"\"Whether to search for images recursively in the folder.\"\"\"\n        return self._recursive\n\n    def _prepare(self) -&gt; None:\n        if not self._folder.exists() or not self._folder.is_dir():\n            raise NotADirectoryError(self._folder)\n        if self._recursive:\n            _files_unsorted = list(self._folder.rglob(\"*\"))\n        else:\n            _files_unsorted = list(self._folder.iterdir())\n\n        _files_unsorted = [f for f in _files_unsorted if f.suffix in _image_extensions]\n        self._found_files = sorted(_files_unsorted)\n\n    def _get_sample(self, idx: int) -&gt; ImageSample:\n        file = self._found_files[idx]\n        ptype = ParserRegistry.get(file.suffix[1:])\n        assert ptype is not None, f\"No parser found for file {file}\"\n        image = StoredItem(LocalFileReader(file), ptype(), shared=False)\n        return ImageSample(image=image)\n\n    def __call__(self) -&gt; Dataset[ImageSample]:\n        self._prepare()\n        return LazyDataset(len(self._found_files), self._get_sample)\n</code></pre>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImagesFolderSource.folder","title":"<code>folder</code>  <code>property</code>","text":"<p>Path to the folder where the dataset is stored.</p>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImagesFolderSource.is_recursive","title":"<code>is_recursive</code>  <code>property</code>","text":"<p>Whether to search for images recursively in the folder.</p>"},{"location":"autoapi/pipewine/sources/images_folder/#pipewine.sources.images_folder.ImagesFolderSource.__init__","title":"<code>__init__(folder, recursive=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>folder</code> <code>Path</code> <p>Path to the folder where the dataset is stored.</p> required <code>recursive</code> <code>bool</code> <p>Whether to search for images recursively in the folder. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/sources/images_folder.py</code> <pre><code>def __init__(self, folder: Path, recursive: bool = False) -&gt; None:\n    \"\"\"\n    Args:\n        folder (Path): Path to the folder where the dataset is stored.\n        recursive (bool, optional): Whether to search for images recursively in the\n            folder. Defaults to False.\n    \"\"\"\n    super().__init__()\n    self._folder = folder\n    self._recursive = recursive\n    self._found_files = []\n</code></pre>"},{"location":"autoapi/pipewine/sources/underfolder/","title":"underfolder","text":"<p>Source that reads the dataset from file system using Pipewine \"Underfolder\" format.</p>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource","title":"<code>UnderfolderSource</code>","text":"<p>               Bases: <code>DatasetSource[Dataset[T]]</code></p> <p>Source that reads the dataset from file system using Pipewine \"Underfolder\" format.</p> Source code in <code>pipewine/sources/underfolder.py</code> <pre><code>class UnderfolderSource[T: Sample](DatasetSource[Dataset[T]]):\n    \"\"\"Source that reads the dataset from file system using Pipewine \"Underfolder\" format.\"\"\"\n\n    def __init__(self, folder: Path, sample_type: type[T] | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            folder (Path): Path to the folder where the dataset is stored.\n            sample_type (type[T] | None, optional): Type of the samples to produce.\n                Defaults to None (TypelessSample).\n        \"\"\"\n        super().__init__()\n        self._folder = folder\n        self._root_files: dict[str, Path] = {}\n        self._root_items: dict[str, StoredItem] = {}\n        self._sample_files: list[dict[str, Path]] = []\n        self._sample_type = sample_type or TypelessSample\n\n    @classmethod\n    def data_path(cls, root_folder: Path) -&gt; Path:\n        \"\"\"The path to the data folder inside the root folder.\"\"\"\n        return root_folder / \"data\"\n\n    @property\n    def sample_type(self) -&gt; type[T] | type[TypelessSample]:\n        \"\"\"Type of the samples produced by this source.\"\"\"\n        return self._sample_type\n\n    @property\n    def folder(self) -&gt; Path:\n        \"\"\"Path to the folder where the dataset is stored.\"\"\"\n        return self._folder\n\n    @property\n    def data_folder(self) -&gt; Path:\n        \"\"\"Path to the data folder inside the root folder.\"\"\"\n        return self.data_path(self.folder)\n\n    def _extract_key(self, name: str) -&gt; str:\n        return name.partition(\".\")[0]\n\n    def _extract_id_key(self, name: str) -&gt; tuple[int, str] | None:\n        id_key_split = name.partition(\"_\")\n        if not id_key_split[2]:\n            warnings.warn(\n                f\"{self.__class__}: cannot parse file name '{name}' as &lt;id&gt;_&lt;key&gt;.&lt;ext&gt;\"\n            )\n            return None\n        try:\n            return (int(id_key_split[0]), self._extract_key(id_key_split[2]))\n        except ValueError:\n            warnings.warn(\n                f\"{self.__class__}: file name '{name}' does not start with an integer\"\n            )\n            return None\n\n    def _scan_root_files(self) -&gt; None:\n        if not self._folder.exists():\n            raise NotADirectoryError(f\"Folder '{self._folder}' does not exist.\")\n\n        root_items: dict[str, Path] = {}\n        with os.scandir(str(self._folder)) as it:\n            for entry in it:\n                if entry.is_file():\n                    key = self._extract_key(entry.name)\n                    if key:\n                        root_items[key] = Path(entry.path)\n        self._root_files = root_items\n\n    def _scan_sample_files(self) -&gt; None:\n        data_folder = self.data_folder\n        if not data_folder.exists():\n            raise NotADirectoryError(f\"Folder '{data_folder}' does not exist.\")\n\n        sample_files: list[dict[str, Path]] = []\n        with os.scandir(str(data_folder)) as it:\n            for entry in it:\n                if entry.is_file():\n                    id_key = self._extract_id_key(entry.name)\n                    if id_key:\n                        sample_files.extend(\n                            ({} for _ in range(id_key[0] - len(sample_files) + 1))\n                        )\n                        sample_files[id_key[0]][id_key[1]] = Path(entry.path)\n        self._sample_files = sample_files\n\n    def _prepare(self) -&gt; None:\n        self._scan_root_files()\n        self._scan_sample_files()\n\n    def _size(self) -&gt; int:\n        return len(self._sample_files)\n\n    def _get_item(self, k: str, v: Path) -&gt; StoredItem | None:\n        maybe_root = self._root_items.get(k)\n        if maybe_root is not None:\n            return maybe_root\n        ext = v.suffix[1:]\n        parser_type = ParserRegistry.get(ext)\n        if parser_type is None:\n            warnings.warn(\n                f\"No parser found for extension '{ext}', make sure the extension \"\n                \"is correct and/or implement a custom Parser for it.\",\n            )\n            return None\n        reader = LocalFileReader(v)\n        annotated_type = None\n        if issubclass(self.sample_type, TypedSample):\n            annotation = get_annotations(self.sample_type, eval_str=True).get(k)\n            if (\n                annotation is not None\n                and hasattr(annotation, \"__args__\")\n                and len(annotation.__args__) &gt; 0\n            ):\n                annotated_type = origin_type(annotation.__args__[0])\n        parser = parser_type(type_=annotated_type)\n        if k in self._root_files:\n            result = StoredItem(reader, parser, shared=True)\n            self._root_items[k] = result\n        else:\n            result = StoredItem(reader, parser, shared=False)\n        return result\n\n    def _get_sample(self, idx: int) -&gt; T:\n        data = {}\n        for k, v in chain(self._sample_files[idx].items(), self._root_files.items()):\n            item = self._get_item(k, v)\n            if item is not None:\n                data[k] = item\n        return self.sample_type(**data)  # type: ignore\n\n    def __call__(self) -&gt; Dataset[T]:\n        self._prepare()\n        return LazyDataset(self._size(), self._get_sample)\n</code></pre>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource.data_folder","title":"<code>data_folder</code>  <code>property</code>","text":"<p>Path to the data folder inside the root folder.</p>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource.folder","title":"<code>folder</code>  <code>property</code>","text":"<p>Path to the folder where the dataset is stored.</p>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource.sample_type","title":"<code>sample_type</code>  <code>property</code>","text":"<p>Type of the samples produced by this source.</p>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource.__init__","title":"<code>__init__(folder, sample_type=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>folder</code> <code>Path</code> <p>Path to the folder where the dataset is stored.</p> required <code>sample_type</code> <code>type[T] | None</code> <p>Type of the samples to produce. Defaults to None (TypelessSample).</p> <code>None</code> Source code in <code>pipewine/sources/underfolder.py</code> <pre><code>def __init__(self, folder: Path, sample_type: type[T] | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        folder (Path): Path to the folder where the dataset is stored.\n        sample_type (type[T] | None, optional): Type of the samples to produce.\n            Defaults to None (TypelessSample).\n    \"\"\"\n    super().__init__()\n    self._folder = folder\n    self._root_files: dict[str, Path] = {}\n    self._root_items: dict[str, StoredItem] = {}\n    self._sample_files: list[dict[str, Path]] = []\n    self._sample_type = sample_type or TypelessSample\n</code></pre>"},{"location":"autoapi/pipewine/sources/underfolder/#pipewine.sources.underfolder.UnderfolderSource.data_path","title":"<code>data_path(root_folder)</code>  <code>classmethod</code>","text":"<p>The path to the data folder inside the root folder.</p> Source code in <code>pipewine/sources/underfolder.py</code> <pre><code>@classmethod\ndef data_path(cls, root_folder: Path) -&gt; Path:\n    \"\"\"The path to the data folder inside the root folder.\"\"\"\n    return root_folder / \"data\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/","title":"workflows","text":"<p>Package for Pipewine Workflows and all related components.</p>"},{"location":"autoapi/pipewine/workflows/#pipewine.workflows.draw_workflow","title":"<code>draw_workflow(workflow, path, layout=None, drawer=None)</code>","text":"<p>Utility function to easily draw a workflow.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>The workflow to draw.</p> required <code>path</code> <code>Path</code> <p>The path to save the drawing to.</p> required <code>layout</code> <code>Layout | None</code> <p>The layout to use. Defaults to None, in which case an OptimizedLayout is used.</p> <code>None</code> <code>drawer</code> <code>Drawer | None</code> <p>The drawer to use. Defaults to None, in which case an SVGDrawer is used.</p> <code>None</code> Source code in <code>pipewine/workflows/__init__.py</code> <pre><code>def draw_workflow(\n    workflow: Workflow,\n    path: Path,\n    layout: Layout | None = None,\n    drawer: Drawer | None = None,\n) -&gt; None:\n    \"\"\"Utility function to easily draw a workflow.\n\n    Args:\n        workflow (Workflow): The workflow to draw.\n        path (Path): The path to save the drawing to.\n        layout (Layout | None, optional): The layout to use. Defaults to None, in\n            which case an OptimizedLayout is used.\n        drawer (Drawer | None, optional): The drawer to use. Defaults to None, in\n            which case an SVGDrawer is used.\n    \"\"\"\n    layout = layout or OptimizedLayout()\n    drawer = drawer or SVGDrawer()\n    vg = layout.layout(workflow)\n    with open(path, \"wb\") as fp:\n        drawer.draw(vg, fp)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/#pipewine.workflows.run_workflow","title":"<code>run_workflow(workflow, executor=None, event_queue=None, tracker=None)</code>","text":"<p>Utility function to easily run a workflow, optionally attaching an event queue and a tracker to it.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>The workflow to run.</p> required <code>executor</code> <code>WorkflowExecutor | None</code> <p>The workflow executor to use. Defaults to None, in which case a SequentialWorkflowExecutor is used.</p> <code>None</code> <code>event_queue</code> <code>EventQueue | None</code> <p>The event queue to use. Defaults to None, in which case a ProcessSharedEventQueue is used. The event queue is not used if the tracker is not provided.</p> <code>None</code> <code>tracker</code> <code>Tracker | None</code> <p>The tracker to use. Defaults to None, in which case no tracker is used.</p> <code>None</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If an exception occurs during the execution of the workflow, cleaning up the used resources and re-raising it transparently.</p> Source code in <code>pipewine/workflows/__init__.py</code> <pre><code>def run_workflow(\n    workflow: Workflow,\n    executor: WorkflowExecutor | None = None,\n    event_queue: EventQueue | None = None,\n    tracker: Tracker | None = None,\n) -&gt; None:\n    \"\"\"Utility function to easily run a workflow, optionally attaching an event queue\n    and a tracker to it.\n\n    Args:\n        workflow (Workflow): The workflow to run.\n        executor (WorkflowExecutor | None, optional): The workflow executor to use.\n            Defaults to None, in which case a SequentialWorkflowExecutor is used.\n        event_queue (EventQueue | None, optional): The event queue to use. Defaults to\n            None, in which case a ProcessSharedEventQueue is used. The event queue is\n            not used if the tracker is not provided.\n        tracker (Tracker | None, optional): The tracker to use. Defaults to None, in\n            which case no tracker is used.\n\n    Raises:\n        Exception: If an exception occurs during the execution of the workflow, cleaning\n            up the used resources and re-raising it transparently.\n    \"\"\"\n    executor = executor or SequentialWorkflowExecutor()\n    event_queue = event_queue or ProcessSharedEventQueue()\n    success = True\n    try:\n        if event_queue and tracker:\n            event_queue.start()\n            executor.attach(event_queue)\n            tracker.attach(event_queue)\n        executor.execute(workflow)\n    except BaseException as e:\n        success = False\n        raise e\n    finally:\n        if event_queue and tracker:\n            executor.detach()\n            tracker.detach(graceful=success)\n            event_queue.close()\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/","title":"drawing","text":"<p>Workflow visualization utilities.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.Drawer","title":"<code>Drawer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for drawing algorithms, which are responsible for rendering a view graph to a file.</p> <p>Subclasses must implement the draw method, which receives a view graph and a binary buffer and writes the drawing to the buffer.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>class Drawer(ABC):\n    \"\"\"Base class for drawing algorithms, which are responsible for rendering a view\n    graph to a file.\n\n    Subclasses must implement the draw method, which receives a view graph and a binary\n    buffer and writes the drawing to the buffer.\n    \"\"\"\n\n    @abstractmethod\n    def draw(self, vg: ViewGraph, buffer: BinaryIO) -&gt; None:\n        \"\"\"Renders a view graph to a file.\n\n        Args:\n            vg (ViewGraph): The view graph to draw.\n            buffer (BinaryIO): The binary buffer to write the drawing to.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.Drawer.draw","title":"<code>draw(vg, buffer)</code>  <code>abstractmethod</code>","text":"<p>Renders a view graph to a file.</p> <p>Parameters:</p> Name Type Description Default <code>vg</code> <code>ViewGraph</code> <p>The view graph to draw.</p> required <code>buffer</code> <code>BinaryIO</code> <p>The binary buffer to write the drawing to.</p> required Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>@abstractmethod\ndef draw(self, vg: ViewGraph, buffer: BinaryIO) -&gt; None:\n    \"\"\"Renders a view graph to a file.\n\n    Args:\n        vg (ViewGraph): The view graph to draw.\n        buffer (BinaryIO): The binary buffer to write the drawing to.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.Layout","title":"<code>Layout</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for layout algorithms, which are responsible for arranging the nodes of a workflow in a 2D space in  an aesthetically pleasing way.</p> <p>Subclasses must implement the layout method, which receives a workflow and returns a view graph.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>class Layout(ABC):\n    \"\"\"Base class for layout algorithms, which are responsible for arranging the nodes\n    of a workflow in a 2D space in  an aesthetically pleasing way.\n\n    Subclasses must implement the layout method, which receives a workflow and returns\n    a view graph.\n    \"\"\"\n\n    @abstractmethod\n    def layout(self, wf: Workflow) -&gt; ViewGraph:\n        \"\"\"Arranges the nodes of a workflow in a 2D space in an aesthetically pleasing\n        way.\n\n        Args:\n            wf (Workflow): The workflow to layout.\n\n        Returns:\n            ViewGraph: The view graph representing the layout of the workflow.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.Layout.layout","title":"<code>layout(wf)</code>  <code>abstractmethod</code>","text":"<p>Arranges the nodes of a workflow in a 2D space in an aesthetically pleasing way.</p> <p>Parameters:</p> Name Type Description Default <code>wf</code> <code>Workflow</code> <p>The workflow to layout.</p> required <p>Returns:</p> Name Type Description <code>ViewGraph</code> <code>ViewGraph</code> <p>The view graph representing the layout of the workflow.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>@abstractmethod\ndef layout(self, wf: Workflow) -&gt; ViewGraph:\n    \"\"\"Arranges the nodes of a workflow in a 2D space in an aesthetically pleasing\n    way.\n\n    Args:\n        wf (Workflow): The workflow to layout.\n\n    Returns:\n        ViewGraph: The view graph representing the layout of the workflow.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.OptimizedLayout","title":"<code>OptimizedLayout</code>","text":"<p>               Bases: <code>Layout</code></p> <p>A layout algorithm based on a genetic algorithm that optimizes the layout of a workflow in a 2D space.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>class OptimizedLayout(Layout):\n    \"\"\"A layout algorithm based on a genetic algorithm that optimizes the layout of a\n    workflow in a 2D space.\n    \"\"\"\n\n    RGB_SOURCE = (10, 160, 40)\n    RGB_OP = (0, 80, 210)\n    RGB_SINK = (220, 30, 10)\n\n    def __init__(\n        self,\n        optimize_steps: int = 5000,\n        optimize_population: int = 32,\n        optimize_time_budget: float = 3.0,\n        optimize_noise_start: float = 10.0,\n        verbose: bool = False,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            optimize_steps (int, optional): The number of optimization steps. Defaults\n                to 5000.\n            optimize_population (int, optional): The size of the population. Defaults to\n                32.\n            optimize_time_budget (float, optional): The maximum time budget for the\n                optimization. Defaults to 3.0.\n            optimize_noise_start (float, optional): The initial noise level. Defaults to\n                10.0.\n            verbose (bool, optional): Whether to print verbose information. Defaults to\n                False.\n        \"\"\"\n        super().__init__()\n        self._optimize_steps = optimize_steps\n        self._optimize_population = optimize_population\n        self._optimize_time_budget = optimize_time_budget\n        self._optimize_noise_start = optimize_noise_start\n        self._verbose = verbose\n\n    def _asap_sort(self, workflow: Workflow) -&gt; list[list[Node]]:\n        result: list[list[Node]] = []\n        queue: deque[Node] = deque()\n        deps: dict[Node, set[Node]] = {}\n        for node in workflow.get_nodes():\n            inbounds = workflow.get_inbound_edges(node)\n            deps[node] = {x.src.node for x in inbounds}\n            if not inbounds:\n                queue.append(node)\n        while queue:\n            layer: list[Node] = []\n            size = len(queue)\n            for _ in range(size):\n                node = queue.popleft()\n                layer.append(node)\n                for edge in workflow.get_outbound_edges(node):\n                    deps[edge.dst.node].remove(node)\n                    if not deps[edge.dst.node]:\n                        queue.append(edge.dst.node)\n            result.append(layer)\n        return result\n\n    def _socket_name(self, socket: int | str | None, default: str) -&gt; str:\n        return default if socket is None else str(socket)\n\n    def _optimize(self, vg: ViewGraph) -&gt; None:\n        xoffset = np.array([[[1, 0.0]]], dtype=np.float32)\n        n = len(vg.nodes)\n        e = len(vg.edges)\n        layout = np.array([x.position for x in vg.nodes], dtype=np.float32)\n        sizes = np.array([x.size for x in vg.nodes], dtype=np.float32)\n        start_node_idx = np.array([e.start[0] for e in vg.edges])\n        end_node_idx = np.array([e.end[0] for e in vg.edges])\n        start_socket_rel = np.array(\n            [\n                [\n                    sizes[edge.start[0], 0],\n                    vg.nodes[edge.start[0]].outputs[edge.start[1]][1],\n                ]\n                for edge in vg.edges\n            ],\n            dtype=np.float32,\n        )\n        end_socket_rel = np.array(\n            [[0.0, vg.nodes[edge.end[0]].inputs[edge.end[1]][1]] for edge in vg.edges],\n            dtype=np.float32,\n        )\n\n        w, h = layout.max(axis=0) * 1.5\n        maxdist: float = (h**2 + w**2) ** 0.5\n        maxsize = float(sizes.max())\n\n        def fitness_fn(layout: np.ndarray) -&gt; np.ndarray:\n            b = layout.shape[0]\n            start_offsets = np.take_along_axis(layout, start_node_idx[None, :, None], 1)\n            end_offsets = np.take_along_axis(layout, end_node_idx[None, :, None], 1)\n            edge_start = start_offsets + start_socket_rel\n            edge_end = end_offsets + end_socket_rel\n\n            # Minimize edge length\n            dist = np.linalg.norm(edge_end - edge_start, ord=2, axis=-1)\n            bound = 3 * maxsize\n            norm_dist: np.ndarray = 1.0 - np.clip(dist - maxsize, 0.0, bound) / bound\n            edge_distance_term = (norm_dist**2).mean(-1)\n\n            # Keep nodes reasonably distanced\n            # The chebyshev distance with the closest node should be close to the\n            # maximum node size.\n            cdist = np.max(np.abs(layout[:, None] - layout[:, :, None]), axis=-1)\n            cdist += np.eye(n, dtype=np.float32)[None].repeat(b, 0) * maxdist\n            mindist = cdist.min(axis=-1)\n            node_distance_term = (np.clip(mindist, None, maxsize) / maxsize).mean(-1)\n\n            # Edge straightness\n            dy = edge_end[..., 1] - edge_start[..., 1]\n            dx = edge_end[..., 0] - edge_start[..., 0]\n            edge_straightness = (1 - np.abs(np.atan2(dy, dx)) / (np.pi / 2)).clip(0, 1)\n            edge_straightness_term = edge_straightness.mean(-1)\n\n            # Penalty on edge/edge crossings\n            mat = _lines_intersect_matrix(edge_start - xoffset, edge_end + xoffset)\n            edge_edge_cross_term = (e - mat.any(-1).sum(-1) / 2) / e\n\n            # Penalty on edge/node crossings\n            mat = _lines_rects_intersect_matrix(\n                edge_start + xoffset, edge_end - xoffset, layout, sizes[None]\n            )\n            edge_node_cross_term = (e - mat.any(-1).sum(-1)) / e\n\n            return np.stack(\n                [\n                    edge_distance_term,\n                    node_distance_term,\n                    edge_straightness_term,\n                    edge_edge_cross_term,\n                    edge_node_cross_term,\n                ],\n                -1,\n            )\n\n        def spawn(layouts: np.ndarray, sigma: float) -&gt; np.ndarray:\n            self_idx = np.random.randint(0, n - 1, layouts.shape[0])\n            xvals, yvals = layouts[..., 0], layouts[..., 1]\n            x_self = np.take_along_axis(xvals, self_idx[:, None], 1)\n            y_self = np.take_along_axis(yvals, self_idx[:, None], 1)\n            xdist = np.abs(xvals - x_self)\n            np.put_along_axis(xdist, self_idx[:, None], xdist.max() + 1, 1)\n            nn_idx = xdist.argmin(-1)\n            xnn_dist = xdist.min(-1)\n            nn_idx = np.where(xnn_dist &lt; maxsize / 2, nn_idx, self_idx)\n            x_nn = np.take_along_axis(xvals, nn_idx[:, None], 1)\n            y_nn = np.take_along_axis(yvals, nn_idx[:, None], 1)\n            np.put_along_axis(xvals, self_idx[:, None], x_nn, 1)\n            np.put_along_axis(xvals, nn_idx[:, None], x_self, 1)\n            np.put_along_axis(yvals, self_idx[:, None], y_nn, 1)\n            np.put_along_axis(yvals, nn_idx[:, None], y_self, 1)\n            noise = np.random.normal(0.0, sigma, layouts.shape).astype(np.float32)\n            return layouts + noise\n\n        global_step = 0\n        max_steps = self._optimize_steps\n        max_population = self._optimize_population\n        max_time = self._optimize_time_budget\n        sigma_s = self._optimize_noise_start\n        sigma_e = sigma_s * 1e-4\n        argbest = layout.copy()\n        layouts = layout[None].repeat(max_population, 0)\n        best = -float(\"inf\")\n        layouts = spawn(layouts, sigma_s)\n        layouts[-1] = argbest.copy()\n        t_start = time.time()\n        while True:\n            elapsed = time.time() - t_start\n            if elapsed &gt; max_time or global_step &gt; max_steps:\n                break\n            fitness_comp = fitness_fn(layouts)\n            fitness = fitness_comp.mean(-1)\n            amaxfitness = int(np.argmax(fitness))\n            maxfitness = float(fitness[amaxfitness])\n            if maxfitness &gt; best:\n                best = maxfitness\n                argbest = layouts[amaxfitness].copy()\n                if self._verbose:\n                    print(\n                        f\"Step {global_step}/{max_steps} | \"\n                        f\"Elapsed (s) {round(elapsed, 2)}/{max_time} | \"\n                        f\"Fitness {best} | \"\n                        f\"Fitness components {fitness_comp[amaxfitness]}\"\n                    )\n            fitness = (fitness - fitness.min()) / (fitness.max() - fitness.min() + 1e-5)\n            fsum = fitness.sum()\n            next_idx = np.random.choice(\n                max_population, size=max_population - 1, p=fitness / fsum.clip(min=1e-3)\n            )\n            sigma = sigma_s * np.exp(\n                np.log(sigma_e / sigma_s) * global_step / max_steps\n            )\n            layouts[:-1] = spawn(\n                np.take_along_axis(layouts, next_idx[:, None, None], 0), sigma\n            )\n            layouts[-1] = argbest.copy()\n            global_step += 1\n\n        minxy = argbest.min(axis=0)\n        for i, xy in enumerate(argbest):\n            vg.nodes[i].position = tuple(xy - minxy)\n\n    def layout(self, wf: Workflow) -&gt; ViewGraph:\n        view_nodes: dict[Node, ViewNode] = {}\n        view_edges: list[ViewEdge] = []\n        node_to_idx: dict[Node, int] = {}\n        i = 0\n        margin = 32\n        node_distance = 96\n        fontsize = 16\n        current_x = 0.0\n        for layer in self._asap_sort(wf):\n            current_y = 0.0\n            maxw = 0.0\n            for node in layer:\n                node_to_idx[node] = i\n                inputs: set[str] = set()\n                outputs: set[str] = set()\n                has_collection_input = False\n                has_collection_output = False\n                for e in wf.get_inbound_edges(node):\n                    if not isinstance(e.dst.socket, All):\n                        inputs.add(self._socket_name(e.dst.socket, \"input\"))\n                    else:\n                        has_collection_input = True\n                for e in wf.get_outbound_edges(node):\n                    if not isinstance(e.src.socket, All):\n                        outputs.add(self._socket_name(e.src.socket, \"output\"))\n                    else:\n                        has_collection_output = True\n                if len(wf.get_inbound_edges(node)) == 0:\n                    col = self.RGB_SOURCE\n                elif len(wf.get_outbound_edges(node)) == 0:\n                    col = self.RGB_SINK\n                else:\n                    col = self.RGB_OP\n\n                title_w, title_h = _get_text_size(node.name, fontsize)\n                in_sockets_w = in_sockets_h = out_sockets_w = out_sockets_h = 0.0\n                in_sockets: list[tuple[str, float]] = []\n                out_sockets: list[tuple[str, float]] = []\n                current = title_h\n                m2 = margin / 2\n                if has_collection_input:\n                    socket = \"inputs\"\n                    sw, sh = _get_text_size(socket, fontsize)\n                    in_sockets_w = max(in_sockets_w, sw)\n                    in_sockets_h += m2 + sh\n                    current += m2 + sh\n                    in_sockets.append((socket, current))\n                for socket in sorted(inputs):\n                    sw, sh = _get_text_size(socket, fontsize)\n                    in_sockets_w = max(in_sockets_w, sw)\n                    in_sockets_h += m2 + sh\n                    current += m2 + sh\n                    in_sockets.append((socket, current))\n                current = title_h\n                if has_collection_output:\n                    socket = \"outputs\"\n                    sw, sh = _get_text_size(socket, fontsize)\n                    out_sockets_w = max(out_sockets_w, sw)\n                    out_sockets_h += m2 + sh\n                    current += m2 + sh\n                    out_sockets.append((socket, current))\n                for socket in sorted(outputs):\n                    sw, sh = _get_text_size(socket, fontsize)\n                    out_sockets_w = max(out_sockets_w, sw)\n                    out_sockets_h += m2 + sh\n                    current += m2 + sh\n                    out_sockets.append((socket, current))\n\n                w = max(title_w, in_sockets_w + out_sockets_w) + margin\n                h = title_h + max(in_sockets_h, out_sockets_h) + margin\n\n                view_nodes[node] = ViewNode(\n                    title=node.name,\n                    fontsize=fontsize,\n                    position=(float(current_x), float(current_y)),\n                    size=(w, h),\n                    color=col,\n                    inputs=in_sockets,\n                    outputs=out_sockets,\n                    has_collection_input=has_collection_input,\n                    has_collection_output=has_collection_output,\n                )\n                current_y += h + node_distance\n                maxw = max(maxw, w)\n                i += 1\n            current_x += maxw + node_distance\n        for node in wf.get_nodes():\n            for e in wf.get_outbound_edges(node):\n                src_node = view_nodes[node]\n                dst_node = view_nodes[e.dst.node]\n                src_node_idx = node_to_idx[node]\n                dst_node_idx = node_to_idx[e.dst.node]\n                src_s_idx = dst_s_idx = 0\n                if not isinstance(e.src.socket, All):\n                    src_s_idx = [x[0] for x in src_node.outputs].index(\n                        self._socket_name(e.src.socket, \"output\")\n                    )\n                if not isinstance(e.dst.socket, All):\n                    dst_s_idx = [x[0] for x in dst_node.inputs].index(\n                        self._socket_name(e.dst.socket, \"input\")\n                    )\n                view_edges.append(\n                    ViewEdge((src_node_idx, src_s_idx), (dst_node_idx, dst_s_idx))\n                )\n        vg = ViewGraph(list(view_nodes.values()), view_edges)\n        if len(view_nodes) &gt; 1:\n            self._optimize(vg)\n        return vg\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.OptimizedLayout.__init__","title":"<code>__init__(optimize_steps=5000, optimize_population=32, optimize_time_budget=3.0, optimize_noise_start=10.0, verbose=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>optimize_steps</code> <code>int</code> <p>The number of optimization steps. Defaults to 5000.</p> <code>5000</code> <code>optimize_population</code> <code>int</code> <p>The size of the population. Defaults to 32.</p> <code>32</code> <code>optimize_time_budget</code> <code>float</code> <p>The maximum time budget for the optimization. Defaults to 3.0.</p> <code>3.0</code> <code>optimize_noise_start</code> <code>float</code> <p>The initial noise level. Defaults to 10.0.</p> <code>10.0</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose information. Defaults to False.</p> <code>False</code> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>def __init__(\n    self,\n    optimize_steps: int = 5000,\n    optimize_population: int = 32,\n    optimize_time_budget: float = 3.0,\n    optimize_noise_start: float = 10.0,\n    verbose: bool = False,\n) -&gt; None:\n    \"\"\"\n    Args:\n        optimize_steps (int, optional): The number of optimization steps. Defaults\n            to 5000.\n        optimize_population (int, optional): The size of the population. Defaults to\n            32.\n        optimize_time_budget (float, optional): The maximum time budget for the\n            optimization. Defaults to 3.0.\n        optimize_noise_start (float, optional): The initial noise level. Defaults to\n            10.0.\n        verbose (bool, optional): Whether to print verbose information. Defaults to\n            False.\n    \"\"\"\n    super().__init__()\n    self._optimize_steps = optimize_steps\n    self._optimize_population = optimize_population\n    self._optimize_time_budget = optimize_time_budget\n    self._optimize_noise_start = optimize_noise_start\n    self._verbose = verbose\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.SVGDrawer","title":"<code>SVGDrawer</code>","text":"<p>               Bases: <code>Drawer</code></p> <p>A drawing algorithm that renders a view graph to an SVG file.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>class SVGDrawer(Drawer):\n    \"\"\"A drawing algorithm that renders a view graph to an SVG file.\"\"\"\n\n    RGB_TEXT = \"white\"\n\n    def _draw_node(self, parent: ET.Element, node: ViewNode) -&gt; None:\n        text = node.title\n        w, h = node.size\n        x, y = node.position\n        fontsize = node.fontsize\n        margin = 15\n        sock_rad = 8\n\n        col = f\"rgb{node.color}\"\n        ET.SubElement(\n            parent,\n            \"rect\",\n            x=str(x),\n            y=str(y),\n            width=str(w),\n            height=str(h),\n            fill=col,\n            rx=\"10\",\n        )\n        text_elem = ET.SubElement(\n            parent,\n            \"text\",\n            x=str(x + w / 2),\n            y=str(y + (fontsize + margin) / 2),\n            fill=\"white\",\n        )\n        text_elem.set(\"text-anchor\", \"middle\")\n        text_elem.set(\"dominant-baseline\", \"central\")\n        text_elem.set(\"font-size\", str(fontsize))\n        text_elem.set(\"font-family\", \"Arial\")\n        text_elem.set(\"font-weight\", \"bold\")\n        text_elem.text = text\n\n        for i, (socket, sy) in enumerate(node.inputs):\n            if i == 0 and node.has_collection_input:\n                rectangle = ET.SubElement(\n                    parent,\n                    \"rect\",\n                    x=str(x - margin),\n                    y=str(y + sy - margin),\n                    width=str(w / 2 + margin),\n                    height=str(node.inputs[-1][1] + 2 * margin - sy),\n                    stroke=\"black\",\n                    fill=\"#00000000\",\n                    rx=\"10\",\n                )\n                rectangle.set(\"stroke-width\", \"2\")\n                text_elem = ET.SubElement(\n                    parent, \"text\", x=str(x + 12), y=str(y + sy), fill=\"white\"\n                )\n                text_elem.set(\"dominant-baseline\", \"central\")\n                text_elem.set(\"font-size\", str(fontsize - 4))\n                text_elem.set(\"font-family\", \"Arial\")\n                text_elem.set(\"font-weight\", \"bold\")\n                text_elem.text = socket\n            else:\n                circle = ET.SubElement(\n                    parent,\n                    \"circle\",\n                    cx=str(x),\n                    cy=str(y + sy),\n                    r=str(sock_rad),\n                    fill=\"white\",\n                    stroke=col,\n                )\n                circle.set(\"stroke-width\", \"2\")\n                text_elem = ET.SubElement(\n                    parent,\n                    \"text\",\n                    x=str(x + sock_rad * 1.5),\n                    y=str(y + sy),\n                    fill=\"white\",\n                )\n                text_elem.set(\"dominant-baseline\", \"central\")\n                text_elem.set(\"font-size\", str(fontsize - 4))\n                text_elem.set(\"font-family\", \"Arial\")\n                text_elem.text = socket\n        for i, (socket, sy) in enumerate(node.outputs):\n            if i == 0 and node.has_collection_output:\n                rectangle = ET.SubElement(\n                    parent,\n                    \"rect\",\n                    x=str(x + w / 2),\n                    y=str(y + sy - margin),\n                    width=str(w / 2 + margin),\n                    height=str(node.outputs[-1][1] + 2 * margin - sy),\n                    stroke=\"black\",\n                    fill=\"#00000000\",\n                    rx=\"10\",\n                )\n                rectangle.set(\"stroke-width\", \"2\")\n                text_elem = ET.SubElement(\n                    parent,\n                    \"text\",\n                    x=str(x + w - sock_rad * 1.5),\n                    y=str(y + sy),\n                    fill=\"white\",\n                )\n                text_elem.set(\"text-anchor\", \"end\")\n                text_elem.set(\"dominant-baseline\", \"central\")\n                text_elem.set(\"font-size\", str(fontsize - 4))\n                text_elem.set(\"font-family\", \"Arial\")\n                text_elem.set(\"font-weight\", \"bold\")\n                text_elem.text = socket\n            else:\n                circle = ET.SubElement(\n                    parent,\n                    \"circle\",\n                    cx=str(x + w),\n                    cy=str(y + sy),\n                    r=str(sock_rad),\n                    fill=\"white\",\n                    stroke=col,\n                )\n                circle.set(\"stroke-width\", \"2\")\n                text_elem = ET.SubElement(\n                    parent,\n                    \"text\",\n                    x=str(x + w - sock_rad * 1.5),\n                    y=str(y + sy),\n                    fill=\"white\",\n                )\n                text_elem.set(\"text-anchor\", \"end\")\n                text_elem.set(\"dominant-baseline\", \"central\")\n                text_elem.set(\"font-size\", str(fontsize - 4))\n                text_elem.set(\"font-family\", \"Arial\")\n                text_elem.text = socket\n\n    def _draw_edge(\n        self,\n        parent: ET.Element,\n        x1: float,\n        y1: float,\n        x2: float,\n        y2: float,\n        is_collection: bool,\n    ) -&gt; None:\n        if is_collection:\n            margin = 15\n            stroke_w = 6\n            head_size = 15\n        else:\n            margin = 9\n            stroke_w = 2\n            head_size = 7\n\n        x1 += margin\n        x2 -= margin + head_size + 2\n        h1 = (x2 - x1) * (0.5 - 0.134)\n        h2 = (x2 - x1) * (0.5 + 0.134)\n        line = ET.SubElement(\n            parent,\n            \"path\",\n            d=f\"M {x1} {y1} C {x1 + h1} {y1} {x1 + h2} {y2} {x2} {y2}\",\n            stroke=\"black\",\n            fill=\"none\",\n        )\n        line.set(\"stroke-width\", str(stroke_w))\n\n        tip = ET.SubElement(\n            parent,\n            \"path\",\n            d=f\"M{head_size},0 L0,-{head_size / 2} V{head_size / 2} Z\",\n            fill=\"black\",\n            stroke=\"black\",\n            transform=f\"translate({x2}, {y2})\",\n        )\n        tip.set(\"stroke-width\", \"4\")\n        tip.set(\"stroke-linejoin\", \"round\")\n\n    def draw(self, vg: ViewGraph, buffer: BinaryIO) -&gt; None:\n        margin = 32\n        svg = ET.Element(\"svg\", xmlns=\"http://www.w3.org/2000/svg\")\n        nodes_group = ET.Element(\"g\", transform=f\"translate({margin}, {margin})\")\n        edges_group = ET.Element(\"g\", transform=f\"translate({margin}, {margin})\")\n        groups: list[ET.Element] = []\n        for node in vg.nodes:\n            group = ET.SubElement(nodes_group, \"g\")\n            self._draw_node(group, node)\n            groups.append(group)\n        for edge in vg.edges:\n            src_node_idx, src_sock = edge.start\n            dst_node_idx, dst_sock = edge.end\n            src_node, dst_node = vg.nodes[src_node_idx], vg.nodes[dst_node_idx]\n            x1 = src_node.position[0] + src_node.size[0]\n            y1 = src_node.position[1] + src_node.outputs[src_sock][1]\n            x2 = dst_node.position[0]\n            y2 = dst_node.position[1] + dst_node.inputs[dst_sock][1]\n            is_collection = src_node.has_collection_output and src_sock == 0\n            self._draw_edge(edges_group, x1, y1, x2, y2, is_collection)\n\n        svg.append(nodes_group)\n        svg.append(edges_group)\n        svg.set(\n            \"width\",\n            str(max(node.position[0] + node.size[0] for node in vg.nodes) + 2 * margin),\n        )\n        svg.set(\n            \"height\",\n            str(max(node.position[1] + node.size[1] for node in vg.nodes) + 2 * margin),\n        )\n        ET.ElementTree(svg).write(buffer)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewEdge","title":"<code>ViewEdge</code>  <code>dataclass</code>","text":"<p>The representation of an edge in the view graph.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>@dataclass\nclass ViewEdge:\n    \"\"\"The representation of an edge in the view graph.\"\"\"\n\n    start: tuple[int, int]\n    \"\"\"The index of the source node and the source socket.\"\"\"\n    end: tuple[int, int]\n    \"\"\"The index of the destination node and the destination socket.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewEdge.end","title":"<code>end</code>  <code>instance-attribute</code>","text":"<p>The index of the destination node and the destination socket.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewEdge.start","title":"<code>start</code>  <code>instance-attribute</code>","text":"<p>The index of the source node and the source socket.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewGraph","title":"<code>ViewGraph</code>  <code>dataclass</code>","text":"<p>The representation of a view graph.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>@dataclass\nclass ViewGraph:\n    \"\"\"The representation of a view graph.\"\"\"\n\n    nodes: list[ViewNode]\n    \"\"\"The list of nodes in the view graph.\"\"\"\n    edges: list[ViewEdge]\n    \"\"\"The list of edges in the view graph.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewGraph.edges","title":"<code>edges</code>  <code>instance-attribute</code>","text":"<p>The list of edges in the view graph.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewGraph.nodes","title":"<code>nodes</code>  <code>instance-attribute</code>","text":"<p>The list of nodes in the view graph.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode","title":"<code>ViewNode</code>  <code>dataclass</code>","text":"<p>The representation of a node in the view graph.</p> Source code in <code>pipewine/workflows/drawing.py</code> <pre><code>@dataclass\nclass ViewNode:\n    \"\"\"The representation of a node in the view graph.\"\"\"\n\n    title: str\n    \"\"\"The title of the node.\"\"\"\n    fontsize: int\n    \"\"\"The font size of the title.\"\"\"\n    position: tuple[float, float]\n    \"\"\"The position of the node in the view graph.\"\"\"\n    size: tuple[float, float]\n    \"\"\"The size of the node.\"\"\"\n    color: tuple[int, int, int]\n    \"\"\"The color of the node.\"\"\"\n    inputs: list[tuple[str, float]]\n    \"\"\"A list of (name, delta_y) tuples representing the input sockets.\"\"\"\n    outputs: list[tuple[str, float]]\n    \"\"\"A list of (name, delta_y) tuples representing the output sockets.\"\"\"\n    has_collection_input: bool\n    \"\"\"Whether the node has a collection input.\"\"\"\n    has_collection_output: bool\n    \"\"\"Whether the node has a collection output.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.color","title":"<code>color</code>  <code>instance-attribute</code>","text":"<p>The color of the node.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.fontsize","title":"<code>fontsize</code>  <code>instance-attribute</code>","text":"<p>The font size of the title.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.has_collection_input","title":"<code>has_collection_input</code>  <code>instance-attribute</code>","text":"<p>Whether the node has a collection input.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.has_collection_output","title":"<code>has_collection_output</code>  <code>instance-attribute</code>","text":"<p>Whether the node has a collection output.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.inputs","title":"<code>inputs</code>  <code>instance-attribute</code>","text":"<p>A list of (name, delta_y) tuples representing the input sockets.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.outputs","title":"<code>outputs</code>  <code>instance-attribute</code>","text":"<p>A list of (name, delta_y) tuples representing the output sockets.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.position","title":"<code>position</code>  <code>instance-attribute</code>","text":"<p>The position of the node in the view graph.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.size","title":"<code>size</code>  <code>instance-attribute</code>","text":"<p>The size of the node.</p>"},{"location":"autoapi/pipewine/workflows/drawing/#pipewine.workflows.drawing.ViewNode.title","title":"<code>title</code>  <code>instance-attribute</code>","text":"<p>The title of the node.</p>"},{"location":"autoapi/pipewine/workflows/events/","title":"events","text":"<p>Workflow events and queues.</p>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.Event","title":"<code>Event</code>","text":"<p>Marker class for events.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>class Event:\n    \"\"\"Marker class for events.\"\"\"\n\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue","title":"<code>EventQueue</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for event queues, which are used to communicate events between the workflow executor and trackers.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>class EventQueue(ABC):\n    \"\"\"Base class for event queues, which are used to communicate events between\n    the workflow executor and trackers.\n    \"\"\"\n\n    @abstractmethod\n    def start(self) -&gt; None:\n        \"\"\"Start the event queue thread. This method should be called once before\n        the first emit call.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def emit(self, event: Event) -&gt; None:\n        \"\"\"Emit an event to the queue.\n\n        Args:\n            event (Event): The event to emit.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def capture(self) -&gt; Event | None:\n        \"\"\"Capture an event from the queue, if available.\n\n        Returns:\n            Event | None: The captured event, or None if no event is available.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def capture_blocking(self, timeout: float | None = None) -&gt; Event | None:\n        \"\"\"Capture an event from the queue, blocking until an event is available or the\n        timeout is reached.\n\n        Args:\n            timeout (float | None, optional): The timeout in seconds. Defaults to None,\n                in which case the method blocks indefinitely.\n\n        Returns:\n            Event | None: The captured event, or None if no event is available.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def close(self) -&gt; None:\n        \"\"\"Close the event queue thread. This method should be called once after the\n        last emit call.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue.capture","title":"<code>capture()</code>  <code>abstractmethod</code>","text":"<p>Capture an event from the queue, if available.</p> <p>Returns:</p> Type Description <code>Event | None</code> <p>Event | None: The captured event, or None if no event is available.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>@abstractmethod\ndef capture(self) -&gt; Event | None:\n    \"\"\"Capture an event from the queue, if available.\n\n    Returns:\n        Event | None: The captured event, or None if no event is available.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue.capture_blocking","title":"<code>capture_blocking(timeout=None)</code>  <code>abstractmethod</code>","text":"<p>Capture an event from the queue, blocking until an event is available or the timeout is reached.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float | None</code> <p>The timeout in seconds. Defaults to None, in which case the method blocks indefinitely.</p> <code>None</code> <p>Returns:</p> Type Description <code>Event | None</code> <p>Event | None: The captured event, or None if no event is available.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>@abstractmethod\ndef capture_blocking(self, timeout: float | None = None) -&gt; Event | None:\n    \"\"\"Capture an event from the queue, blocking until an event is available or the\n    timeout is reached.\n\n    Args:\n        timeout (float | None, optional): The timeout in seconds. Defaults to None,\n            in which case the method blocks indefinitely.\n\n    Returns:\n        Event | None: The captured event, or None if no event is available.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Close the event queue thread. This method should be called once after the last emit call.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>@abstractmethod\ndef close(self) -&gt; None:\n    \"\"\"Close the event queue thread. This method should be called once after the\n    last emit call.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue.emit","title":"<code>emit(event)</code>  <code>abstractmethod</code>","text":"<p>Emit an event to the queue.</p> <p>Parameters:</p> Name Type Description Default <code>event</code> <code>Event</code> <p>The event to emit.</p> required Source code in <code>pipewine/workflows/events.py</code> <pre><code>@abstractmethod\ndef emit(self, event: Event) -&gt; None:\n    \"\"\"Emit an event to the queue.\n\n    Args:\n        event (Event): The event to emit.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.EventQueue.start","title":"<code>start()</code>  <code>abstractmethod</code>","text":"<p>Start the event queue thread. This method should be called once before the first emit call.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>@abstractmethod\ndef start(self) -&gt; None:\n    \"\"\"Start the event queue thread. This method should be called once before\n    the first emit call.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.ProcessSharedEventQueue","title":"<code>ProcessSharedEventQueue</code>","text":"<p>               Bases: <code>EventQueue</code></p> <p>Event queue that uses a multiprocessing.Queue to communicate events between the workflow executor and trackers.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>class ProcessSharedEventQueue(EventQueue):\n    \"\"\"Event queue that uses a multiprocessing.Queue to communicate events between\n    the workflow executor and trackers.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the event queue.\"\"\"\n        super().__init__()\n        self._mp_q: Queue | None = None\n        self._id = uuid1().hex\n\n    def start(self) -&gt; None:\n        self._mp_q = get_context(\"spawn\").Queue(100)\n        InheritedData.data[self._id] = self._mp_q\n\n    def emit(self, event: Event) -&gt; None:\n        if self._mp_q is None:\n            raise RuntimeError(\"Queue is closed\")\n        self._mp_q.put(event)\n\n    def capture(self) -&gt; Event | None:\n        if self._mp_q is None:\n            raise RuntimeError(\"Queue is closed\")\n        try:\n            return self._mp_q.get_nowait()\n        except Empty:\n            return None\n\n    def capture_blocking(self, timeout: float | None = None) -&gt; Event | None:\n        if self._mp_q is None:\n            raise RuntimeError(\"Queue is closed\")\n        try:\n            return self._mp_q.get(timeout=timeout)\n        except Empty:\n            return None\n\n    def close(self) -&gt; None:\n        if self._mp_q is not None:\n            self._mp_q.close()\n            self._mp_q.cancel_join_thread()\n            del InheritedData.data[self._id]\n            self._mp_q = None\n\n    def __getstate__(self) -&gt; dict[str, Any]:  # pragma: no cover\n        data = {**self.__dict__}\n        del data[\"_mp_q\"]\n        return data\n\n    def __setstate__(self, data: dict[str, Any]) -&gt; None:\n        self._id = data[\"_id\"]\n        self._mp_q = cast(Queue, InheritedData.data[self._id])\n</code></pre>"},{"location":"autoapi/pipewine/workflows/events/#pipewine.workflows.events.ProcessSharedEventQueue.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the event queue.</p> Source code in <code>pipewine/workflows/events.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the event queue.\"\"\"\n    super().__init__()\n    self._mp_q: Queue | None = None\n    self._id = uuid1().hex\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/","title":"execution","text":"<p>This module contains the implementation necessary for the execution of workflows.</p>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.SequentialWorkflowExecutor","title":"<code>SequentialWorkflowExecutor</code>","text":"<p>               Bases: <code>WorkflowExecutor</code></p> <p>A workflow executor that executes the actions in a workflow in a sequential manner, respecting the dependencies between the nodes.</p> Source code in <code>pipewine/workflows/execution.py</code> <pre><code>class SequentialWorkflowExecutor(WorkflowExecutor):\n    \"\"\"A workflow executor that executes the actions in a workflow in a sequential\n    manner, respecting the dependencies between the nodes.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the executor.\"\"\"\n        super().__init__()\n        self._eq: EventQueue | None = None\n        self._def_cache = True\n        self._def_cache_type = LIFOCache\n        self._def_cache_params = {\"maxsize\": 1}\n        self._def_checkpoint = False\n        self._def_checkpoint_factory = UnderfolderCheckpointFactory()\n        self._def_checkpoint_grabber = Grabber()\n        self._def_collect_after_checkpoint = True\n        self._def_destroy_checkpoints = True\n\n    def attach(self, event_queue: EventQueue) -&gt; None:\n        if self._eq is not None:\n            raise RuntimeError(\"Already attached to another event queue.\")\n        self._eq = event_queue\n\n    def detach(self) -&gt; None:\n        if self._eq is None:\n            raise RuntimeError(\"Not attached to any event queue.\")\n        self._eq = None\n\n    def _register_all_cbs(self, action: AnyAction, node: Node) -&gt; None:\n        action.register_on_iter(partial(_on_iter_cb, self._eq, node))\n        action.register_on_enter(partial(_on_enter_cb, self._eq, node))\n        action.register_on_exit(partial(_on_exit_cb, self._eq, node))\n\n    def _execute_node(\n        self,\n        workflow: Workflow,\n        node: Node,\n        state: dict[Proxy, AnyDataset],\n        id_: str,\n        wf_opts: WfOptions,\n    ) -&gt; None:\n        action = cast(AnyAction, node.action)\n        self._register_all_cbs(action, node)\n        edges = workflow.get_inbound_edges(node)\n        all_or_none = len(edges) == 1 and all(\n            x.dst.socket is None or isinstance(x.dst.socket, All) for x in edges\n        )\n        all_int = all(isinstance(x.dst.socket, (int, All)) for x in edges)\n        all_str = all(isinstance(x.dst.socket, (str, All)) for x in edges)\n        assert len(edges) == 0 or all_or_none or all_int or all_str\n\n        if len(edges) == 0:\n            assert isinstance(action, DatasetSource)\n            output = action()\n        else:\n            assert isinstance(action, (DatasetOperator, DatasetSink))\n            input_: AnyDataset\n            if all_or_none:\n                edge = next(iter(edges))\n                input_ = state[edge.src]\n            elif all_int:\n                inputs_list = [None] * len(edges)\n                for edge in edges:\n                    inputs_list[cast(int, edge.dst.socket)] = state[edge.src]  # type: ignore\n                if issubclass(action.input_type, tuple):\n                    input_ = tuple(inputs_list)  # type: ignore\n                else:\n                    input_ = list(inputs_list)  # type: ignore\n            else:\n                inputs_dict = {}\n                for edge in edges:\n                    inputs_dict[cast(str, edge.dst.socket)] = state[edge.src]\n                if issubclass(action.input_type, Bundle):\n                    input_ = action.input_type(**inputs_dict)\n                else:\n                    input_ = inputs_dict\n\n            output = action(input_)\n\n        if output is None:\n            return\n        state[Proxy(node, All())] = output\n        if isinstance(output, Dataset):\n            self._handle_output(state, Proxy(node, None), output, id_, wf_opts)\n        elif isinstance(output, Sequence):\n            for i, dataset in enumerate(output):\n                self._handle_output(state, Proxy(node, i), dataset, id_, wf_opts)\n        elif isinstance(output, Mapping):\n            for k, v in output.items():\n                self._handle_output(state, Proxy(node, k), v, id_, wf_opts)\n        else:\n            assert isinstance(output, Bundle)\n            for k, v in output.as_dict().items():\n                self._handle_output(state, Proxy(node, k), v, id_, wf_opts)\n\n    def _handle_output(\n        self,\n        state: dict[Proxy, AnyDataset],\n        proxy: Proxy,\n        dataset: Dataset,\n        id_: str,\n        wf_opts: WfOptions,\n    ) -&gt; None:\n        opts = proxy.node.options\n        ckpt = Default.get(\n            opts.checkpoint, wf_opts.checkpoint, default=self._def_checkpoint\n        )\n        if (\n            ckpt\n            and len(dataset) &gt; 0\n            and not isinstance(proxy.node.action, DatasetSource)\n        ):\n            ckpt_fact = Default.get(\n                opts.checkpoint_factory,\n                wf_opts.checkpoint_factory,\n                default=self._def_checkpoint_factory,\n            )\n            grabber = Default.get(\n                opts.checkpoint_grabber,\n                wf_opts.checkpoint_grabber,\n                default=self._def_checkpoint_grabber,\n            )\n            name = proxy.node.name\n            if proxy.socket is not None:\n                name += str(proxy.socket)\n            sink, source = ckpt_fact.create(id_, name, type(dataset[0]), grabber)\n            self._register_all_cbs(sink, proxy.node)\n            self._register_all_cbs(source, proxy.node)\n            sink(dataset)\n\n            collect = Default.get(\n                opts.collect_after_checkpoint,\n                wf_opts.collect_after_checkpoint,\n                default=self._def_collect_after_checkpoint,\n            )\n            if collect:\n                del dataset\n                gc.collect()\n\n            dataset = source()\n\n        cache = Default.get(opts.cache, wf_opts.cache, default=self._def_cache)\n        if cache:\n            cache_type = Default.get(\n                opts.cache_type, wf_opts.cache_type, default=self._def_cache_type\n            )\n            cache_params = Default.get(\n                opts.cache_params, wf_opts.cache_params, default=self._def_cache_params\n            )\n            cache_op = CacheOp(cache_type=cache_type, **{**cache_params})\n            dataset = cache_op(dataset)\n\n        state[proxy] = dataset\n\n    def _topological_sort(self, workflow: Workflow) -&gt; list[Node]:\n        result: list[Node] = []\n        mark: dict[Node, int] = {}\n\n        def visit(node: Node) -&gt; None:\n            mark_of_node = mark.get(node, 0)\n            if mark_of_node == 1:  # pragma: no cover\n                # Excluding from coverage because it's pretty much impossible to create\n                # graphs with cycles with the current graph builder imperative approach.\n                # Might change in the future.\n                raise ValueError(\"The graph contains cycles.\")\n            if mark_of_node == 2:\n                return\n            mark[node] = 1\n            for edge in workflow.get_outbound_edges(node):\n                visit(edge.dst.node)\n            mark[node] = 2\n            result.append(node)\n\n        for node in workflow.get_nodes():\n            visit(node)\n        return result[::-1]\n\n    def execute(self, workflow: Workflow) -&gt; None:\n        id_ = uuid1()\n        wf_opts = workflow.options\n        sorted_graph = self._topological_sort(workflow)\n        state: dict[Proxy, AnyDataset] = {}\n        for node in sorted_graph:\n            self._execute_node(workflow, node, state, id_.hex, wf_opts)\n\n        for node in workflow.get_nodes():\n            opts = node.options\n            destroy = Default.get(\n                opts.destroy_checkpoints,\n                wf_opts.destroy_checkpoints,\n                default=self._def_destroy_checkpoints,\n            )\n            ckpt_fact = Default.get(\n                opts.checkpoint_factory,\n                wf_opts.checkpoint_factory,\n                default=self._def_checkpoint_factory,\n            )\n            if destroy and ckpt_fact is not None:\n                ckpt_fact.destroy(id_.hex, node.name)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.SequentialWorkflowExecutor.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the executor.</p> Source code in <code>pipewine/workflows/execution.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the executor.\"\"\"\n    super().__init__()\n    self._eq: EventQueue | None = None\n    self._def_cache = True\n    self._def_cache_type = LIFOCache\n    self._def_cache_params = {\"maxsize\": 1}\n    self._def_checkpoint = False\n    self._def_checkpoint_factory = UnderfolderCheckpointFactory()\n    self._def_checkpoint_grabber = Grabber()\n    self._def_collect_after_checkpoint = True\n    self._def_destroy_checkpoints = True\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.WorkflowExecutor","title":"<code>WorkflowExecutor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract class for workflow executors, which are responsible for executing the actions contained in a workflow in the correct order.</p> <p>Subclasses must implement the <code>execute</code>, <code>attach</code>, and <code>detach</code> methods.</p> Source code in <code>pipewine/workflows/execution.py</code> <pre><code>class WorkflowExecutor(ABC):\n    \"\"\"Abstract class for workflow executors, which are responsible for executing\n    the actions contained in a workflow in the correct order.\n\n    Subclasses must implement the `execute`, `attach`, and `detach` methods.\n    \"\"\"\n\n    @abstractmethod\n    def execute(self, workflow: Workflow) -&gt; None:\n        \"\"\"Executes the given workflow.\n\n        Args:\n            workflow (Workflow): The workflow to execute.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def attach(self, event_queue: EventQueue) -&gt; None:\n        \"\"\"Attaches the executor to an event queue, allowing it to emit events.\n\n        Args:\n            event_queue (EventQueue): The event queue to attach to.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def detach(self) -&gt; None:\n        \"\"\"Detaches the executor from the event queue.\"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.WorkflowExecutor.attach","title":"<code>attach(event_queue)</code>  <code>abstractmethod</code>","text":"<p>Attaches the executor to an event queue, allowing it to emit events.</p> <p>Parameters:</p> Name Type Description Default <code>event_queue</code> <code>EventQueue</code> <p>The event queue to attach to.</p> required Source code in <code>pipewine/workflows/execution.py</code> <pre><code>@abstractmethod\ndef attach(self, event_queue: EventQueue) -&gt; None:\n    \"\"\"Attaches the executor to an event queue, allowing it to emit events.\n\n    Args:\n        event_queue (EventQueue): The event queue to attach to.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.WorkflowExecutor.detach","title":"<code>detach()</code>  <code>abstractmethod</code>","text":"<p>Detaches the executor from the event queue.</p> Source code in <code>pipewine/workflows/execution.py</code> <pre><code>@abstractmethod\ndef detach(self) -&gt; None:\n    \"\"\"Detaches the executor from the event queue.\"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/execution/#pipewine.workflows.execution.WorkflowExecutor.execute","title":"<code>execute(workflow)</code>  <code>abstractmethod</code>","text":"<p>Executes the given workflow.</p> <p>Parameters:</p> Name Type Description Default <code>workflow</code> <code>Workflow</code> <p>The workflow to execute.</p> required Source code in <code>pipewine/workflows/execution.py</code> <pre><code>@abstractmethod\ndef execute(self, workflow: Workflow) -&gt; None:\n    \"\"\"Executes the given workflow.\n\n    Args:\n        workflow (Workflow): The workflow to execute.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/","title":"model","text":"<p>Base classes and components for building workflows.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.AnyAction","title":"<code>AnyAction = DatasetSource | DatasetOperator | DatasetSink</code>  <code>module-attribute</code>","text":"<p>Type alias for 'Actions', which can be dataset sources, operators or sinks.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.All","title":"<code>All</code>","text":"<p>Sentinel object that represents all elements in a sequence or mapping.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>class All:\n    \"\"\"Sentinel object that represents all elements in a sequence or mapping.\"\"\"\n\n    def __hash__(self) -&gt; int:\n        return 1\n\n    def __repr__(self) -&gt; str:\n        return self.__class__.__name__\n\n    def __eq__(self, value: object) -&gt; bool:\n        return isinstance(value, All)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.CheckpointFactory","title":"<code>CheckpointFactory</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for factories that create and destroy checkpoints.</p> <p>Subclasses should implement the <code>create</code> and <code>destroy</code> methods.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>class CheckpointFactory(ABC):\n    \"\"\"Base class for factories that create and destroy checkpoints.\n\n    Subclasses should implement the `create` and `destroy` methods.\n    \"\"\"\n\n    @abstractmethod\n    def create[\n        T: Sample\n    ](\n        self, execution_id: str, name: str, sample_type: type[T], grabber: Grabber\n    ) -&gt; tuple[DatasetSink[Dataset[T]], DatasetSource[Dataset[T]]]:\n        \"\"\"Create a checkpoint for a given execution and name.\n\n        Args:\n            execution_id: The unique identifier for the current workflow execution.\n            name: The name of the checkpoint.\n            sample_type: The type of the samples that will be stored in the checkpoint.\n            grabber: The grabber that will be used to write the dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def destroy(self, execution_id: str, name: str) -&gt; None:\n        \"\"\"Destroy a checkpoint for a given execution and name.\n\n        Args:\n            execution_id: The unique identifier for the current workflow execution.\n            name: The name of the checkpoint.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.CheckpointFactory.create","title":"<code>create(execution_id, name, sample_type, grabber)</code>  <code>abstractmethod</code>","text":"<p>Create a checkpoint for a given execution and name.</p> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>str</code> <p>The unique identifier for the current workflow execution.</p> required <code>name</code> <code>str</code> <p>The name of the checkpoint.</p> required <code>sample_type</code> <code>type[T]</code> <p>The type of the samples that will be stored in the checkpoint.</p> required <code>grabber</code> <code>Grabber</code> <p>The grabber that will be used to write the dataset.</p> required Source code in <code>pipewine/workflows/model.py</code> <pre><code>@abstractmethod\ndef create[\n    T: Sample\n](\n    self, execution_id: str, name: str, sample_type: type[T], grabber: Grabber\n) -&gt; tuple[DatasetSink[Dataset[T]], DatasetSource[Dataset[T]]]:\n    \"\"\"Create a checkpoint for a given execution and name.\n\n    Args:\n        execution_id: The unique identifier for the current workflow execution.\n        name: The name of the checkpoint.\n        sample_type: The type of the samples that will be stored in the checkpoint.\n        grabber: The grabber that will be used to write the dataset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.CheckpointFactory.destroy","title":"<code>destroy(execution_id, name)</code>  <code>abstractmethod</code>","text":"<p>Destroy a checkpoint for a given execution and name.</p> <p>Parameters:</p> Name Type Description Default <code>execution_id</code> <code>str</code> <p>The unique identifier for the current workflow execution.</p> required <code>name</code> <code>str</code> <p>The name of the checkpoint.</p> required Source code in <code>pipewine/workflows/model.py</code> <pre><code>@abstractmethod\ndef destroy(self, execution_id: str, name: str) -&gt; None:\n    \"\"\"Destroy a checkpoint for a given execution and name.\n\n    Args:\n        execution_id: The unique identifier for the current workflow execution.\n        name: The name of the checkpoint.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Default","title":"<code>Default</code>","text":"<p>Sentinel object that represents a default value for an optional argument.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>class Default:\n    \"\"\"Sentinel object that represents a default value for an optional argument.\"\"\"\n\n    def __repr__(self) -&gt; str:\n        return \"Default\"\n\n    @classmethod\n    def get[T](cls, *optionals: T | \"Default\", default: T) -&gt; T:\n        \"\"\"Get the first non-default value from a list of optionals.\n\n        Returns:\n            T: The first non-default value found in the list of optionals. If no\n                non-default value is found, the default value is returned.\n        \"\"\"\n        for maybe in optionals:\n            if not isinstance(maybe, Default):\n                return maybe\n        return default\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Default.get","title":"<code>get(*optionals, default)</code>  <code>classmethod</code>","text":"<p>Get the first non-default value from a list of optionals.</p> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>The first non-default value found in the list of optionals. If no non-default value is found, the default value is returned.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>@classmethod\ndef get[T](cls, *optionals: T | \"Default\", default: T) -&gt; T:\n    \"\"\"Get the first non-default value from a list of optionals.\n\n    Returns:\n        T: The first non-default value found in the list of optionals. If no\n            non-default value is found, the default value is returned.\n    \"\"\"\n    for maybe in optionals:\n        if not isinstance(maybe, Default):\n            return maybe\n    return default\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Edge","title":"<code>Edge</code>  <code>dataclass</code>","text":"<p>An edge in a workflow graph that connects two nodes.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>@dataclass(unsafe_hash=True)\nclass Edge:\n    \"\"\"An edge in a workflow graph that connects two nodes.\"\"\"\n\n    src: Proxy\n    \"\"\"The source of the edge.\"\"\"\n    dst: Proxy\n    \"\"\"The destination of the edge.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Edge.dst","title":"<code>dst</code>  <code>instance-attribute</code>","text":"<p>The destination of the edge.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Edge.src","title":"<code>src</code>  <code>instance-attribute</code>","text":"<p>The source of the edge.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Node","title":"<code>Node</code>  <code>dataclass</code>","text":"<p>A node in a workflow graph.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>@dataclass(unsafe_hash=True)\nclass Node[T: AnyAction]:\n    \"\"\"A node in a workflow graph.\"\"\"\n\n    name: str\n    \"\"\"The name of the node.\"\"\"\n    action: T = field(hash=False)\n    \"\"\"The action to be executed.\"\"\"\n    options: WfOptions = field(default_factory=WfOptions, hash=False, compare=False)\n    \"\"\"The options for the execution of the node.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Node.action","title":"<code>action = field(hash=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The action to be executed.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Node.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the node.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Node.options","title":"<code>options = field(default_factory=WfOptions, hash=False, compare=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The options for the execution of the node.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Proxy","title":"<code>Proxy</code>  <code>dataclass</code>","text":"<p>A proxy object that represents the input or output of a node in a workflow graph.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>@dataclass(unsafe_hash=True)\nclass Proxy:\n    \"\"\"A proxy object that represents the input or output of a node in a workflow graph.\"\"\"\n\n    node: Node\n    \"\"\"The node that the proxy is associated with.\"\"\"\n    socket: int | str | None | All\n    \"\"\"The socket of the node that the proxy is associated with. \n\n    - If set to an integer value, the proxy represents the output of a node that returns\n        a sequence of datasets, and the value represents the index of the dataset in the\n        sequence.\n    - If set to a string value, the proxy represents the output of a node that returns\n        a mapping of datasets, and the value represents the key of the dataset in the\n        mapping.\n    - If set to None, the proxy represents the output of a node that returns a single\n        dataset, thus needing no index or key.\n    - If set to All, the proxy represents all the outputs of a node that returns a\n        collection of datasets and it is used to connect the whole collection to another\n        node.\n    \"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Proxy.node","title":"<code>node</code>  <code>instance-attribute</code>","text":"<p>The node that the proxy is associated with.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Proxy.socket","title":"<code>socket</code>  <code>instance-attribute</code>","text":"<p>The socket of the node that the proxy is associated with. </p> <ul> <li>If set to an integer value, the proxy represents the output of a node that returns     a sequence of datasets, and the value represents the index of the dataset in the     sequence.</li> <li>If set to a string value, the proxy represents the output of a node that returns     a mapping of datasets, and the value represents the key of the dataset in the     mapping.</li> <li>If set to None, the proxy represents the output of a node that returns a single     dataset, thus needing no index or key.</li> <li>If set to All, the proxy represents all the outputs of a node that returns a     collection of datasets and it is used to connect the whole collection to another     node.</li> </ul>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.UnderfolderCheckpointFactory","title":"<code>UnderfolderCheckpointFactory</code>","text":"<p>               Bases: <code>CheckpointFactory</code></p> <p>A checkpoint factory that creates checkpoints as underfolder datasets.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>class UnderfolderCheckpointFactory(CheckpointFactory):\n    \"\"\"A checkpoint factory that creates checkpoints as underfolder datasets.\"\"\"\n\n    def __init__(self, folder: Path | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            folder (Path, optional): The folder where the checkpoints will be stored.\n                Defaults to None, in which case the folder will be a temporary\n                directory.\n        \"\"\"\n        self._folder = folder or Path(gettempdir()) / \"pipewine_workflows\"\n\n    def create[\n        T: Sample\n    ](\n        self, execution_id: str, name: str, sample_type: type[T], grabber: Grabber\n    ) -&gt; tuple[DatasetSink[Dataset[T]], DatasetSource[Dataset[T]]]:\n        path = self._folder / execution_id / name\n        sink = UnderfolderSink(path, grabber=grabber)\n        source = UnderfolderSource(path, sample_type=sample_type)\n        return sink, source\n\n    def destroy(self, execution_id: str, name: str) -&gt; None:\n        rm_path = self._folder / execution_id / name\n        if rm_path.is_dir():\n            shutil.rmtree(rm_path)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.UnderfolderCheckpointFactory.__init__","title":"<code>__init__(folder=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>folder</code> <code>Path</code> <p>The folder where the checkpoints will be stored. Defaults to None, in which case the folder will be a temporary directory.</p> <code>None</code> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def __init__(self, folder: Path | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        folder (Path, optional): The folder where the checkpoints will be stored.\n            Defaults to None, in which case the folder will be a temporary\n            directory.\n    \"\"\"\n    self._folder = folder or Path(gettempdir()) / \"pipewine_workflows\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions","title":"<code>WfOptions</code>  <code>dataclass</code>","text":"<p>Options for the execution of a workflow node. These options can be passed to both when creating a <code>Workflow</code> object or when adding a single node to the workflow, in which case the options will be used only for that node.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>@dataclass\nclass WfOptions:\n    \"\"\"Options for the execution of a workflow node. These options can be passed to\n    both when creating a `Workflow` object or when adding a single node to the workflow,\n    in which case the options will be used only for that node.\n    \"\"\"\n\n    cache: bool | Default = field(default_factory=Default)\n    \"\"\"Whether to cache the output of the node. If True, a `CacheOp` operator will be\n    automatically applied to the output of the node. If False, no caching will be\n    performed.\n    \"\"\"\n    cache_type: type | Default = field(default_factory=Default)\n    \"\"\"The type of the cache to use. If not specified, a default cache type will be\n    used. This option is only relevant if `cache` is True.\n    \"\"\"\n    cache_params: dict[str, Any] | Default = field(default_factory=Default)\n    \"\"\"Additional parameters to pass to the cache constructor. This option is only \n    relevant if `cache` is True.\n    \"\"\"\n    checkpoint: bool | Default = field(default_factory=Default)\n    \"\"\"Whether to create a checkpoint for the node, automatically writing the output\n    of a node to disk to avoid recomputing all lazy operations. If False, no checkpoint \n    will be created.\n    \"\"\"\n    checkpoint_factory: CheckpointFactory | Default = field(default_factory=Default)\n    \"\"\"The factory to use to create and destroy checkpoints. If not specified, a default\n    factory will be used. This option is only relevant if `checkpoint` is True.\n    \"\"\"\n    checkpoint_grabber: Grabber | Default = field(default_factory=Default)\n    \"\"\"The grabber to use to write the checkpoint. If not specified, a default grabber\n    will be used. This option is only relevant if `checkpoint` is True.\n    \"\"\"\n    collect_after_checkpoint: bool | Default = field(default_factory=Default)\n    \"\"\"Whether to force the garbage collector to run after checkpointing the output\n    of the node.\n    \"\"\"\n    destroy_checkpoints: bool | Default = field(default_factory=Default)\n    \"\"\"Whether to destroy the checkpoints after the workflow execution is finished.\"\"\"\n\n    def __repr__(self) -&gt; str:\n        opts = [\n            f\"{k}={v}\" for k, v in self.__dict__.items() if not isinstance(v, Default)\n        ]\n        opts_repr = \", \".join(opts)\n        return f\"{self.__class__.__name__}({opts_repr})\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.cache","title":"<code>cache = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to cache the output of the node. If True, a <code>CacheOp</code> operator will be automatically applied to the output of the node. If False, no caching will be performed.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.cache_params","title":"<code>cache_params = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional parameters to pass to the cache constructor. This option is only  relevant if <code>cache</code> is True.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.cache_type","title":"<code>cache_type = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The type of the cache to use. If not specified, a default cache type will be used. This option is only relevant if <code>cache</code> is True.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.checkpoint","title":"<code>checkpoint = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to create a checkpoint for the node, automatically writing the output of a node to disk to avoid recomputing all lazy operations. If False, no checkpoint  will be created.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.checkpoint_factory","title":"<code>checkpoint_factory = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The factory to use to create and destroy checkpoints. If not specified, a default factory will be used. This option is only relevant if <code>checkpoint</code> is True.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.checkpoint_grabber","title":"<code>checkpoint_grabber = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The grabber to use to write the checkpoint. If not specified, a default grabber will be used. This option is only relevant if <code>checkpoint</code> is True.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.collect_after_checkpoint","title":"<code>collect_after_checkpoint = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to force the garbage collector to run after checkpointing the output of the node.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.WfOptions.destroy_checkpoints","title":"<code>destroy_checkpoints = field(default_factory=Default)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to destroy the checkpoints after the workflow execution is finished.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow","title":"<code>Workflow</code>","text":"<p>A workflow is a directed acyclic graph of nodes that represent actions to be executed in a specific order. The nodes are connected by edges that represent the flow of data between the nodes.</p> <p>See the \"Workflows\" section in the Pipewine documentation for more information.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>class Workflow:\n    \"\"\"A workflow is a directed acyclic graph of nodes that represent actions to be\n    executed in a specific order. The nodes are connected by edges that represent the\n    flow of data between the nodes.\n\n    See the \"Workflows\" section in the Pipewine documentation for more information.\n    \"\"\"\n\n    _INPUT_NAME = \"input\"\n    _OUTPUT_NAME = \"output\"\n\n    def __init__(self, options: WfOptions | None = None) -&gt; None:\n        \"\"\"\n        Args:\n            options (WfOptions, optional): The options for the workflow. Defaults to\n                None, in which case default options will be used.\n        \"\"\"\n        self._options = options or WfOptions()\n        self._nodes: set[Node] = set()\n        self._nodes_by_name: dict[str, Node] = {}\n        self._inbound_edges: dict[Node, set[Edge]] = defaultdict(set)\n        self._outbound_edges: dict[Node, set[Edge]] = defaultdict(set)\n        self._name_counters: dict[str, int] = defaultdict(int)\n\n    @property\n    def options(self) -&gt; WfOptions:\n        \"\"\"The options for the workflow.\"\"\"\n        return self._options\n\n    def _gen_node_name(self, action: AnyAction) -&gt; str:\n        title = action.__class__.__name__\n        self._name_counters[title] += 1\n        return f\"{title}_{self._name_counters[title]}\"\n\n    def get_nodes(self) -&gt; set[Node]:\n        \"\"\"Get all the nodes in the workflow.\"\"\"\n        return self._nodes\n\n    def get_node(self, name: str) -&gt; Node | None:\n        \"\"\"Get a node by name.\"\"\"\n        return self._nodes_by_name.get(name)\n\n    def get_inbound_edges(self, node: Node) -&gt; set[Edge]:\n        \"\"\"Get the inbound edges of a node.\n\n        Args:\n            node (Node): The node to get the inbound edges for.\n\n        Returns:\n            set[Edge]: The inbound edges of the node.\n        \"\"\"\n        if node not in self._inbound_edges:\n            msg = f\"Node '{node.name}' not found\"\n            raise ValueError(msg)\n\n        return self._inbound_edges[node]\n\n    def get_outbound_edges(self, node: Node) -&gt; set[Edge]:\n        \"\"\"Get the outbound edges of a node.\n\n        Args:\n            node (Node): The node to get the outbound edges for.\n\n        Returns:\n            set[Edge]: The outbound edges of the node.\n        \"\"\"\n        if node not in self._outbound_edges:\n            msg = f\"Node '{node.name}' not found\"\n            raise ValueError(msg)\n\n        return self._outbound_edges[node]\n\n    def node[\n        T: AnyAction\n    ](self, action: T, name: str | None = None, options: WfOptions | None = None) -&gt; T:\n        \"\"\"Wrap any action into a workflow node and then add it to the workflow.\n\n        Args:\n            action (T): The action to be executed. This can be a dataset source,\n                operator or sink.\n            name (str, optional): The name of the node. If not specified, a name will be\n                generated automatically. Defaults to None.\n            options (WfOptions, optional): The options for the execution of the node.\n                Defaults to None, in which case default workflow options will be used.\n\n        Raises:\n            ValueError: if the name is already associated to another node.\n            ValueError: if the action has the wrong type or it is not annotated properly.\n\n        Returns:\n            T: A callable object that mimics the action's signature and that, when\n                called, instead of executing the action (as the original object would\n                do), it will just tell the workflow to create the necessary edges to\n                connect the action to other nodes.\n        \"\"\"\n        name = name or self._gen_node_name(cast(AnyAction, action))\n        if name in self._nodes_by_name:\n            raise ValueError(f\"Name {name} is already associated to another node.\")\n        options = options or WfOptions()\n        node = Node(name=name, action=action, options=options)\n        self._nodes.add(node)\n        self._nodes_by_name[node.name] = node\n        self._inbound_edges[node] = set()\n        self._outbound_edges[node] = set()\n\n        action_ = cast(AnyAction, action)\n        return_val: Proxy | Sequence[Proxy] | Mapping[str, Proxy] | Bundle[Proxy] | None\n        if isinstance(action_, DatasetSink):\n            return_val = None\n        else:\n            return_t = action_.output_type\n            if issubclass(return_t, Dataset):\n                return_val = Proxy(node, None)\n            elif (\n                issubclass(return_t, tuple)\n                and isinstance(\n                    ann := get_annotations(action.__call__, eval_str=True)[\"return\"],\n                    GenericAlias,\n                )\n                and len(ann.__args__) &gt; 0\n                and ann.__args__[-1] is not Ellipsis\n            ):\n                # If the size of the tuple is statically known, we can allow iter() and\n                # len() in the returned proxy object.\n                return_val = tuple(Proxy(node, i) for i in range(len(ann.__args__)))\n            elif issubclass(return_t, Sequence):\n                return_val = _ProxySequence(lambda idx: Proxy(node, idx))\n            elif issubclass(return_t, Mapping):\n                return_val = _ProxyMapping(lambda k: Proxy(node, k))\n            elif issubclass(return_t, Bundle):\n                fields = return_t.__dataclass_fields__\n                return_val = _ProxyBundle(**{k: Proxy(node, k) for k in fields})\n            else:  # pragma: no cover (unreachable)\n                raise ValueError(f\"Unknown type '{return_t}'\")\n\n        def connect(*args, **kwargs):\n            everything = list(args) + list(kwargs.values())\n            edges: list[Edge] = []\n            for arg in everything:\n                if isinstance(arg, Proxy):\n                    edges.append(Edge(arg, Proxy(node, None)))\n                elif isinstance(arg, _ProxySequence):\n                    orig_node = arg._factory(0).node\n                    edges.append(Edge(Proxy(orig_node, All()), Proxy(node, All())))\n                elif isinstance(arg, Sequence):\n                    edges.extend([Edge(x, Proxy(node, i)) for i, x in enumerate(arg)])\n                elif isinstance(arg, _ProxyMapping):\n                    orig_node = cast(Node, arg._factory(\"a\").node)\n                    edges.append(Edge(Proxy(orig_node, All()), Proxy(node, All())))\n                elif isinstance(arg, Mapping):\n                    edges.extend([Edge(v, Proxy(node, k)) for k, v in arg.items()])\n                elif isinstance(arg, Bundle):\n                    edges.extend(\n                        [Edge(v, Proxy(node, k)) for k, v in arg.as_dict().items()]\n                    )\n                else:  # pragma: no cover (unreachable)\n                    raise ValueError(f\"Unknown type '{type(arg)}'\")\n\n            for edge in edges:\n                self._inbound_edges[edge.dst.node].add(edge)\n                self._outbound_edges[edge.src.node].add(edge)\n\n            return return_val\n\n        return connect  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.options","title":"<code>options</code>  <code>property</code>","text":"<p>The options for the workflow.</p>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.__init__","title":"<code>__init__(options=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>options</code> <code>WfOptions</code> <p>The options for the workflow. Defaults to None, in which case default options will be used.</p> <code>None</code> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def __init__(self, options: WfOptions | None = None) -&gt; None:\n    \"\"\"\n    Args:\n        options (WfOptions, optional): The options for the workflow. Defaults to\n            None, in which case default options will be used.\n    \"\"\"\n    self._options = options or WfOptions()\n    self._nodes: set[Node] = set()\n    self._nodes_by_name: dict[str, Node] = {}\n    self._inbound_edges: dict[Node, set[Edge]] = defaultdict(set)\n    self._outbound_edges: dict[Node, set[Edge]] = defaultdict(set)\n    self._name_counters: dict[str, int] = defaultdict(int)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.get_inbound_edges","title":"<code>get_inbound_edges(node)</code>","text":"<p>Get the inbound edges of a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to get the inbound edges for.</p> required <p>Returns:</p> Type Description <code>set[Edge]</code> <p>set[Edge]: The inbound edges of the node.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def get_inbound_edges(self, node: Node) -&gt; set[Edge]:\n    \"\"\"Get the inbound edges of a node.\n\n    Args:\n        node (Node): The node to get the inbound edges for.\n\n    Returns:\n        set[Edge]: The inbound edges of the node.\n    \"\"\"\n    if node not in self._inbound_edges:\n        msg = f\"Node '{node.name}' not found\"\n        raise ValueError(msg)\n\n    return self._inbound_edges[node]\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.get_node","title":"<code>get_node(name)</code>","text":"<p>Get a node by name.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def get_node(self, name: str) -&gt; Node | None:\n    \"\"\"Get a node by name.\"\"\"\n    return self._nodes_by_name.get(name)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.get_nodes","title":"<code>get_nodes()</code>","text":"<p>Get all the nodes in the workflow.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def get_nodes(self) -&gt; set[Node]:\n    \"\"\"Get all the nodes in the workflow.\"\"\"\n    return self._nodes\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.get_outbound_edges","title":"<code>get_outbound_edges(node)</code>","text":"<p>Get the outbound edges of a node.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Node</code> <p>The node to get the outbound edges for.</p> required <p>Returns:</p> Type Description <code>set[Edge]</code> <p>set[Edge]: The outbound edges of the node.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def get_outbound_edges(self, node: Node) -&gt; set[Edge]:\n    \"\"\"Get the outbound edges of a node.\n\n    Args:\n        node (Node): The node to get the outbound edges for.\n\n    Returns:\n        set[Edge]: The outbound edges of the node.\n    \"\"\"\n    if node not in self._outbound_edges:\n        msg = f\"Node '{node.name}' not found\"\n        raise ValueError(msg)\n\n    return self._outbound_edges[node]\n</code></pre>"},{"location":"autoapi/pipewine/workflows/model/#pipewine.workflows.model.Workflow.node","title":"<code>node(action, name=None, options=None)</code>","text":"<p>Wrap any action into a workflow node and then add it to the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>T</code> <p>The action to be executed. This can be a dataset source, operator or sink.</p> required <code>name</code> <code>str</code> <p>The name of the node. If not specified, a name will be generated automatically. Defaults to None.</p> <code>None</code> <code>options</code> <code>WfOptions</code> <p>The options for the execution of the node. Defaults to None, in which case default workflow options will be used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the name is already associated to another node.</p> <code>ValueError</code> <p>if the action has the wrong type or it is not annotated properly.</p> <p>Returns:</p> Name Type Description <code>T</code> <code>T</code> <p>A callable object that mimics the action's signature and that, when called, instead of executing the action (as the original object would do), it will just tell the workflow to create the necessary edges to connect the action to other nodes.</p> Source code in <code>pipewine/workflows/model.py</code> <pre><code>def node[\n    T: AnyAction\n](self, action: T, name: str | None = None, options: WfOptions | None = None) -&gt; T:\n    \"\"\"Wrap any action into a workflow node and then add it to the workflow.\n\n    Args:\n        action (T): The action to be executed. This can be a dataset source,\n            operator or sink.\n        name (str, optional): The name of the node. If not specified, a name will be\n            generated automatically. Defaults to None.\n        options (WfOptions, optional): The options for the execution of the node.\n            Defaults to None, in which case default workflow options will be used.\n\n    Raises:\n        ValueError: if the name is already associated to another node.\n        ValueError: if the action has the wrong type or it is not annotated properly.\n\n    Returns:\n        T: A callable object that mimics the action's signature and that, when\n            called, instead of executing the action (as the original object would\n            do), it will just tell the workflow to create the necessary edges to\n            connect the action to other nodes.\n    \"\"\"\n    name = name or self._gen_node_name(cast(AnyAction, action))\n    if name in self._nodes_by_name:\n        raise ValueError(f\"Name {name} is already associated to another node.\")\n    options = options or WfOptions()\n    node = Node(name=name, action=action, options=options)\n    self._nodes.add(node)\n    self._nodes_by_name[node.name] = node\n    self._inbound_edges[node] = set()\n    self._outbound_edges[node] = set()\n\n    action_ = cast(AnyAction, action)\n    return_val: Proxy | Sequence[Proxy] | Mapping[str, Proxy] | Bundle[Proxy] | None\n    if isinstance(action_, DatasetSink):\n        return_val = None\n    else:\n        return_t = action_.output_type\n        if issubclass(return_t, Dataset):\n            return_val = Proxy(node, None)\n        elif (\n            issubclass(return_t, tuple)\n            and isinstance(\n                ann := get_annotations(action.__call__, eval_str=True)[\"return\"],\n                GenericAlias,\n            )\n            and len(ann.__args__) &gt; 0\n            and ann.__args__[-1] is not Ellipsis\n        ):\n            # If the size of the tuple is statically known, we can allow iter() and\n            # len() in the returned proxy object.\n            return_val = tuple(Proxy(node, i) for i in range(len(ann.__args__)))\n        elif issubclass(return_t, Sequence):\n            return_val = _ProxySequence(lambda idx: Proxy(node, idx))\n        elif issubclass(return_t, Mapping):\n            return_val = _ProxyMapping(lambda k: Proxy(node, k))\n        elif issubclass(return_t, Bundle):\n            fields = return_t.__dataclass_fields__\n            return_val = _ProxyBundle(**{k: Proxy(node, k) for k in fields})\n        else:  # pragma: no cover (unreachable)\n            raise ValueError(f\"Unknown type '{return_t}'\")\n\n    def connect(*args, **kwargs):\n        everything = list(args) + list(kwargs.values())\n        edges: list[Edge] = []\n        for arg in everything:\n            if isinstance(arg, Proxy):\n                edges.append(Edge(arg, Proxy(node, None)))\n            elif isinstance(arg, _ProxySequence):\n                orig_node = arg._factory(0).node\n                edges.append(Edge(Proxy(orig_node, All()), Proxy(node, All())))\n            elif isinstance(arg, Sequence):\n                edges.extend([Edge(x, Proxy(node, i)) for i, x in enumerate(arg)])\n            elif isinstance(arg, _ProxyMapping):\n                orig_node = cast(Node, arg._factory(\"a\").node)\n                edges.append(Edge(Proxy(orig_node, All()), Proxy(node, All())))\n            elif isinstance(arg, Mapping):\n                edges.extend([Edge(v, Proxy(node, k)) for k, v in arg.items()])\n            elif isinstance(arg, Bundle):\n                edges.extend(\n                    [Edge(v, Proxy(node, k)) for k, v in arg.as_dict().items()]\n                )\n            else:  # pragma: no cover (unreachable)\n                raise ValueError(f\"Unknown type '{type(arg)}'\")\n\n        for edge in edges:\n            self._inbound_edges[edge.dst.node].add(edge)\n            self._outbound_edges[edge.src.node].add(edge)\n\n        return return_val\n\n    return connect  # type: ignore\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/","title":"tracking","text":"<p>Progress tracking utilities for workflows.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.CursesTracker","title":"<code>CursesTracker</code>","text":"<p>               Bases: <code>Tracker</code></p> <p>A tracker that uses curses to display the progress of a workflow in a terminal.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>class CursesTracker(Tracker):\n    \"\"\"A tracker that uses curses to display the progress of a workflow in a terminal.\"\"\"\n\n    MAX_COLOR = 1000\n\n    def __init__(self, refresh_rate: float = 0.1) -&gt; None:\n        \"\"\"\n        Args:\n            refresh_rate (float, optional): The refresh rate of the display in seconds.\n                Defaults to 0.1.\n        \"\"\"\n        super().__init__()\n        self._refresh_rate = refresh_rate\n        self._n_shades = 10\n\n        self._eq: EventQueue | None = None\n        self._tui_thread: threading.Thread | None = None\n        self._read_thread: threading.Thread | None = None\n        self._buffer: deque[Event] = deque()\n        self._stop_flag_forced = threading.Event()\n        self._stop_flag_graceful = threading.Event()\n\n    def attach(self, event_queue: EventQueue) -&gt; None:\n        if self._eq is not None or self._tui_thread is not None:\n            raise RuntimeError(\"Already attached to another event queue.\")\n        self._eq = event_queue\n        self._tui_thread = threading.Thread(target=self._tui_loop)\n        self._read_thread = threading.Thread(target=self._read_loop)\n        self._stop_flag_forced.clear()\n        self._stop_flag_graceful.clear()\n        self._tui_thread.start()\n        self._read_thread.start()\n\n    def detach(self, graceful: bool = True) -&gt; None:\n        if self._eq is None or self._tui_thread is None:\n            raise RuntimeError(\"Not attached to any event queue.\")\n        assert self._read_thread is not None\n        if graceful:\n            self._stop_flag_graceful.set()\n        else:\n            self._stop_flag_forced.set()\n        self._read_thread.join()\n        self._tui_thread.join()\n        self._read_thread = None\n        self._tui_thread = None\n        self._eq = None\n\n    def _get_group(self, group: TaskGroup, path: str) -&gt; TaskGroup:\n        path_chunks = path.split(\"/\")\n        for p in path_chunks:\n            if p not in group.groups:\n                group.groups[p] = TaskGroup(p)\n            group = group.groups[p]\n        return group\n\n    def _spawn_task(self, group: TaskGroup, path: str, total: int) -&gt; Task:\n        group_path, _, task_name = path.rpartition(\"/\")\n        units = [False for _ in range(total)]\n        task = Task(task_name, units)\n        group = self._get_group(group, group_path)\n        group.tasks[task_name] = task\n        return task\n\n    def _get_task(self, group: TaskGroup, path: str) -&gt; Task:\n        group_path, _, task_name = path.rpartition(\"/\")\n        group = self._get_group(group, group_path)\n        return group.tasks[task_name]\n\n    def _preorder(\n        self, group: TaskGroup, depth: int\n    ) -&gt; list[tuple[int, Task | TaskGroup]]:\n        next = depth + 1\n        result: list[tuple[int, Task | TaskGroup]] = [(depth, group)]\n        for tg in group.groups.values():\n            result.extend(self._preorder(tg, depth=next))\n        for task in group.tasks.values():\n            result.append((next, task))\n        return result\n\n    def _compute_bar(self, units: list[bool], width: int) -&gt; list[list[int]]:\n        if len(units) &lt; width:\n            bar = []\n            div = width // len(units)\n            rem = width % len(units)\n            for ui, unit in enumerate(units):\n                bar.append([div + int(ui &lt; rem), int(unit), 1])\n        else:\n            bar = [[1, 0, 0] for _ in range(width)]\n            for ui, unit in enumerate(units):\n                cell = bar[int(ui / len(units) * len(bar))]\n                cell[1] += int(unit)\n                cell[2] += 1\n        return bar\n\n    def _init_colors(self) -&gt; None:  # pragma: no cover\n        curses.start_color()\n        curses.use_default_colors()\n        if curses.can_change_color():\n            for i in range(self._n_shades):\n                c = int((float(i + 1) / self._n_shades) * self.MAX_COLOR)\n                curses.init_color(i + 1, c, c, c)\n                curses.init_pair(i + 1, i + 1, -1)\n        else:\n            self._n_shades = 5\n            curses.init_pair(1, curses.COLOR_BLACK, -1)\n            curses.init_pair(2, curses.COLOR_RED, -1)\n            curses.init_pair(3, curses.COLOR_YELLOW, -1)\n            curses.init_pair(4, curses.COLOR_GREEN, -1)\n            curses.init_pair(5, curses.COLOR_WHITE, -1)\n\n    def _color_of(self, frac: float) -&gt; int:\n        return curses.color_pair(1 + int(frac * (self._n_shades - 1)))\n\n    def _render_tasks(\n        self,\n        screen: window,\n        tasks: list[tuple[int, Task | TaskGroup]],\n        global_step: int,\n    ) -&gt; None:\n        screen.clear()\n        bar_elem = \"\u2588\u2588\"\n        H, W = screen.getmaxyx()\n        TITLE_H, TITLE_W = len(tasks), min(20, W - 1)\n        PROG_H = TITLE_H\n        PROG_W = W - TITLE_W\n        padding = 0\n        for _, entry in tasks:\n            if isinstance(entry, Task):\n                padding_i = len(str(len(entry.units))) * 2 + 1 + 11\n                padding = max(padding, padding_i)\n        bar_w = (PROG_W - padding) // len(bar_elem)\n        title_pad = curses.newpad(TITLE_H, TITLE_W)\n        prog_pad = curses.newpad(PROG_H, PROG_W)\n        for i, (depth, entry) in enumerate(tasks):\n            space = TITLE_W - 2 * depth - 1\n            text = entry.name\n            if len(text) &gt; space:  # pragma: no cover\n                start = (global_step // 2) % (len(text) - space)\n                text = text[start : start + space]\n            title_pad.addstr(i, 2 * depth, text)\n            if isinstance(entry, Task):\n                j = 0\n                if entry.complete:\n                    color = self._color_of(1.0)\n                    if bar_w &gt; 0:  # pragma: no cover\n                        prog_pad.addstr(i, 0, bar_elem * bar_w, color)\n                        j += len(bar_elem) * bar_w\n                    sum_ = len(entry.units)\n                else:\n                    if bar_w &gt; 0:  # pragma: no cover\n                        for size, comp, total in self._compute_bar(entry.units, bar_w):\n                            color = self._color_of(comp / total)\n                            prog_pad.addstr(i, j, bar_elem * size, color)\n                            j += size * len(bar_elem)\n                    sum_ = sum(entry.units)\n                total = len(entry.units)\n                perc = round((sum_ / total) * 100, 2)\n                text = f\"{sum_}/{total} {perc}%\"\n                if len(text) + 2 &lt; PROG_W:  # pragma: no cover\n                    prog_pad.addstr(i, j + 2, text)\n\n        title_pad.noutrefresh(max(0, TITLE_H - H), 0, 0, 0, H - 1, W - 1)\n        prog_pad.noutrefresh(max(0, TITLE_H - H), 0, 0, TITLE_W, H - 1, W - 1)\n        curses.doupdate()\n\n    def _curses(self, stdscr: window) -&gt; None:\n        self._init_colors()\n        root = TaskGroup(\"__root__\")\n        global_step = -1\n        while not self._stop_flag_forced.is_set():\n            time.sleep(self._refresh_rate)\n            global_step = global_step + 1 % 10000\n            while True:\n                try:\n                    event = self._buffer.popleft()\n                except:\n                    break\n\n                if isinstance(event, TaskStartEvent):\n                    task = self._spawn_task(root, event.task_id, event.total)\n                elif isinstance(event, TaskUpdateEvent):\n                    task = self._get_task(root, event.task_id)\n                    task.units[event.unit] = True\n                elif isinstance(event, TaskCompleteEvent):\n                    task = self._get_task(root, event.task_id)\n                    task.complete = True\n\n            list_of_tasks = self._preorder(root, -1)[1:]\n            if len(list_of_tasks) &gt; 0:\n                self._render_tasks(stdscr, list_of_tasks, global_step)\n\n            if (\n                self._stop_flag_graceful.is_set()\n                and not self._buffer\n                and self._read_thread\n                and not self._read_thread.is_alive()\n            ):\n                break\n\n    def _read_loop(self) -&gt; None:\n        eq = self._eq\n        assert eq is not None\n        while not self._stop_flag_forced.is_set():\n            while (event := eq.capture_blocking(timeout=0.1)) is not None:\n                self._buffer.append(event)\n            if event is None and self._stop_flag_graceful.is_set():\n                break\n\n    def _tui_loop(self) -&gt; None:\n        wrapper(self._curses)\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.CursesTracker.__init__","title":"<code>__init__(refresh_rate=0.1)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>refresh_rate</code> <code>float</code> <p>The refresh rate of the display in seconds. Defaults to 0.1.</p> <code>0.1</code> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>def __init__(self, refresh_rate: float = 0.1) -&gt; None:\n    \"\"\"\n    Args:\n        refresh_rate (float, optional): The refresh rate of the display in seconds.\n            Defaults to 0.1.\n    \"\"\"\n    super().__init__()\n    self._refresh_rate = refresh_rate\n    self._n_shades = 10\n\n    self._eq: EventQueue | None = None\n    self._tui_thread: threading.Thread | None = None\n    self._read_thread: threading.Thread | None = None\n    self._buffer: deque[Event] = deque()\n    self._stop_flag_forced = threading.Event()\n    self._stop_flag_graceful = threading.Event()\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Task","title":"<code>Task</code>  <code>dataclass</code>","text":"<p>Data container that represents the state of a task.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass Task:\n    \"\"\"Data container that represents the state of a task.\"\"\"\n\n    name: str\n    \"\"\"The name of the task.\"\"\"\n    units: list[bool]\n    \"\"\"A list of booleans that represent the completion state of each unit.\"\"\"\n    complete: bool = False\n    \"\"\"Whether the task is complete.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Task.complete","title":"<code>complete = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether the task is complete.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Task.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the task.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Task.units","title":"<code>units</code>  <code>instance-attribute</code>","text":"<p>A list of booleans that represent the completion state of each unit.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskCompleteEvent","title":"<code>TaskCompleteEvent</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrackingEvent</code></p> <p>Event that signals the completion of a task.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass TaskCompleteEvent(TrackingEvent):\n    \"\"\"Event that signals the completion of a task.\"\"\"\n\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskGroup","title":"<code>TaskGroup</code>  <code>dataclass</code>","text":"<p>Data container that represents a group of tasks.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass TaskGroup:\n    \"\"\"Data container that represents a group of tasks.\"\"\"\n\n    name: str\n    \"\"\"The name of the group.\"\"\"\n    groups: OrderedDict[str, \"TaskGroup\"] = field(default_factory=OrderedDict)\n    \"\"\"A dictionary of subgroups.\"\"\"\n    tasks: OrderedDict[str, Task] = field(default_factory=OrderedDict)\n    \"\"\"A dictionary of tasks.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskGroup.groups","title":"<code>groups = field(default_factory=OrderedDict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of subgroups.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskGroup.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the group.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskGroup.tasks","title":"<code>tasks = field(default_factory=OrderedDict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A dictionary of tasks.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskStartEvent","title":"<code>TaskStartEvent</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrackingEvent</code></p> <p>Event that signals the start of a task.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass TaskStartEvent(TrackingEvent):\n    \"\"\"Event that signals the start of a task.\"\"\"\n\n    total: int\n    \"\"\"The total number of units in the task.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskStartEvent.total","title":"<code>total</code>  <code>instance-attribute</code>","text":"<p>The total number of units in the task.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskUpdateEvent","title":"<code>TaskUpdateEvent</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TrackingEvent</code></p> <p>Event that signals the update of a task.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass TaskUpdateEvent(TrackingEvent):\n    \"\"\"Event that signals the update of a task.\"\"\"\n\n    unit: int\n    \"\"\"The index of the unit that was updated.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TaskUpdateEvent.unit","title":"<code>unit</code>  <code>instance-attribute</code>","text":"<p>The index of the unit that was updated.</p>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Tracker","title":"<code>Tracker</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for tracking the progress of a workflow.</p> <p>Subclasses should implement the <code>attach</code> and <code>detach</code> methods.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>class Tracker(ABC):\n    \"\"\"Base class for tracking the progress of a workflow.\n\n    Subclasses should implement the `attach` and `detach` methods.\n    \"\"\"\n\n    @abstractmethod\n    def attach(self, event_queue: EventQueue) -&gt; None:\n        \"\"\"Attach the tracker to an event queue.\n\n        Args:\n            event_queue (EventQueue): The event queue to attach to.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def detach(self, graceful: bool = True) -&gt; None:\n        \"\"\"Detach the tracker from the event queue.\n\n        Args:\n            graceful (bool, optional): Whether to wait for the event queue to be empty\n                before detaching. Defaults to True.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Tracker.attach","title":"<code>attach(event_queue)</code>  <code>abstractmethod</code>","text":"<p>Attach the tracker to an event queue.</p> <p>Parameters:</p> Name Type Description Default <code>event_queue</code> <code>EventQueue</code> <p>The event queue to attach to.</p> required Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@abstractmethod\ndef attach(self, event_queue: EventQueue) -&gt; None:\n    \"\"\"Attach the tracker to an event queue.\n\n    Args:\n        event_queue (EventQueue): The event queue to attach to.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.Tracker.detach","title":"<code>detach(graceful=True)</code>  <code>abstractmethod</code>","text":"<p>Detach the tracker from the event queue.</p> <p>Parameters:</p> Name Type Description Default <code>graceful</code> <code>bool</code> <p>Whether to wait for the event queue to be empty before detaching. Defaults to True.</p> <code>True</code> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@abstractmethod\ndef detach(self, graceful: bool = True) -&gt; None:\n    \"\"\"Detach the tracker from the event queue.\n\n    Args:\n        graceful (bool, optional): Whether to wait for the event queue to be empty\n            before detaching. Defaults to True.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TrackingEvent","title":"<code>TrackingEvent</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Event</code></p> <p>Base class for tracking events.</p> Source code in <code>pipewine/workflows/tracking.py</code> <pre><code>@dataclass\nclass TrackingEvent(Event):\n    \"\"\"Base class for tracking events.\"\"\"\n\n    task_id: str\n    \"\"\"The unique identifier of the task.\"\"\"\n</code></pre>"},{"location":"autoapi/pipewine/workflows/tracking/#pipewine.workflows.tracking.TrackingEvent.task_id","title":"<code>task_id</code>  <code>instance-attribute</code>","text":"<p>The unique identifier of the task.</p>"},{"location":"tutorial/actions/","title":"\u2699\ufe0f Actions","text":""},{"location":"tutorial/actions/#overview","title":"Overview","text":"<p>In this section you will learn what are the main operational abstractions that define how Pipewine transforms data, how to interact with them and how to build re-usable building blocks for your data pipelines.</p> <p>Note</p> <p>This section assumes you already know the basics on the way the data model is structured. If you haven't already, go take a look at the Data Model section. </p> <p>The main components that are explained in this section include:</p> <ul> <li>Sources - Action that creates Pipewine datasets from external storages.</li> <li>Sinks - Action that saves Pipewine datasets to external storages.</li> <li>Operators - Action that transforms Pipewine dataset/s into Pipewine dataset/s.</li> <li>Mappers - Applies the same operation to every sample of a dataset.</li> <li>Grabber - Distributes work to a multi-processing pool of workers.</li> </ul>"},{"location":"tutorial/actions/#types-of-actions","title":"Types of Actions","text":"<p>Pipewine uses the broad term \"Action\" to denote any of the following components:</p> <ul> <li><code>DatasetSource</code> - A component that is able to create one or more <code>Dataset</code> objects.  </li> <li><code>DatasetSink</code> - A component that is able to consume one or more <code>Dataset</code> objects. </li> <li><code>DatasetOperator</code> - A component that accepts and returns one or more <code>Dataset</code> objects.</li> </ul> <p>In general, actions can accept and return one of the following:</p> <ul> <li>A single <code>Dataset</code> object.</li> <li>A <code>Sequence</code> of datasets.</li> <li>A <code>tuple</code> of datasets.</li> <li>A <code>Mapping</code> of strings to datasets.</li> <li>A <code>Bundle</code> of datasets.</li> </ul> <p>Specific types of actions statically declare the type of inputs and outputs they accept/return to ensure that their type is always known at development time (as soon as you write the code).</p> <p>Example</p> <p><code>CatOp</code> is a type of <code>DatasetOperation</code> that concatenates one or more datasets into a single one, therefore, its input type is <code>Sequence[Dataset]</code> and its output is <code>Dataset</code>.</p> <p>Any modern static type checker such as <code>mypy</code> or <code>PyLance</code>  will complain if you try to pass anything else to it, preventing such errors at dev time.</p> <p>Tip</p> <p>As a general rule of thumb of what type of input/output to choose when implementing custom actions:</p> <ul> <li>Use <code>Dataset</code> when you want to force that the action accepts/returns exactly one dataset.</li> <li>Use <code>Sequence</code> when you need the operation to accept/return more than one dataset, but you don't know a priory how many, their order or their type.</li> <li>Use <code>tuple</code> in settings similar to the <code>Sequence</code> case but you also know at dev time the exact number of datasets, their order and their type. Tuples, contrary to <code>Sequences</code>, also allow you to specify the type of each individual element.</li> <li>Use <code>Mapping</code> when you need to accept/return collections of datasets that do not have any ordering relationship, but are instead associated with an arbitrary (not known a priori) set of string keys.</li> <li>Use <code>Bundle</code> in settings similar to the <code>Mapping</code> case but you also know at dev time the exact number of datasets, the name of their keys and their type. More details on <code>Bundle</code> in the next sub-section. </li> </ul>"},{"location":"tutorial/actions/#bundle-objects","title":"Bundle objects","text":"<p>Pipewine <code>Bundle[T]</code> are objects that have the following characteristics:</p> <ul> <li>They behave like a Python <code>dataclass</code>.</li> <li>The type of all fields is bound to <code>T</code>.</li> <li>Field names and types are statically known and cannot be modified.</li> <li>They are anemic objects: they only act as a data container and define no methods.</li> </ul> <p>Note</p> <p>You may already have encountered the <code>Bundle</code> type without noticing when we introduced the TypedSample. <code>TypedSample</code> is a <code>Bundle[Item]</code>.</p> <p>Example</p> <p>Creating a <code>Bundle</code> is super easy:</p> <pre><code># Inherit from Bundle[T] and declare some fields\nclass MyBundle(Bundle[str]):\n    foo: str\n    bar: str\n\n# Constructs like a dataclass\nmy_bundle = MyBundle(foo=\"hello\", bar=\"world\")\n\n# Access content with dot notation\nmy_bundle.foo # &gt;&gt;&gt; hello\nmy_bundle.bar # &gt;&gt;&gt; world\n\n# Convert to a plain dict[str, T] when needed\nmy_bundle.as_dict() # &gt;&gt;&gt; {\"foo\": \"hello\", \"bar\": \"world\"}\n\n# Create from a dictionary\nmy_bundle_2 = MyBundle.from_dict({\"foo\": \"hello\", \"bar\": \"world\"})\n</code></pre> <p>Note</p> <p>Despite their similarity <code>Bundle</code> objects do not rely on Pydantic and do not inherit from <code>BaseModel</code>. Do not expect them to provide validation, parsing or dumping functionalities, as they are essentially dataclasses plus some constraints.</p>"},{"location":"tutorial/actions/#eagerlazy-behavior","title":"Eager/Lazy Behavior","text":"<p>Suppose we need to load and display N images stored as files in a directory, comparing the two approaches:</p> <ul> <li>Eager - Every computation is carried out as soon as the necessary data is ready.</li> <li>Lazy - Every operation is performed only when it cannot be delayed anymore.</li> </ul> <p>Let's explore the trade-off between the two approaches:</p> Eager Lazy \u274c Long initialization time: all images must be loaded before the first one is displayed. \u2705 Short initialization time: no loading is done during initialization. \u2705 When the user requests an image, it is immediately available. \u274c When the user requests an image, it must wait for the application to load it before viewing it. \u274c High (and unbounded) memory usage, necessary to keep all images loaded at the same time. \u2705 Always keep at most one image loaded into memory. \u274c Risk of performing unnecessary computation: what if the user only wants to display a small subset of images? \u2705 If the user is not interested in a particular image, it won't be loaded at all. \u2705 Accessing an image multiple times has no additional cost. \u274c Accessing an image multiple times has a cost proportional to the number of times it is accessed. \u2705 Optimizations may be easier and more effective on batched operations. \u274c Optimizations are harder to perform or less effective on small batches accessed out-of-order. <p>As you can see, there are positive and negative aspects in both approaches. You may be inclined to think that for this silly example of loading and displaying images, the lazy approach is clearly better. After all, who would use a software that needs to load the entire photo album of thousands of photos just to display a single image?</p> <p>As you may have noticed in the previous section, Pipewine allows you to choose both approaches, but when there is no clear reason to prefer the eager behavior, its components are implemented so that they behave lazily: <code>Item</code> only reads data when requested to do so, <code>LazyDataset</code> creates the actual <code>Sample</code> object upon indexing. As you will see, most Pipewine actions follow a similar pattern.</p> <p>This decision, inherited from the old Pipelime library, is motivated mainly by the following reasons:</p> <ul> <li>Data pipelines may operate on large amounts of data that we cannot afford to load upfront and keep loaded into system memory. Even small datasets with hundreds of images will impose a significant memory cost on any modern machine. Larger datasets may not be possible to load at all!</li> <li>Until a time machine is invented, there is no way to undo wasteful computation. Suppose you have an image classification dataset and you want to select all samples with the category \"horse\": there is no point in loading an image once you know that it's not what you are looking for. Lazy behavior prevents this kind of unnecessary computation by design.</li> <li>The risk of performing some computation twice can be mitigated by using caches with the right eviction policy.</li> </ul>"},{"location":"tutorial/actions/#parallelism","title":"Parallelism","text":"<p>Pipewine provides you with a <code>Grabber</code> object, a simple tool allows you to iterate over a sequence with a parallel pool of multi-processing workers.</p> <p>Why multi-processing and not multi-threading? Because of GIL (Global Interpreter Lock), multi-threading in Python allows concurrency but not parallelism, effectively granting the same speed as a single-threaded program.</p> <p>To effectively parallelize our code we are left with two options:</p> <ol> <li>Avoid using Python, and instead write components in other languages such as C/C++, then create bindings that allow the usage from Python. </li> <li>Bite the bullet and use multi-processing instead of multi-threading. If compared with threads, processes are a lot more expensive to create and manage, often requiring to serialize and deserialize the whole execution stack upon creation.</li> </ol> <p>The second option is clearly better: </p> <ul> <li>You still get the option to parallelize work in a relatively easy way without having to integrate complex bindings for execution environments other than the Python interpreter. </li> <li>Process creation and communication overhead becomes negligible when dealing with large amounts of relatively independent jobs that don't need much synchronization. This is a relatively common scenario in data pipelines.</li> <li>It does not prevent you from implementing optimized multi-threaded components in other languages. Think about it: whenever you use tools like Numpy inside a Pipewine action, you are doing exactly this. </li> </ul>"},{"location":"tutorial/actions/#grabber","title":"Grabber","text":"<p>To parallelize tasks with Pipewine you can use a <code>Grabber</code> object: a simple wrapper around a multiprocessing <code>Pool</code> of workers iterate through an ordered sequence of objects.</p> <p>Note</p> <p><code>Grabber</code> objects can work on arbitrary sequences of objects, provided they are serializable.</p> <p>The <code>Grabber</code> object also does a few nice things for you:</p> <ul> <li>Manages the lifecycle of the underlying pool of processes.</li> <li>Handles exceptions freeing resources and returning the errors transparently to the parent process.</li> <li>Automatically invokes a callback function whenever a task is completed. The callback is run by the individual workers.</li> </ul> <p>When creating a <code>Grabber</code> you need to choose three main parameters that govern the execution:</p> <ul> <li><code>num_workers</code>: The number of concurrent workers. Ideally, in a magic world where all workload is perfectly distributed and communication overhead is zero, the total time is proportional to <code>num_jobs / num_workers</code>, so the higher the better. In reality, there are many factors that make the returns of adding more processes strongly diminishing, and sometimes even negative:<ul> <li>Work is not perfectly distributed and it's not complete until the slowest worker hasn't finished. </li> <li>Processes are expensive to create. With small enough workloads, a single process may actually be faster than a concurrent pool.</li> <li>Everytime an object is passed from one process to the other it needs to be serialized and then de-serialized. This can become quite expensive when dealing with large data structures.</li> <li>Processes need synchronization mechanisms that temporarily halt the computation.</li> <li>Sometimes the bottleneck is not the computation itself: if a single process reading data from the network completely saturates your bandwidth, adding more processes won't fix the issue.</li> <li>Sometimes multiprocessing can be much slower than single processing. A typical example is when concurrently writing to a mechanical HDD, causing it to waste a lot of time going back and forth the writing locations.  </li> <li>Your code may already be parallelized. Whenever you use libraries like Numpy, PyTorch, OpenCV (and many more), you are calling C/C++ bindings that run very efficient multi-threaded code outside of the Python interpreter, or even code that runs on devices other than your CPU (e.g. CUDA-enabled GPUs). Adding multiprocessing in these cases will only add overhead costs to something that already uses your computational resources nearly optimally.</li> <li>Due to memory constraints the number of processes you can keep running may be limited to a small number. E.g. if each process needs to keep loaded a 10GB chunk of data and the available memory is just 32GB, the maximum amount of processes you can run without swapping is 3. Adding a 4th process will make the system start swapping, greatly deteriorating the overall execution speed.</li> </ul> </li> <li><code>prefetch</code>: The number of tasks that are assigned to each worker whenever they are ready. It's easier to explain this with an analogy. Imagine you need to deliver 1000 products to customers and you have 4 couriers ready to deliver them. How inefficient would it be if every courier delivered one product at a time, returning to the warehouse whenever they complete a delivery? You would still parallelize work across 4 couriers, but would also incur in massive synchronization costs (the extra time it takes for the courier to return to the warehouse each time). A smarter solution would be to assign a larger batch of deliveries to each courier (e.g. 50) so that they would have to return to the warehouse less frequently. </li> <li><code>keep_order</code>: Sometimes, the order in which the operations are performed is not that relevant. Executing tasks out-of-order requires even less synchronization, usually resulting in faster overall execution.</li> </ul> <p>Let's see an example of how a <code>Grabber</code> works. </p> <p>Example</p> <p>We want to iterate through a sequence of objects that, whenever indexed, performs some expensive operation. In this example we simulate it using a <code>time.sleep()</code>.</p> <p>Here is our <code>SlowSequence</code>:</p> <pre><code>class SlowSequence(Sequence[int]):\n\"\"\"Return the numbers in range [A, B), slowly.\"\"\"\n\n    def __init__(self, start: int, stop: int) -&gt; None:\n        self._start = start\n        self._stop = stop\n\n    def __len__(self) -&gt; int:\n        return self._stop - self._start\n\n    @overload\n    def __getitem__(self, idx: int) -&gt; int: ...\n    @overload\n    def __getitem__(self, idx: slice) -&gt; \"SlowSequence\": ...\n    def __getitem__(self, idx: int | slice) -&gt; \"SlowSequence | int\":\n        # Let's not worry about slicing for now\n        if isinstance(idx, slice):\n            raise NotImplementedError()\n\n        # Raise if index is out of bounds\n        if idx &gt;= len(self):\n            raise IndexError(idx)\n\n        # Simulate a slow operation and return\n        time.sleep(0.1)\n        return idx + self._start\n</code></pre> <p>Let's create an instance of <code>SlowSequence</code> and define a callback function that we need to call every time we iterate on it:</p> <pre><code>def my_callback(index: int) -&gt; None:\n    print(f\"Callback {index} invoked by process {os.getpid()}\")\n\n# The sequence object of which we want to compute the sum\nsequence = SlowSequence(100, 200)\n</code></pre> <p>Next, let's try to compute the total sum of a <code>SlowSequence</code> with a simple for loop.</p> <pre><code># Compute the sum, and measure total running time\nt1 = time.perf_counter()\n\ntotal = 0\nfor i, x in enumerate(sequence):\n    my_callback(i)\n    total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 10.008610311000666\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total running time is roughly 10s, checks out, since every iteration takes approximately 0.1s and we iterate 100 times. The PID printed by the callback function is always the same, since all computation is performed by a single process.</p> <p>Let's do the same but with with a <code>Grabber</code> with 5 concurrent workers</p> <pre><code>t1 = time.perf_counter()\n\ntotal = 0\ngrabber = Grabber(num_workers=5)\nwith grabber(sequence, callback=my_callback) as par_sequence:\n    for i, x in par_sequence:\n        total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 2.011717862998921\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total time is now near to 2s, the result of evenly splitting among 5 parallel workers. As you can see from the console, the callback is being called by 5 different PIDs:</p> <pre><code>Callback 0 invoked by process 21219\nCallback 2 invoked by process 21220\nCallback 4 invoked by process 21221\nCallback 6 invoked by process 21222\nCallback 8 invoked by process 21223\n...\n</code></pre> <p>Important note: what <code>Grabber</code> just parallelizes the <code>__getitem__</code> method and the callback function. The body of the for loop (the summation) is not parallelized and it's executed by the parent process.</p>"},{"location":"tutorial/actions/#sources-and-sinks","title":"Sources and Sinks","text":"<p><code>DatasetSource</code> and <code>DatasetSink</code> are the two base classes for every Pipewine action that respectively reads or writes datasets (or collections of datasets) from/to external storages.</p> <p>Pipewine offers built-in support for Underfolder datasets, a flexible dataset format inherited by Pipelime that works well with many small-sized multi-modal datasets with arbitrary content. </p> <p>In order to provide a simple yet effective alternative to just read a dataset of images from a directory tree, pipewine also provides you the \"Images Folder\" format.</p> <p>You are strongly encouraged to create custom dataset sources and sinks for whatever format you like.</p>"},{"location":"tutorial/actions/#underfolder","title":"Underfolder","text":"<p>An underfolder dataset is a directory located anywhere on the file system, with no constraint on its name. Every underfolder must contain a subfolder named <code>data</code>, plus some additional files.</p> <p>Every file contained in the <code>data</code> subfolder corresponds to an item and must be named: <code>$INDEX_$KEY.$EXTENSION</code>, where:</p> <ul> <li><code>$INDEX</code> is a non-negative integer number representing the index of the sample in the dataset, prefixed with an arbitrary number of <code>0</code> characters that are ignored. </li> <li><code>$KEY</code>  is a string representing the key of the item in the sample. Every name that can be used as a Python variable name is a valid key.</li> <li><code>$EXTENSION</code> is the file extension you would normally use. Pipewine can only read files if there is a registered <code>Parser</code> class that supports its extension.  </li> </ul> <p>Every file outside of the <code>data</code> folder (also called \"root file\") represents a shared item that every sample in the dataset will inherit.</p> <p>Warning</p> <p>All sample indices must be contiguous: if a sample with index N is present, then all the samples from 0 to N-1 must also be present in the dataset.</p> <p>Pros and cons of the Underfolder dataset format:</p> <ul> <li>\u2705 No need to setup databases, making them extremely easy to create and access. You can even create them by renaming a bunch of files using your favourite file explorer or a shell script.</li> <li>\u2705 You can easily inspect and edit the content of an underfolder dataset just by opening the individual files you are interested in with your favourite tools.</li> <li>\u2705 It's a very flexible format that you can use in many scenarios. </li> <li>\u274c Databases exist for a reason. Putting 100.000 files in a local directory and expecting it to be efficient is pure fantasy. You should avoid using underfolders when dealing with large datasets or when performance is critical.</li> <li>\u274c There is an issue when used with datasets where all items are shared (an edge case that's very rare in practice). In these cases, the original length of the dataset is lost after writing. Although fixing this issue is quite easy, it would break the forward-compatibility (reading data written by a future version of the library), so it will likely remain unfixed.</li> </ul> <p>Example</p> <p>You can see an example of an Underfolder dataset here. The dataset contains samples corresponding to the 26 letters of the english alphabet, with RGB images and metadata.</p> <p>Basic <code>UnderfolderSource</code> usage with no typing information:</p> <pre><code># Create the source object from an existing directory Path.\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\nsource = UnderfolderSource(path)\n\n# Call the source object to create a new Dataset instance.\ndataset = source()\n\n# Do stuff with the dataset\nsample = dataset[4]\nprint(sample[\"image\"]().reshape(-1, 3).mean(0))  # &gt;&gt;&gt; [244.4, 231.4, 221.7]\nprint(sample[\"metadata\"]()[\"color\"])  #            &gt;&gt;&gt; \"orange\"\nprint(sample[\"shared\"]()[\"vowels\"])  #             &gt;&gt;&gt; [\"a\", \"e\", \"i\", \"o\", \"u\"]\n</code></pre> <p>Fully typed usage:</p> <pre><code>class LetterMetadata(pydantic.BaseModel):\n    letter: str\n    color: str\n\nclass SharedMetadata(pydantic.BaseModel):\n    vowels: list[str]\n    consonants: list[str]\n\nclass LetterSample(TypedSample):\n    image: Item[np.ndarray]\n    metadata: Item[LetterMetadata]\n    shared: Item[SharedMetadata]\n\n# Create the source object from an existing directory Path.\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\nsource = UnderfolderSource(path, sample_type=LetterSample)\n\n# Call the source object to create a new Dataset instance.\ndataset = source()\n\n# Do stuff with the dataset\nsample = dataset[4]\nprint(sample.image().reshape(-1, 3).mean(0))  # &gt;&gt;&gt; [244.4, 231.4, 221.7]\nprint(sample.metadata().color)  #               &gt;&gt;&gt; \"orange\"\nprint(sample.shared().vowels)  #                &gt;&gt;&gt; [\"a\", \"e\", \"i\", \"o\", \"u\"]\n</code></pre> <p>You can use <code>UnderfolderSink</code> to write any Pipewine dataset, even if it wasn't previously read as an underfolder.</p> <pre><code># Write the dataset with an underfolder sink\noutput_path = Path(gettempdir()) / \"underfolder_write_example\"\nsink = UnderfolderSink(output_path)\nsink(dataset) # &lt;-- Writes data to the specified path.\n</code></pre>"},{"location":"tutorial/actions/#images-folders","title":"Images Folders","text":"<p>Images folder are a much simpler alternative when the input dataset only consists of unstructured image files contained in a directory tree. </p> <p>By default, <code>ImagesFolderSource</code> looks at image files inside the specified directory path. If used with the <code>recursive</code> argument set to true, it will also look recursively into subfolders. The image files are sorted in lexicographic order to ensure that the result is always deterministic.</p> <p>Contrary to the Underfolder format, which can accept and parse any type of sample, this source forces its samples to be <code>ImageSample</code> objects, i.e. samples that only contain an <code>image</code> item represented by a numpy array.</p> <p>Example</p> <p>Basic <code>ImagesFolderSource</code> usage:</p> <p>``` py</p>"},{"location":"tutorial/actions/#create-the-source-object-from-an-existing-directory-path","title":"Create the source object from an existing directory Path.","text":"<p>path = Path(\"tests/sample_data/images_folders/folder_1\") source = ImagesFolderSource(path, recursive=True)</p>"},{"location":"tutorial/actions/#call-the-source-object-to-create-a-new-dataset-instance","title":"Call the source object to create a new Dataset instance.","text":"<p>dataset = source()</p>"},{"location":"tutorial/actions/#custom-formats","title":"Custom Formats","text":"<p>You can add custom implementations of the <code>DatasetSource</code> and <code>DatasetSink</code> interfaces that allow reading and writing datasets with custom formats. To better illustrate how to do so, this section will walk you through an example use case of a source and a sink for folders containing JPEG image files.</p> <p>Warning</p> <p>In case you really need to read images from an unstructured folder, just use the <code>ImagesFolderSource</code> class, without having to write it yourself. This is just a simplified example to help you understand how these components work.</p> <p>Before you start, you should always think of the type of datasets you are going to read. Some formats may be flexible enough to support any kind of dataset, while others may be restricted to only a specific type. Pipewine gives you the tools to choose any of the two options in a type-safe way, but also to completely ignore all the typing part and to always return un-typed structures as it was with the old Pipelime.</p> <p>Example</p> <p>In this example case it's very simple: we only want to read samples that contain a single item named \"image\" that loads as a numpy array:</p> <pre><code>class ImageSample(TypedSample):\n    image: Item[np.ndarray]\n</code></pre> <p>Next, let's implement the dataset source:</p> <ol> <li>Inherit from <code>DatasetSource</code> and specify the type of data that will be read.</li> <li>[Optional] Implement an <code>__init__</code> method. </li> <li>Implement the <code>__call__</code> method, accepting no arguments and returning an instance of chosen type of dataset. You can choose to load the whole dataset upfront (eager), or to return a <code>LazyDataset</code> that will load the samples only when needed.</li> </ol> <p>Tip</p> <p>When overriding the <code>__init__</code> method, always remember to call <code>super().__init__()</code>, otherwise the object won't be correctly initialized.</p> <p>Example</p> <pre><code>class ImagesFolderSource(DatasetSource[Dataset[ImageSample]]):\n    def __init__(self, folder: Path) -&gt; None:\n        super().__init__()  # Always call the superclass constructor!\n        self._folder = folder\n        self._files: list[Path]\n\n    def __call__(self) -&gt; Dataset[ImageSample]:\n        # Find all JPEG files in the folder in lexicographic order.\n        jpeg_files = filter(lambda x: x.suffix == \".jpeg\", self._folder.iterdir())\n        self._files = sorted(list(jpeg_files))\n\n        # Return a lazy dataset thet loads the samples with the _get_sample method\n        return LazyDataset(len(self._files), self._get_sample)\n\n    def _get_sample(self, idx: int) -&gt; ImageSample:\n        # Create an Item that reads a JPEG image from the i-th file.\n        reader = LocalFileReader(self._files[idx])\n        parser = JpegParser()\n        image_item = StoredItem(reader, parser)\n\n        # Return an ImageSample that only contains the image item.\n        return ImageSample(image=image_item)\n</code></pre> <p>Next, let's implement the dataset sink, which is somewhat specular to the source:</p> <ol> <li>Inherit from <code>DatasetSink</code> and specify the type of data that will be written.</li> <li>[Optional] Implement an <code>__init__</code> method. </li> <li>Implement the <code>__call__</code> method, accepting an instance of the chosen type of dataset and returning nothing. </li> </ol> <p>Tip</p> <p>Since sinks are always placed at the end of pipelines, computation cannot be delayed any further, giving them no option but to loop over the data in an eager way inside the <code>__call__</code> method. We recommend doing this using the <code>self.loop</code> method with a grabber. Doing this has two advantages:</p> <ol> <li>It loops over the data using a grabber, meaning that if there are some lazy operations that still need to be computed, they can be run efficiently in parallel.</li> <li>It automatically invokes callbacks that send progress updates to whoever is listening to them, enabling live progress tracking in long-running jobs.</li> </ol> <p>Example</p> <pre><code>class ImagesFolderSink(DatasetSink[Dataset[ImageSample]]):\n    def __init__(self, folder: Path, grabber: Grabber | None = None) -&gt; None:\n        super().__init__()  # Always call the superclass constructor!\n        self._folder = folder\n        self._grabber = grabber or Grabber()\n\n    def __call__(self, data: Dataset[ImageSample]) -&gt; None:\n        self._folder.mkdir(parents=True, exist_ok=True)\n\n        # Compute the amount of 0-padding to preserve lexicographic order.\n        zpadding = len(str(len(data)))\n\n        # Iterate over the dataset and write each sample.\n        for i, sample in self.loop(data, self._grabber, name=\"Writing\"):\n            fname = self._folder / f\"{str(i).zfill(zpadding)}.jpeg\"\n            write_item_to_file(sample.image, fname)\n</code></pre> <p>Custom dataset sources and sinks can be registered to the Pipewine CLI to allow you to manipulate your own datasets using the same commands you would normally use for every other dataset format. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/actions/#operators","title":"Operators","text":"<p>We have seen how to read and write datasets using dataset sources and sinks, now it's time to see how to apply operations that transform datasets into other datasets. This is done by implementations of the <code>DatasetOperator</code> base class. </p> <p>Similarly to sources and sinks, operators must statically declare the type of data they accept as input and the one they return, for a total of 25 (5x5) possible combinations of input and output types.</p> <p>Example</p> <p>For example, a dataset operator that splits a single dataset into exactly 3 parts, should specify:</p> <ul> <li>Input type: <code>Dataset</code></li> <li>Output type: <code>tuple[Dataset, Dataset, Dataset]</code></li> </ul> <p>Operators are constructed as plain Python objects, then they behave like a Python <code>Callable</code> accepting and returning either a dataset or a collection of datasets.</p> <p>Example</p> <p>Here is an example of how to use a <code>DatasetOperator</code> that concatenates two or more datasets into a single one.</p> <pre><code># Given N datasets\ndataset1: Dataset\ndataset2: Dataset\ndataset3: Dataset\n\n# Construct the operator object\nconcatenate = CatOp()\n\n# Call the object\nfull_dataset = concatenate([dataset1, dataset2, dataset3])\n</code></pre> <p>Just like most Pipewine components, dataset operators can either be:</p> <ul> <li>Eager: performing all the computation when called and returning dataset/s containing the computation results. This is similar to the way old Pipelime's <code>PipelimeCommand</code> objects behave.</li> <li>Lazy: the call immediately returns a <code>LazyDataset</code> instance that performs the actual computation when requested. </li> </ul> <p>Tip</p> <p>In reality, it often makes sense to use a mix of the two approaches. You can perform some eager computation upfront then return a lazy dataset that uses the result of the previous computation.  </p>"},{"location":"tutorial/actions/#built-in-operators","title":"Built-in Operators","text":"<p>Pipewine has some useful generic operators that are commonly used in many workflows. Here is a list of the currently available built-in operators with a brief explanation. More in-depth documentation in the API reference.</p> <p>Iteration operators: operators that operate on single datasets changing their length or the order of samples.</p> <ul> <li><code>SliceOp</code>: return a slice (as in Python slices) of a dataset.</li> <li><code>RepeatOp</code>: replicate the samples of a dataset N times.</li> <li><code>CycleOp</code>: replicate the samples of a dataset until the desired number of samples is reached.</li> <li><code>IndexOp</code>: select a sub-sequence of samples of a dataset with arbitrary order.</li> <li><code>ReverseOp</code>: invert the order of samples in a dataset (same as <code>SliceOp(step=-1)</code>).</li> <li><code>PadOp</code>: extend a dataset to a desired length by replicating the i-th sample.</li> </ul> <p>Merge operators: operators that merge a collection of datasets into a single one.</p> <ul> <li><code>CatOp</code>: concatenate one or more datasets into a single dataset preserving the order of samples.</li> <li><code>ZipOp</code>: zip two or more dataset with the same length by merging the contents of the individual samples.</li> </ul> <p>Split operators: operators that split a single dataset into a collection of datasets.</p> <ul> <li><code>BatchOp</code>: split a dataset into many datasets of the desired size.</li> <li><code>ChunkOp</code>: split a dataset into a desired number of chunks (of approximately the same size).</li> <li><code>SplitOp</code>: split a dataset into an arbitrary amount of splits with arbitrary size.</li> </ul> <p>Functional operators: operators that transform datasets based on the result of a user-defined function.</p> <ul> <li><code>FilterOp</code>: keep only (or discard) samples that verify an arbitrary predicate.</li> <li><code>GroupByOp</code>: split a dataset grouping together samples that evaluate to the same value of a given function.</li> <li><code>SortOp</code>: sort a dataset with a user-defined sorting key function.</li> <li><code>MapOp</code>: apply a user-defined function (<code>Mapper</code>) to each sample of a dataset.</li> </ul> <p>Random operators: operators that apply non-deterministic random transformations.</p> <ul> <li><code>ShuffleOp</code>: sort the samples of a dataset in random order.</li> </ul> <p>Cache operators: operators that do not apply any transformation to the actual data, bu only change the way they are accessed.</p> <ul> <li><code>CacheOp</code>: adds a caching layer that memorizes samples to avoid computing them multiple times.</li> </ul>"},{"location":"tutorial/actions/#custom-operators","title":"Custom Operators","text":"<p>As with sources and sinks, you can implement your own operators that manipulate data as you wish. To help you understand how to do so, this section will walk you through the implementation of an example operator that normalizes images.</p> <p>To implement the dataset operator:</p> <ol> <li>Inherit from <code>DatasetOperator</code> and specify both the input and output types. You can choose to implement operators that work with specific types of data or to allow usage with arbitrary types.</li> <li>[Optional] Implement an <code>__init__</code> method.</li> <li>Implement the <code>__call__</code> method, accepting and returning the previously specified types of data. </li> </ol> <p>Example</p> <p>Here is the code for the <code>NormalizeOp</code>, an example operator that applies a channel-wise z-score normalization to all images of a dataset. </p> <p>The implementation is realatively naive: reading and stacking all images in a dataset is not a good strategy memory-wise. In a real-world scenario, you want to compute mean and standard deviation using constant-memory approaches. However, for the sake of this tutorial we can disregard this aspect and focus on other aspects of the code.</p> <p>Important things to consider:</p> <ul> <li>Since we want to be able to use this operator with many types of dataset, we inherit from <code>DatasetOperator[Dataset, Dataset]</code>, leaving the type of dataset unspecified for now.</li> <li>The <code>__call__</code> method has a typevar <code>T</code> that is used to tell the type-checker that the type of samples contained in the input dataset is preserved in the output dataset, allowing us to use this operator with any dataset we want.</li> <li>In the first part of the <code>__call__</code> method, we eagerly compute mu and sigma vectors iterating over the whole dataset.</li> <li>In the second part of the <code>__call__</code> method we return a lazy dataset instance that applies the normalization.</li> </ul> <pre><code>class NormalizeOp(DatasetOperator[Dataset, Dataset]):\n    def __init__(self, grabber: Grabber | None = None) -&gt; None:\n        super().__init__()\n        self._grabber = grabber or Grabber()\n\n    def _normalize[\n        T: Sample\n    ](self, dataset: Dataset[T], mu: np.ndarray, sigma: np.ndarray, i: int) -&gt; T:\n        # Get the image of the i-th sample\n        sample = dataset[i]\n        image = sample[\"image\"]().astype(np.float32)\n\n        # Apply normalization then bring values back to the [0, 255] range, \n        # clipping values below -sigma to 0 and above sigma to 255.\n        image = (image - mu) / sigma\n        image = (image * 255 / 2 + 255 / 2).clip(0, 255).astype(np.uint8)\n\n        # Replace the image item value\n        return sample.with_value(\"image\", image)\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        # Compute mu and sigma on the dataset (eager)\n        all_images = []\n        for _, sample in self.loop(x, self._grabber, name=\"Computing stats\"):\n            all_images.append(sample[\"image\"]())\n        all_images_np = np.concatenate(all_images)\n        mu = all_images_np.mean((0, 1), keepdims=True)\n        sigma = all_images_np.std((0, 1), keepdims=True)\n\n        # Lazily apply the normalization with precomputed mu and sigma\n        return LazyDataset(len(x), partial(self._normalize, x, mu, sigma))\n</code></pre> <p>Custom dataset operators can be registered to the Pipewine CLI to allow you to apply custom transformations to your datasets using a common CLI. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/actions/#mappers","title":"Mappers","text":"<p>Pipewine <code>Mapper</code> objects (essentially the same as Pipelime Stages), allow you to quickly implement dataset operators by defining a function that transforms individual samples.</p> <p>This allows you to write way less code when all the following conditions apply:</p> <ul> <li>The operation accepts and returns a single dataset.</li> <li>The input and output datasets have the same length.</li> <li>The i-th sample of the output dataset can be computed as a pure function of the i-th sample of the input dataset.</li> </ul> <p>Danger</p> <p>Never use mappers when the function that transforms samples is stateful. Not only Pipewine does not preserve the order of samples when calling mappers, but the execution may be run concurrently in different processes that do not share memory.</p> <p>In these cases, use a <code>DatasetOperator</code> instead of a <code>Mapper</code>.</p> <p>Mappers must be used in combination with <code>MapOp</code>, a built-in dataset operator that lazily applies a mapper to every sample of a dataset.</p> <p>Example</p> <p>Example usage of a mapper that renames items in a dataset:</p> <pre><code># Given a dataset\ndataset: Dataset\n\n# Rename all items named \"image\" into \"my_image\"\nop = MapOp(RenameMapper({\"image\": \"my_image\"}))\n\n# Apply the mapper\ndataset = op(dataset)\n</code></pre> <p>If you need to apply multiple mappers, instead of applying them individually using multiple <code>MapOp</code>, you should compose the mappers into a single one using the built-in <code>ComposeMapper</code> and then apply it with a single <code>MapOp</code>.</p> <p>All mappers are statically annotated with two type variables representing the type of input and output samples they accept/return, enabling static type checking. E.g. <code>Mapper[SampleA, SampleB]</code> accepts samples of type <code>SampleA</code> and returns samples of type <code>SampleB</code>. When composed in a <code>ComposeMapper</code>, it will automatically detect the input and output type from the sequence of mappers it is constructed with.</p> <p>Example</p> <p>Here, the static type-checker automatically infers type <code>Mapper[SampleA, SampleE]</code> for the variable <code>composed</code>:</p> <pre><code>mapper_ab: Mapper[SampleA, SampleB]\nmapper_bc: Mapper[SampleB, SampleC]\nmapper_cd: Mapper[SampleC, SampleD]\nmapper_de: Mapper[SampleD, SampleE]\n\ncomposed = ComposeMapper((mapper_ab, mapper_bc, mapper_cd, mapper_de))\n</code></pre>"},{"location":"tutorial/actions/#built-in-mappers","title":"Built-in Mappers","text":"<p>Pipewine has some useful generic mappers that are commonly used in many workflows. Here is a list of the currently available built-in mappers with a brief explanation. More in-depth documentation in the API reference.</p> <p>Key transformation mappers: modify the samples by adding, removing and renaming keys.</p> <ul> <li><code>DuplicateItemMapper</code>: create a copy of an existing item and give it a different name.</li> <li><code>FormatKeysMapper</code>: rename items using a format string.</li> <li><code>RenameMapper</code>: rename items using a mapping from old to new keys.</li> <li><code>FilterKeysMapper</code>: keep or discard a subset of items.</li> </ul> <p>Item transformation mappers: modify item properties such as parser and sharedness.</p> <ul> <li><code>ConvertMapper</code>: change the parser of a subset of items, e.g. convert PNG to JPEG.</li> <li><code>ShareMapper</code>: change the sharedness of a subset of items.</li> </ul> <p>Cryptography mappers: currently contains only <code>HashMapper</code>.</p> <ul> <li><code>HashMapper</code>: computes a secure hash of a subset of items, useful to perform deduplication or integrity checks.</li> </ul> <p>Special mappers:</p> <ul> <li><code>CacheMapper</code>: converts all items to <code>CachedItem</code>. </li> <li><code>ComposeMapper</code>: composes many mappers into a single object that applies all functions sequentially. </li> </ul>"},{"location":"tutorial/actions/#custom-mappers","title":"Custom Mappers","text":"<p>To implement a <code>Mapper</code>:</p> <ol> <li>Inherit from <code>Mapper</code> and specify both the input and output sample types.</li> <li>[Optional] Implement an <code>__init__</code> method.</li> <li>Implement the <code>__call__</code> method, accepting an integer and the input sample, and returning the output sample.</li> </ol> <p>Danger</p> <p>Remember: mappers are stateless. Never use object fields to store state between subsequent calls.</p> <p>Example</p> <p>In this example we will implement a mapper that inverts the colors of an image.</p> <pre><code>class ImageSample(TypedSample):\n    image: Item[np.ndarray]\n\nclass InvertRGBMapper(Mapper[ImageSample, ImageSample]):\n    def __call__(self, idx: int, x: ImageSample) -&gt; ImageSample:\n        return x.with_value(\"image\", 255 - x.image())\n</code></pre> <p>Custom mappers can be registered to the Pipewine CLI to allow you to apply custom transformations to your datasets using a common CLI. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/cache/","title":"\ud83d\udcbe Cache","text":""},{"location":"tutorial/cache/#overwiew","title":"Overwiew","text":"<p>By default, accessing a Pipewine dataset via the <code>__getitem__</code> method often implies calling a function that performs some computation and then constructs a new Sample object. Furthermore, accessing items on a sample may require a read from external storages e.g. a file system or a remote DB. </p> <p>When dealing with deterministic operations (i.e. same inputs imply the same outputs) it makes little sense to perform these computations and/or reads more than once: we could store the output of the first call and then, for every successive call, just look it up and return it. </p> <p>That's a technique called Memoization that while being very effective at speeding computation up, it also has no bound on the number of elements it keeps stored, meaning that it will eventually end up using all the available memory on our system, if the data we are trying to store does not entirely fit in system memory.</p> <p>Note that if we had the guarantee that every dataset could fit in the system memory, we could just apply every operation eagerly and avoid dealing with this caching headache altogether, but unfortunately (a) it's surprisingly common to deal with datasets that cannot fit in your system's RAM and (b) even if they did, would you be ok with Pipewine using 95% of your system memory?</p> <p>Pipewine has some built-in tools to provide a reasonable trade-off between (a) wasting time performing the same computation multiple times but minimizing the memory usage and (b) eliminating all waste but keeping everything memorized, because often, something in between these two extreme cases is highly preferable. </p> <p>Pipewine allows you to cache data in three different ways:</p> <ul> <li>Cache at the item level, to accelerate consecutive accesses to the same item object, mainly to avoid multiple I/O for the same data.</li> <li>Cache at the dataset level, to accelerate consecutive accesses (<code>__getitem__</code>) to the same dataset, avoiding the re-computation of lazy operations.  </li> <li>Checkpoints, to save all intermediate results to disk (applying all the lazy operations) and then read the results.</li> </ul> <p>Note</p> <p>There is no cache at the sample level, because sample objects receive all their items at construction time and they always own a reference to all items they contain, making them always immediately available. </p>"},{"location":"tutorial/cache/#item-cache","title":"Item Cache","text":"<p>Item cache converts all items of a sample into <code>CachedItem</code> instances. As you may have seen on the previous section of this tutorial, <code>CachedItem</code> is a wrapper of a regular <code>Item</code> store the result of the <code>__call__</code> method when called for the first time, then, every subsequent call simply returns the stored value without calling the inner item twice. <code>CachedItem</code> are lightweight and computationally inexpensive, they merely store a reference and return it.</p> <p>The <code>__call__</code> method of a stored item implies a read from disk (or other sources), calling it multiple times results in the slow read operation being repeated!</p> <p>Example</p> <p>Very slow code, without item cache every time we access the \"image\" item we read it from disk!</p> <p>This runs in approximately 130 ms on my system. </p> <pre><code>dataset = UnderfolderSource(Path(\"my_dataset\"))()\nsample = dataset[0]\n\nfor _ in range(1000):\n    sample[\"image\"]()\n</code></pre> <p>We can improve this situation by using item caches, storing the result of the first read, then immediately return it in every subsequent call.</p> <p>Example</p> <p>Same code with item caches, this time it takes only 0.2 ms!</p> <pre><code>dataset: Dataset = UnderfolderSource(Path(\"my_dataset\"))()\ndataset = ItemCacheOp()(dataset)\nsample = dataset[0]\n\nfor _ in range(1000):\n    sample[\"image\"]()\n</code></pre> <p>Contrary to Pipelime, item caches die with the sample object: whenever we index the dataset, we get a brand new sample object with newly created (thus empty) <code>CachedItem</code> instances.</p> <p>Warning</p> <p>Even though we call <code>ItemCacheOp</code>, this is as slow as the first example where we did not use item caching at all! </p> <p>This is because the sample is re-created every time we index the dataset. <pre><code>dataset: Dataset = UnderfolderSource(Path(\"my_dataset\"))()\ndataset = ItemCacheOp()(dataset)\n\nfor _ in range(1000):\n    dataset[0][\"image\"]()\n</code></pre></p> <p>This also means that unless you keep references to samples, you cannot possibly blow up your memory using item cache.</p> <p>A few tips on item caches:</p> <p>Tip</p> <p>Use <code>ItemCacheOp</code> when you need to access the same item on the same sample multiple times and you want to avoid reading from disk.</p> <p>The following code runs in approximately 19ms on my system. Without item caches, it would take 29ms, roughly 50% longer!</p> <pre><code>def function1(sample: Sample) -&gt; None:\n    print(\"Average Color: \", sample[\"image\"]().mean((0, 1)))\n\n\ndef function2(sample: Sample) -&gt; None:\n    print(\"Color Standard Deviation: \", sample[\"image\"]().std((0, 1)))\n\n\nif __name__ == \"__main__\":\n    path = Path(\"tests/sample_data/underfolders/underfolder_0\")\n\n    dataset = UnderfolderSource(path)()\n    dataset = ItemCacheOp()(dataset)\n\n    for sample in dataset:\n        function1(sample)\n        function2(sample)\n</code></pre> <p>Tip</p> <p>Results of previous operations are always stored inside <code>MemoryItems</code>. It makes no sense to cache those.</p> <p>Tip</p> <p>If you only access an item once for every sample in the dataset, your code won't benefit from using item cache.</p>"},{"location":"tutorial/cache/#dataset-cache","title":"Dataset Cache","text":"<p>Item caches are very simple, inexpensive and relatively safe to use, but they cannot do anything about repeated computations of lazy operations. </p> <p>Let's consider the following example: </p> <p>Example</p> <p>We have a dataset of 26 samples, for each of them we compute statistics about an item named \"image\", like the average color, the standard deviation, the minimum and maximum pixel values. We then repeat the results 100 times to artificially increase the dataset length.</p> <pre><code>class ComputeStatsMapper(Mapper[Sample, Sample]):\n    def __call__(self, idx: int, x: Sample) -&gt; Sample:\n        image: np.ndarray = x[\"image\"]()\n        mean = image.mean((0, 1)).tolist()\n        std = image.std((0, 1)).tolist()\n        min_, max_ = int(image.min()), int(image.max())\n        stats_item = MemoryItem(\n            {\"mean\": mean, \"std\": std, \"min\": min_, \"max\": max_}, YAMLParser()\n        )\n        return x.with_item(\"stats\", stats_item)\n\n\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\n\ndataset = UnderfolderSource(path)()\ndataset = MapOp(ComputeStatsMapper())(dataset)\ndataset = RepeatOp(100)(dataset)\n\nfor sample in dataset: \n    print(sample[\"stats\"]())\n</code></pre> <p>The code above runs (on my system) in about 750 ms. That's a very long time, even considering the slow speed of the Python interpreter. That's because <code>RepeatOp</code> is lazy, and the i-th sample is only computed when the dataset is indexed. In total, the dataset is indexed 26 * 100 = 2600 times, each time computing the image stats from scratch:</p> <ul> <li>User requests sample 0 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0</li> <li>User requests sample 1 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1</li> <li>...</li> <li>User requests sample 26 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0</li> <li>User requests sample 27 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1</li> <li>...</li> </ul> <p>If the program were run eagerly (that's not the case with Pipewine), the stats were computed on the 26 original images, then the results would be repeated 100 times with virtually no cost.</p> <p>To solve this problem we can use <code>CacheOp</code>, a special operator that adds a layer of cache to the result of an operation, combined with <code>MemoCache</code>, a very simple cache that memorizes everything with no bound on the number of cached elements.</p> <pre><code>dataset = UnderfolderSource(path)()\ndataset = MapOp(ComputeStatsMapper())(dataset)\ndataset = CacheOp(MemoCache)(dataset)  # &lt;- Activates dataset caching\ndataset = RepeatOp(100)(dataset)\n\nfor sample in dataset: \n    print(sample[\"stats\"]())\n</code></pre> <p>The code now runs in 45 ms and the stats computation is only performed 26 times:</p> <ul> <li>User requests sample 0 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0 -&gt; cache result 0</li> <li>User requests sample 1 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1 -&gt; cache result 1</li> <li>...</li> <li>User requests sample 26 -&gt; maps to sample 0 of original dataset -&gt; return cached sample 0</li> <li>User requests sample 27 -&gt; maps to sample 1 of original dataset -&gt; return cached sample 1</li> <li>...</li> </ul> <p>As you might have noticed, <code>CacheOp</code> can be parametrized with the type of cache to use, and optionally implementation-specific parameters to further customize the way data is cached. There is no silver bullet here, and the type of cache you should use strongly depends on the order the data is accessed, and if chosen wrongly it can lead to 0% hit rate: the worst case scenario where you pay the price of increased memory utilization, additional overhead, and get nothing in return.</p> <p>Note</p> <p><code>CacheOp</code>, regardless of the policy, always applies item-level caching and inherits all its benefits.</p> <p>This is useful because if you want dataset-level caching, you probably want item-level caching as well, sparing you the effort of always applying two <code>ItemCacheOp</code> after <code>CacheOp</code>.</p> <p>For Pipewine, <code>Cache</code> objects are anything that behaves like this:</p> <pre><code>class Cache[K, V]:\n    def clear(self) -&gt; None: ... # &lt;- Removes everything from the cache.\n    def get(self, key: K) -&gt; V | None: ... # Return the value of key, if any.\n    def put(self, key: K, value: V) -&gt; None: ... # Assign a value to a key.\n</code></pre> <p>Let'see the available cache types, together with their pros and cons.</p>"},{"location":"tutorial/cache/#memocache","title":"MemoCache","text":"<p><code>MemoCache</code> is a basic memoization. It can only grow in size since it never evicts elements and has no upper bound to the number of elements it can keep memorized.</p> <p>Access and insertion are both O(1) and are the fastest among all the cache types implemented in Pipewine.</p> <p>Success</p> <p>Best used to cache the output of long computations where either:</p> <ul> <li>The number of samples in the dataset is small (better if known a-priori).</li> <li>The size of the cached data is relatively small. </li> <li>You don't care about memory, you just want to always guarantee the maximum possible cache hit rate by brute-force.</li> </ul> <p>Failure</p> <p><code>MemoCache</code> is unbounded: it may end up using all the available memory on your system.</p>"},{"location":"tutorial/cache/#rrcache","title":"RRCache","text":"<p><code>RRCache</code> is a bounded cache that follows a stochastic replacement rule. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting a random element.</p> <p>Access and insertion are both O(1). Access is exactly as fast as with <code>MemoCache</code>, insertion includes additional overhead due to the RNG.</p> <p>Success</p> <p>You can use a random replacement cache when the order in which samples are accessed is unknown, but still you want to statistically do better than the worst case scenario (0% hit rate).</p> <p>Failure</p> <p>Random replacement cache is not optimal if:</p> <ul> <li>You need deterministic running time.</li> <li>You know the order in which samples are accessed and you can select a more appropriate cache type.</li> </ul>"},{"location":"tutorial/cache/#fifocache","title":"FIFOCache","text":"<p><code>FIFOCache</code> is a bounded cache that stores element in a queue data structure. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting the element that was least recently inserted in the cache (not to be confused with LRU caches), regardless of how many times they were accessed before.</p> <p>Basically, the first element that is inserted is going to be the first that gets evicted.</p> <p>Access and insertion are both O(1). Access is exactly as fast as with <code>MemoCache</code>, insertion includes additional overhead due the eviction policy implementation.</p> <p>Success</p> <p>Use <code>FIFOCache</code> when an element is likely going to be accessed multiple times shortly after being inserted in the cache.</p> <p>Failure</p> <p>The FIFO eviction policy guarantees a 0% hit rate when the elements are accessed in order from first to last in multiple cycles. </p>"},{"location":"tutorial/cache/#lifocache","title":"LIFOCache","text":"<p><code>LIFOCache</code> is a bounded cache that stores element in a stack data structure. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting the element that was most recently inserted in the cache (not to be confused with MRU caches), regardless of how many times they were accessed before.</p> <p>Basically, the first element that is inserted is going to be the last that gets evicted.</p> <p>Access and insertion are both O(1), same cost as with <code>FIFOCache</code>. </p> <p>Success</p> <p>Use <code>LIFOCache</code> when:</p> <ul> <li>Elements are accessed in order from first to last in multiple cycles.  </li> <li>The first elements to be inserted are likely going to be accessed more frequently.  </li> </ul> <p>Failure</p> <p>The LIFO eviction policy performs terribly when recently inserted elements are likely going to be accessed in subsequent, but not immediately consecutive, calls.</p>"},{"location":"tutorial/cache/#lrucache","title":"LRUCache","text":"<p><code>LRUCache</code> (Least Recently Used Cache) is a bounded cache that whenever an element needs to be added and no space is available, it will forget the element that was least recently accessed. In contrast with the <code>FIFOCache</code>, the eviction policy takes into account the way elements are accessed, making this solution slightly more complex.</p> <p>Access and insertion are both O(1), both include additional overhead due to potential state changes in the underlying data structure. The implementation is a modified version of the one proposed in the Python3.12 standard library that uses a circular doubly linked list + hashmap. </p> <p>Success</p> <p>Use <code>LRUCache</code> when an element that was recently accessed is likely going to be accessed again in the future.</p> <p>Failure</p> <p>The LRU eviction policy guarantees a 0% hit rate when the elements are accessed in order from first to last in multiple cycles, similarly to <code>FIFOCache</code>.</p>"},{"location":"tutorial/cache/#mrucache","title":"MRUCache","text":"<p><code>MRUCache</code> (Most Recently Used Cache) is a bounded cache that whenever an element needs to be added and no space is available, it will forget the element that was most recently accessed. In contrast with the <code>LIFOCache</code>, the eviction policy takes into account the way elements are accessed, making this solution slightly more complex.</p> <p>Access and insertion are both O(1), same costs as <code>LRUCache</code>. </p> <p>Success</p> <p>Use <code>MRUCache</code> when:</p> <ul> <li>Elements are accessed in order from first to last in multiple cycles.  </li> <li>Elements that are recently accessed are likely not going to be accessed again for a long time.</li> </ul> <p>Failure</p> <p>The MRU eviction policy performs terribly when recently inserted elements are likely going to be accessed in subsequent calls.</p>"},{"location":"tutorial/cache/#benchmark","title":"Benchmark","text":"<p>Here is a very naive benchmark of different cache eviction policies compared under different access patterns, under the following conditions:</p> <ul> <li>The dataset consists of 26 elements.</li> <li>The total number of calls is 26000.</li> <li>The maximum cache size is set to 5.</li> </ul> <p>Types of access patterns used:</p> <ul> <li> <p>\"Cyclic\" accesses elements from first to last in multiple cycles. E.g.</p> <p><code>0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 ...</code></p> </li> <li> <p>\"Back and Forth\" accesses elements from first to last in even cycles, and from last to first in odd cycles. E.g. </p> <p><code>0 1 2 3 4 5 6 7 8 9 9 8 7 6 5 4 3 2 1 0 0 1 2 3 ...</code></p> </li> <li> <p>\"Hot Element\" accesses the elements similarly to \"Cyclic\" in odd indexes, but in every even index it accesses the first \"hot\" element:</p> <p><code>0 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 0 0 1 0 2 ...</code></p> </li> <li> <p>\"Blocks\" accesses the elements similarly to \"Cyclic\" in odd indexes, but in chunks of K elements. E.g. with K=4:</p> <p><code>0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 ...</code></p> </li> <li> <p>\"Sliding Window\" accesses the elements in groups of K increasing indexes. E.g. with K=4:</p> <p><code>0 1 2 3 1 2 3 4 2 3 4 5 3 4 5 6 4 5 6 7 5 6 7 8 6 7 8 9 ...</code></p> </li> <li> <p>\"Uniform\" accesses the elements in random order, sampling from a uniform distribution.</p> </li> <li>\"Zipfian\" accesses the elements in random order, sampling from a zipfian distribution.</li> <li>\"Random Walk\" accesses the items in random order, where the i-th index is computed by adding a random shift from the previous one, sampled from a normal distribution with a small positive shift.</li> </ul> <p></p>"},{"location":"tutorial/cache/#checkpoints","title":"Checkpoints","text":"<p>So far we have seen how to mitigate the problem of multiple accesses to the same item using <code>ItemCacheOp</code> cache and how to avoid re-computing the same lazy operation when accessing a dataset with the same index multiple times <code>CacheOp</code>. Pipewine has a final caching mechanism called \"checkpoint\" that can be used when both:</p> <ul> <li>Datasets are not guaranteed to entirely fit into system memory. E.g. using <code>MemoCache</code> would crash your pipeline.</li> <li>Some operations are very slow and the risk of computing them multiple times due to a cache miss is unacceptable. In this case using any policy other than <code>MemoCache</code> would not eliminate this risk entirely.</li> </ul> <p>Checkpoints are a rather stupid but effective idea to deal with this situation: we eagerly write the whole dataset to disk (or to a DB) and then return a lazy dataset that reads from it. </p> <p>This way we use disk space (which we can safely assume to be enough to store the dataset) to cache intermediate results, so that future accesses won't have to re-compute all the lazy operations that were needed to reach that intermediate state.</p> <p>Tip</p> <p>Checkpoints are not a free lunch: you shift the cost from computation to I/O.  As a very approximate rule of thumb, under many simplifying assumptions, checkpoints should be convenient if:</p> <pre><code>(N - 1) * compute_cost &gt; (N + 1) * io_cost\n</code></pre> <p>Where <code>N</code> is the average number of times that the same computation is repeated, <code>compute_cost</code> is the total cost of computing all computations once, <code>io_cost</code> is the total cost of reading or writing the results of all comptuation once. </p> <p>Checkpoints are currently not a class or any software component you can import from Pipewine, they are a functionality of Pipewine Workflows, a higher level concept that was not explained in this tutorial so far. However, you don't need Workflows to use checkpoints, you can simply use a pair of a <code>DatasetSink</code> and <code>DatasetSource</code>.</p> <p>Note</p> <p>When using workflows, placing a checkpoint in the middle of a pipeline is way easier than this: all you need to do is set a boolean flag <code>checkpoint=True</code> where needed, or, if you want to use a checkpoint after every single operation, you can set a flag to enable them by default.</p> <p>Refer to the Workflows section for more info.</p> <p>Example</p> <p>A rather extreme example of checkpoint usage: starting from the \"letters\" toy dataset used in the examples so far, we want to replace the 26 (badly) hand-drawn images with AI-generated ones. Then, we want to replicate the whole dataset 100 times.</p> <p>Yes, we only want to generate a total of 26 images, not 2600, using Gen-AI.</p> <p>Suppose we have access to an API that costs us 1$ per generated image and we wrapped that into a <code>Mapper</code>:</p> <pre><code>class RegenerateImage(Mapper[Sample, Sample]):\n    def __call__(self, idx: int, x: Sample) -&gt; Sample:\n        metadata = x[\"metadata\"]()\n        letter, color = metadata[\"letter\"], metadata[\"color\"]\n        prompt = (\n            f\"Create an artistic representation of the letter {letter} in a visually \"\n            f\"striking style. The letter should be prominently displayed in {color}, \"\n            \"with a background that complements or contrasts it artistically. Use \"\n            \"textures, lighting, and creative elements to make the design unique \"\n            \"and aesthetically appealing.\"\n        )\n        gen_image = extremely_expensive_api_that_costs_1_USD_per_image(prompt)\n        return x.with_value(\"image\", gen_image)\n</code></pre> <p>To (correclty) generate only 26 images, then repeat the whole dataset 100 times, we can use a checkpoint just after applying the <code>RegenerateImage</code> mapper:</p> <pre><code>path = Path(\"tests/sample_data/underfolders/underfolder_0\")\ndataset = UnderfolderSource(path)()\ndataset = MapOp(RegenerateImage())(dataset)\n\n# Checkpoint\nUnderfolderSink(ckpt := Path(\"/tmp/ckpt\"))(dataset)\ndataset = UnderfolderSource(ckpt)()\n\ndataset = RepeatOp(100)(dataset)\n</code></pre> <p>Without the checkpoint, we would instead call the API a total of 2600 times, generating 100x more images than we originally intended and wasting a lot of money.</p>"},{"location":"tutorial/cache/#cache-and-multi-processing","title":"Cache and Multi-processing","text":"<p>Multiprocessing in python does not allow processes to hold references to arbitrary python objects in shared memory. Besides a restricted set of built-in types, all the communication between processes uses the message passing paradigm. For process A to send an object to process B, many expensive things need to happen: - A and B need to hold a pair of connected sockets. - A must serialize the object into bytes. - A must write the bytes into the socket. - B must read the bytes from the socket. - B must de-serialize the bytes into a copy of the original object.</p> <p>As pointed out in the Python docs, using shared state between multiple processes should be avoided as much as possible, for the following reasons:</p> <ul> <li>Synchronization: when processes need to share state, they need mechanisms like locks, semaphores or other synchronization primitives to prevent race conditions and mess everything up.</li> <li>Performance overhead: each time an object is passed from one process to the other, it needs to be serialized and de-serialized, which is generally an expensive operation.</li> <li>Memory overhead: each time an object is passed from one process to the other it is basically as if it were deep-copied, vastly increasing memory usage.</li> <li>Debugging difficulty: debugging race conditions is not fun, they can be very hard to detect and reproduce.</li> <li>Scalability: if many processes depend on shared state, the system might not scale well with increasing number of processes due to contention and synchronization overhead.</li> </ul> <p>To avoid these issues Pipewine caches are not shared between processes, meaning that if process A computes a sample and caches it, the result will only be cached for process A. Later, if process B needs that sample and looks for it in its own cache, it won't find it and will have to compute it.</p> <p>The only exception is if the cache was partially populated in the main process: in this case the cache is cloned and every child process inherits its own copy at the moment of spawning. All state changes that occur later are independent for each child process and are discarded when the processes die. No change is reflected on the original copy of the cache in the main process.</p> <p>We can say that multi-processing makes caching less effective: since processes do not share state and potentially the same sample can be cached multiple times in more than one process we use memory less efficiently. With a pool of N processes we can either have the same hit rate as with a single process by using N times the amount of memory, or we can keep the memory fixed but accomplish a much smaller hit rate.</p> <p>Checkpoints in contrast, assuming that N python processes are not enough to saturate your read/write bandwith, do not suffer this problem: each sample is written exactly once independently from the others without need for synchronization.</p>"},{"location":"tutorial/cache/#comparison-with-pipelime","title":"Comparison with Pipelime","text":"<p>Comparison of cache-related features in Pipelime and Pipewine:</p> Feature Pipelime (OLD) Pipewine (NEW) Item Cache Enabled by default, can be disabled with <code>@no_data_cache()</code> decorator. Disabled by default, can be enabled by either adding a <code>ItemCacheOp</code> or <code>CacheMapper</code> where necessary. Dataset Cache Depends on how the <code>SourceSequence</code> or <code>PipedSequence</code> steps of your pipeline are implemented: some of them construct samples when the <code>__getitem__</code> is called, others construct them upfront and hold references to them. In the former case, no dataset caching is done. In the latter case, if item caching is not disabled, everything will be memorized as the pipeline progresses. In some cases, it is necessary to turn off caching completely to avoid running out of memory. Disabled by default, can be enabled by using <code>CacheOp</code> with the desired eviction policy. Supports many cache eviction policies to bound the number of cached samples to a safe amount and avoid out of memory issues. Checkpoints Every <code>PipelimeCommand</code> reads inputs and writes outputs from/to external sources, essentially making checkpoints enabled by default for every <code>PipelimeCommand</code>. The command also specifies where intermediate steps are written and in which format. Any change in either location and format requires you to apply changes to the command that writes the intermediate result and to every command that reads them. Checkpoints are disabled by default but can be enabled by adding a pair of sink and source between any two steps of the pipeline. The individual steps of the pipeline do not know anything about checkpoints, requiring you to apply no change to your operators. When using workflows Pipewine can be configured to automatically inject a pair of source and sink between any two steps of the pipeline. Cache and Multi-processing No sharing is performed, child processes inherit a copy of the parent process cache. Cache state is lost on process exit. Same as Pipelime."},{"location":"tutorial/cli/","title":"\ud83d\udda5\ufe0f CLI","text":"<p>Pipewine provides a command-line interface (CLI) that allows you to perform various operations on your data without writing any code. The CLI is built using Typer, a library that makes it easy to create command-line interfaces in Python.</p>"},{"location":"tutorial/cli/#usage","title":"Usage","text":"<p>To use the CLI, all you need to do is install Pipewine and run the <code>pipewine</code> command in your terminal. This will display a list of available commands and options.</p> <pre><code>pipewine\n</code></pre> <p>Equivalently, you can run the command with the <code>--help</code> flag to display the same information.</p> <pre><code>pipewine --help\n</code></pre> <p>The <code>--help</code> flag can also be used with any command to display information about that specific command.</p> <pre><code>pipewine command sub-command --help\n</code></pre> <p>To display the version of the current Pipewine installation, you can use the <code>--version</code> flag.</p> <pre><code>pipewine --version\n</code></pre>"},{"location":"tutorial/cli/#mappers-and-operators","title":"Mappers and Operators","text":"<p>To apply an operator to your data, you can use the <code>op</code> command followed by the operator name and the required arguments. You can get a full list of available operators using the <code>--help</code> flag with the <code>op</code> command.</p> <pre><code>pipewine op --help\n</code></pre> <p>Similarly, for mapper commands, you can use the <code>map</code> command followed by the mapper name and the required arguments. You can get a full list of available mappers using the <code>--help</code> flag with the <code>map</code> command.</p> <pre><code>pipewine map --help\n</code></pre> <p>Options differ between each operator, so you should use the <code>--help</code> flag with the operator name to get more information about the required arguments. E.g., to get more information about the <code>repeat</code> operator, you can run the following command:</p> <pre><code>pipewine op repeat --help\npipewine map hash --help\n</code></pre> <p>Example</p> <p>Suppose you want to repeat a dataset 10 times, you can do so using the <code>repeat</code> operator. The following command will repeat the dataset <code>my_dataset</code> 10 times and save the result to <code>my_repeated_dataset</code>:</p> <pre><code>pipewine op repeat -i my_dataset -o my_repeated_dataset --times 10\n</code></pre> <p>Success</p> <p>When you successfully run a command, you will see a message indicating that the operation was successful.</p> <pre><code>\u256d\u2500 Workflow Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Workflow completed successfully. \u2502\n\u2502 Started:  2025-02-22 11:39:38    \u2502\n\u2502 Finished: 2025-02-22 11:39:38    \u2502\n\u2502 Total duration: 0:00:00.207026   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Failure</p> <p>If an error occurs during the execution of a command, you will see an error message indicating what went wrong.</p> <pre><code>\u256d\u2500 Workflow Status \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 Workflow failed.                                                                                                  \u2502\n\u2502                                                                                                                   \u2502\n\u2502 Traceback (most recent call last):                                                                                \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/cli/utils.py\", line 168, in run_cli_workflow                           \u2502\n\u2502     run_workflow(workflow, tracker=CursesTracker() if tui else None)                                              \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/workflows/__init__.py\", line 73, in run_workflow                       \u2502\n\u2502     raise e                                                                                                       \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/workflows/__init__.py\", line 70, in run_workflow                       \u2502\n\u2502     executor.execute(workflow)                                                                                    \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/workflows/execution.py\", line 273, in execute                          \u2502\n\u2502     self._execute_node(workflow, node, state, id_.hex, wf_opts)                                                   \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/workflows/execution.py\", line 167, in _execute_node                    \u2502\n\u2502     output = action(input_)                                                                                       \u2502\n\u2502              ^^^^^^^^^^^^^^                                                                                       \u2502\n\u2502   File \"/home/luca/repos/pipewine/pipewine/sinks/underfolder.py\", line 129, in __call__                           \u2502\n\u2502     raise FileExistsError(                                                                                        \u2502\n\u2502 FileExistsError: Folder /tmp/output already exists and policy OverwritePolicy.FORBID is used. Either change the   \u2502\n\u2502 destination path or set a weaker policy.                                                                          \u2502\n\u2502                                                                                                                   \u2502\n\u2502 Started:  2025-02-22 11:40:42                                                                                     \u2502\n\u2502 Finished: 2025-02-22 11:40:42                                                                                     \u2502\n\u2502 Total duration: 0:00:00.104007                                                                                    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Both the <code>op</code> and <code>map</code> commands support changing the input and output formats using the <code>--input-format</code> (<code>-I</code>) and <code>--output-format</code> (<code>-O</code>) options. You can print the available input and output formats using the <code>--format-help</code> flag.</p> <p>For convenience, this flag will also print all the recognized file extensions that can be parsed by Pipewine items. </p> <pre><code>pipewine op --format-help\npipewine map --format-help\n</code></pre> <p>By default, the input and output formats are set to <code>underfolder</code>. When using an underfolder as an output format, you can optionally specify, alongside the output path, the overwrite policy and copy policy to use. </p> <pre><code>PATH[,OVERWRITE_POLICY[,COPY_POLICY]]\n</code></pre> <p>By default, the overwrite policy is set to <code>forbid</code> and the copy policy is set to <code>hard_link</code>.</p> <p>Example</p> <p>Example, suppose we want to repeat a dataset 10 times and save the result to an underfolder named <code>my_repeated_dataset</code> that may already exist. We can do so using the following command:</p> <pre><code>pipewine op repeat -i my_dataset -o my_repeated_dataset,overwrite --times 10\n</code></pre> <p>Suppose we also want to change the copy policy and instead of hard linking the files, we want a full copy. We can do so using the following command:</p> <pre><code>pipewine op repeat -i my_dataset -o my_repeated_dataset,overwrite,replicate --times 10\n</code></pre> <p>You can also use a custom <code>Grabber</code> to run the command using multi-processing parallelism. To do so, you can use the <code>--grabber</code> (<code>-g</code>) option followed by the number of processes to use and optionally the chunk size. </p> <p>Example</p> <pre><code>pipewine op -g 8 repeat -i my_dataset -o my_repeated_dataset --times 100\npipewine op -g 8,50 repeat -i my_dataset -o my_repeated_dataset --times 100\n</code></pre> <p>By default, commands report progress in a TUI interface. You can disable this behavior by using the <code>--no-tui</code> flag.</p> <p>Example</p> <pre><code>pipewine op --no-tui repeat -i my_dataset -o my_repeated_dataset --times 10\n</code></pre>"},{"location":"tutorial/cli/#workflows","title":"Workflows","text":"<p>Pipewine also supports running workflows from the CLI. You can use the <code>wf</code> command followed by the workflow name and the required arguments. You can get a full list of available workflows using the <code>--help</code> flag with the <code>wf</code> command.</p> <pre><code>pipewine wf --help\n</code></pre> <p>Workflows can optionally be drawn using the <code>--draw [path]</code> option. This will disable the workflow execution and instead draw the workflow to the specified path. </p>"},{"location":"tutorial/cli/#extension","title":"Extension","text":"<p>Pipewine CLI is designed to be easily extensible, similarly to the old Pipelime CLI, by specifying a list of custom modules to load dynamically. These modules can be loaded using the <code>--module</code> (<code>-m</code>) option followed by the module name. </p> <p>Extension modules can be provided as a path to a Python file or a module path. </p> <p>Warning</p> <p>When loading an extension module from a python file, multiprocessing is disabled due to a limitation of the <code>multiprocessing</code> module.</p> <p>This issue was already present in the old Pipelime package and is due to the fact that the <code>multiprocessing</code> module fails to correctly modules that are imported dynamically from a single file, without a parent package.</p> <p>Multiprocessing should work correctly when loading modules from a package.</p>"},{"location":"tutorial/cli/#adding-custom-operators","title":"Adding custom operators","text":"<p>To allow operators defined in python packages (or scripts) other than the <code>pipewine</code> package, all you need to do is create a function that creates the operator object decorate it with the <code>op_cli</code> decorator.</p> <p>The decorated function is parsed by <code>Typer</code> and its signature is used to generate the CLI interface. You can use any type hints supported by <code>Typer</code> to specify the operator arguments, if any.</p> <p>Do not worry about the input and output parameters, as <code>Pipewine</code> will automatically add them to the function signature dynamically, based on the type of operator returned by your function.</p> <p>If, for some reason, you want to give a custom name to the command, you can use the <code>name</code> argument of the <code>op_cli</code> decorator.</p> <p>Example</p> <p>This is how the <code>repeat</code> operator is defined in the <code>pipewine</code> package:</p> <pre><code>@op_cli()\ndef repeat(\n    times: Annotated[int, Option(..., \"--times\", \"-t\")],\n    interleave: Annotated[bool, Option(..., \"--interleave\", \"-I\")] = False,\n) -&gt; RepeatOp:\n    \"\"\"Repeat a dataset N times replicating the samples.\"\"\"\n    return RepeatOp(times, interleave=interleave)\n</code></pre> <p>Equivalently, if you don't like the <code>Annotated</code> syntax you can use the old (deprecated) <code>Option</code> default value syntax:</p> <pre><code>@op_cli()\ndef repeat(\n    times: int = Option(..., \"--times\", \"-t\"),\n    interleave: bool = Option(False, \"--interleave\", \"-I\"),\n) -&gt; RepeatOp:\n    \"\"\"Repeat a dataset N times replicating the samples.\"\"\"\n    return RepeatOp(times, interleave=interleave)\n</code></pre> <p>If you are rapid-protyping and don't want to write the help strings, you can avoid using <code>Option</code> altogether and just use the default values:</p> <pre><code>@op_cli()\ndef repeat(times: int, interleave: bool = False) -&gt; RepeatOp:\n    return RepeatOp(times, interleave=interleave)\n</code></pre> <p>Note</p> <p>It is crucial that the function return type is correctly annotated with the type of operator that it returns. This is used by <code>Pipewine</code> to correctly parse the operator and its arguments, and to generate the correct CLI interface.</p> <p>If the command has some eager behavior that can be parallelized using a <code>Grabber</code>, you can add a <code>grabber: Grabber</code> positional argument to the function signature. Pipewine will automatically recognize this argument and pass a <code>Grabber</code> object to the function when it is called.</p> <p>Example</p> <p>This is how the <code>sort</code> operator, which has some eager computation that can be parallelized, is defined in the <code>pipewine</code> package:</p> <pre><code>@op_cli()\ndef sort(\n    grabber: Grabber,\n    key: Annotated[str, Option(..., \"--key\", \"-k\")],\n    reverse: Annotated[bool, Option(..., \"--reverse\", \"-r\")] = False,\n) -&gt; SortOp:\n    ... # omitted for brevity\n</code></pre>"},{"location":"tutorial/cli/#adding-custom-mappers","title":"Adding custom mappers","text":"<p>Adding custom mappers is very similar to adding custom operators. The only difference is that you need to use the <code>map_cli</code> decorator instead of the <code>op_cli</code> decorator, and of course, the function should return a <code>Mapper</code> object instead of an <code>Operator</code> object.</p> <p>Everything else is the same, including the possibility to specify a custom name for the command using the <code>name</code> argument of the <code>map_cli</code> decorator.</p> <p>Example</p> <p>This is how the <code>filter-keys</code> mapper is defined in the <code>pipewine</code> package:</p> <pre><code>@map_cli()\ndef filter_keys(\n    keys: Annotated[list[str], Option(..., \"-k\", \"--keys\")],\n    negate: Annotated[bool, Option(..., \"-n\", \"--negate\")] = False,\n) -&gt; FilterKeysMapper:\n    \"\"\"Keep only or remove a subset of items.\"\"\"\n    return FilterKeysMapper(keys, negate=negate)\n</code></pre>"},{"location":"tutorial/cli/#adding-custom-data-formats","title":"Adding custom data formats","text":"<p>Currently, the Pipewine CLI only supports the <code>underfolder</code> format for both input and output. However, you can easily register custom <code>DatasetSource</code> and <code>DatasetSink</code> classes to support additional data formats.</p> <p>This can be done using the <code>source_cli</code> and <code>sink_cli</code> decorators, respectively. </p> <p>To add a custom input format, you can use the <code>source_cli</code> decorator on a function that  accepts:</p> <ul> <li>A <code>str</code> argument containing some text that must be parsed into a <code>DatasetSource</code> object.</li> <li>A <code>Grabber</code> argument that may be used by the dataset source, in case it requires to do some eager computation that can run in parallel.</li> <li>A <code>type[Sample]</code> argument with the concrete type of the samples that the dataset source will return. </li> </ul> <p>To add a custom output format, you can use the <code>sink_cli</code> decorator on a function that accepts:</p> <ul> <li>A <code>str</code> argument containing some text that must be parsed into a <code>DatasetSink</code> object.</li> <li>A <code>Grabber</code> argument that may be used by the dataset sink, in case it can parallelize the writing of the dataset.</li> </ul> <p>(The sample type is not needed for the output format, as the dataset sink can simply infer it from the samples that it receives.)</p> <p>You can use the docstring of the function to specify the help message that will be displayed with the <code>--format-help</code> flag.</p> <p>You can also specify a custom name for the input/output using the <code>name</code> argument of the <code>source_cli</code> or <code>sink_cli</code> decorator.</p> <p>Note</p> <p>Input and output formats are not Typer commands since they cannot be called directly from the CLI. Instead, they are only used to parse the input/output text according to the format specified by the user.</p> <p>Hence, the function signature should not include typer-specific annotations, such as <code>Option</code> or <code>Argument</code>.</p> <p>Example</p> <p>This is how the <code>underfolder</code> format is registered to the CLI:</p> <pre><code>@source_cli()\ndef underfolder(\n    text: str, grabber: Grabber, sample_type: type[Sample]\n) -&gt; UnderfolderSource:\n    \"\"\"PATH: Path to the dataset folder.\"\"\"\n    return UnderfolderSource(Path(text), sample_type=sample_type)\n\n@sink_cli()\ndef underfolder(text: str, grabber: Grabber) -&gt; UnderfolderSink:\n    \"\"\"PATH[,OVERWRITE=forbid[,COPY_POLICY=hard_link]]\"\"\"\n    path, ow_policy, copy_policy = _split_and_parse_underfolder_text(text)\n    return UnderfolderSink(\n        Path(path), grabber=grabber, overwrite_policy=ow_policy, copy_policy=copy_policy\n    )\n</code></pre>"},{"location":"tutorial/cli/#adding-custom-workflows","title":"Adding custom workflows","text":"<p>Adding custom workflows is very similar to adding custom operators and mappers, with the following differences:</p> <ul> <li>You need to use the <code>wf_cli</code> decorator instead.</li> <li>The function must return a <code>Workflow</code> object.</li> <li>Pipewine will not automatically add the input and output parameters to the function, so you need to specify them explicitly in the function signature.</li> </ul> <p>Example</p> <p>Example workflow:</p> <pre><code>@wf_cli(name=\"example\")\ndef example(\n    input: Annotated[Path, Option(..., \"-i\", \"--input\", help=\"Input folder.\")],\n    output: Annotated[Path, Option(..., \"-o\", \"--output\", help=\"Output folder.\")],\n    repeat_n: Annotated[\n        int, Option(..., \"-r\", \"--repeat\", help=\"Repeat n times.\")\n    ] = 100,\n    workers: Annotated[int, Option(..., \"-w\", \"--workers\", help=\"Num workers.\")] = 0,\n) -&gt; Workflow:\n    grabber = Grabber(workers, 50)\n    wf = Workflow(WfOptions(checkpoint_grabber=grabber))\n    data = wf.node(UnderfolderSource(input, sample_type=LetterSample))()\n    data = wf.node(RepeatOp(repeat_n))(data)\n    data = wf.node(MapOp(ColorJitter()), options=WfOptions(checkpoint=True))(data)\n    groups = wf.node(GroupByOp(group_fn, grabber=None))(data)\n    vowels = groups[\"vowel\"]\n    consonants = groups[\"consonant\"]\n    vowels = wf.node(SortOp(sort_fn, grabber=None))(vowels)\n    consonants = wf.node(SortOp(sort_fn, grabber=None))(consonants)\n    data = wf.node(CatOp())([vowels, consonants])\n    wf.node(UnderfolderSink(output, grabber=grabber))(data)\n    return wf\n</code></pre>"},{"location":"tutorial/data/","title":"\ud83d\uddc3\ufe0f Data Model","text":""},{"location":"tutorial/data/#overview","title":"Overview","text":"<p>In this section your will learn what are the main data abstractions upon which Pipewine is built, how to interact with them and extend them according to your needs.</p> <p>Pipewine data model is composed of three main abstractions: </p> <ul> <li>Dataset - A Sequence of <code>Sample</code> instances, where \"sequence\" means an ordered collection that supports indexing, slicing and iteration.</li> <li>Sample - A Mapping of strings to <code>Item</code> instances, where \"mapping\" means a set of key-value pairs that supports indexing and iteration. </li> <li>Item - An object that has access to the underlying data unit. E.g. images, text, structured metadata, numpy arrays, and whatever serializable object you may want to include in your dataset.</li> </ul> <p>Plus, some lower level components that are detailed later on. You can disregard them for now:</p> <ul> <li>Parser - Defines how an item should encode/decode the associated data.</li> <li>Reader - Defines how an item should access data stored elsewhere.</li> </ul>"},{"location":"tutorial/data/#dataset","title":"Dataset","text":"<p><code>Dataset</code> is the highest-level container and manages the following information:</p> <ul> <li>How many samples it contains</li> <li>In which order</li> </ul> <p>It provides methods to access individual samples or slices of datasets, as in Python slice.</p> <p>Note</p> <p>A <code>Dataset</code> is an immutable Python Sequence, supporting all its methods.</p> <p>All <code>Dataset</code> objects are Generics, meaning that they can be hinted with information about the type of samples they contain. This is especially useful if you are using a static type checker.</p> <p>Example</p> <p>Example usage of a <code>Dataset</code> object:</p> <pre><code># Given a Dataset of MySample's \ndataset: Dataset[MySample]\n\n# Supports len\nnumber_of_samples = len(dataset)\n\n# Supports indexing\nsample_0 = dataset[0]    # The type checker infers the type: MySample\nsample_51 = dataset[51]  # The type checker infers the type: MySample\n\n# Suppors slicing\nsub_dataset = dataset[10:20] # The type checker infers the type: Dataset[MySample]\n\n# Supports iteration\nfor sample in dataset:\n    ...\n</code></pre> <p>By default Pipewine provides two implementations of the <code>Dataset</code> interface: </p> <ul> <li><code>ListDataset</code></li> <li><code>LazyDataset</code> </li> </ul>"},{"location":"tutorial/data/#listdataset","title":"ListDataset","text":"<p>A <code>ListDataset</code> is basically a wrapper around a Python <code>list</code>, such that, whenever indexed, the result is immediately available. </p> <p>To achieve this, it has two fundamental requirements:</p> <ol> <li>All samples must be known at creation time.</li> <li>All samples must be always loaded into memory.</li> </ol> <p>Due to these limitations, it's rarely used in the built-in operations, since the lazy alternative <code>LazyDataset</code> combined with caching provides a better trade-off, but it may be handy to have when:</p> <ul> <li>The number of samples is small.</li> <li>Samples are lightweight (i.e. no images, 3d data, huge tensors etc...)</li> </ul> <p>Example</p> <p>Example of how to construct a <code>ListDataset</code>:</p> <pre><code># Create a list of samples\nsamples = [ ... ] \n\n# Wrap it in a ListDataset\ndataset = ListDataset(samples)\n</code></pre> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(N) (including the construction of the list)</li> <li>Length - O(1)</li> <li>Indexing - O(1)</li> <li>Slicing - O(N)</li> </ul>"},{"location":"tutorial/data/#lazydataset","title":"LazyDataset","text":"<p>The smarter alternative is <code>LazyDataset</code>, a type of <code>Dataset</code> that defers the computation of the samples as late as possible. That's right, when using a <code>LazyDataset</code> samples are created when it is indexed, using a user-defined function that is passed at creation time.  </p> <p>This has some implications:</p> <ul> <li>Samples are not required to be known at creation time, meaning that you can create a <code>LazyDataset</code> in virually zero time.</li> <li>Samples are not required to be kept loaded into memory the whole time, meaning that the memory required by <code>LazyDataset</code> is constant.</li> <li>Constant-time slicing.</li> <li>The computatonal cost shifts to the indexing part, which now carries the burden of creating and returning samples. </li> </ul> <p>Example</p> <p>Let's see an example of how to create and use a <code>LazyDataset</code>: </p> <pre><code># Define a function that creates samples from an integer index.\ndef get_sample_fn(idx: int) -&gt; Sample:\n    print(f\"Called with index: {idx}\")\n\n    sample = ... # Omitted \n    return sample\n\n# Create a LazyDataset of length 10\ndataset = LazyDataset(10, get_sample_fn)\n\n# Do some indexing\nsample_0 = dataset[0] # Prints 'Called with index: 0'\nsample_1 = dataset[1] # Prints 'Called with index: 1'\nsample_2 = dataset[2] # Prints 'Called with index: 2'\n\n# Indexing the same sample multiple times calls the function multiple times\nsample_1 = dataset[1] # Prints 'Called with index: 1'\n</code></pre> <p>Warning</p> <p>What if my function is very expensive to compute? Is <code>LazyDataset</code> going to call it every time the dataset is indexed?</p> <p>Yes, but that can be avoided by using Caches, which are not managed by the <code>LazyDataset</code> class.</p> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(1)</li> <li>Length - O(1)</li> <li>Indexing - Depends on <code>get_sample_fn</code> and <code>index_fn</code>.</li> <li>Slicing - O(1)</li> </ul>"},{"location":"tutorial/data/#sample","title":"Sample","text":"<p><code>Sample</code> is a mapping-like container of <code>Item</code> objects. If dataset were tables (as in a SQL database), samples would be individual rows. Contrary to samples in a dataset, items in a sample do not have any ordering relationship and instead of being indexed with an integer, they are indexed by key.</p> <p>Note</p> <p>A <code>Sample</code> is an immutable Python Mapping, supporting all its methods.</p> <p>Example</p> <p>Let's see an example on how to use a <code>Sample</code> object as a python mapping:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Get the number of items inside the sample\nnumber_of_items = len(sample) \n\n# Retrieve an item named \"image\".\n# This does not return the actual image, but merely an Item that has access to it.\n# This will be explained in detail later.\nitem_image = sample[\"image\"] \n\n# Retrieve an item named \"metadata\"\nitem_metadata = sample[\"metadata\"]\n\n# Iterate on all keys\nfor key in sample.keys():\n    ...\n\n# Iterate on all items\nfor item in sample.values():\n    ...\n\n# Iterate on all key-item pairs\nfor key, item in sample.items():\n    ...\n</code></pre> <p>In addition to all <code>Mapping</code> methods, <code>Sample</code> provides a set of utility methods to create modified copies (samples are immutable) where new items are added, removed or have their content replaced by new values.</p> <p>Example</p> <p>Example showing how to manipulate <code>Sample</code> objects using utility methods:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Add/Replace the item named \"image\" with another item\nnew_sample = sample.with_item(\"image\", new_image_item)\n\n# Add/Replace multiple items at once\nnew_sample = sample.with_items(image=new_image_item, metadata=new_metadata_item)\n\n# Replace the contents of the item named \"image\" with new data\nnew_sample = sample.with_value(\"image\", np.array([[[...]]]))\n\n# Replace the contents of multiple items at once\nnew_sample = sample.with_values(image=np.array([[[...]]]), metadata={\"foo\": 42})\n\n# Remove one or more items\nnew_sample = sample.without(\"image\")\nnew_sample = sample.without(\"image\", \"metadata\") \n\n# Remove everything but one or more items\nnew_sample = sample.with_only(\"image\")\nnew_sample = sample.with_only(\"image\", \"metadata\")\n\n# Rename items\nnew_sample = sample.remap({\"image\": \"THE_IMAGE\", \"metadata\": \"THE_METADATA\"})\n</code></pre> <p>In contrast with <code>Datasets</code>, pipewine does not offer a lazy version of samples, meaning that the all items are always kept memorized. Usually, you want to keep the number of items per sample bound to a constant number.</p> <p>Pipewine provides two main <code>Sample</code> implementations that differ in the way they handle typing information.</p>"},{"location":"tutorial/data/#typelesssample","title":"TypelessSample","text":"<p>The most basic type of <code>Sample</code> is <code>TypelessSample</code>, akin to the old Pipelime <code>Sample</code>.  This class is basically a wrapper around a dictionary of items of unknown type.</p> <p>When using <code>TypelessSample</code> it's your responsibility to know what is the type of each item, meaning that if you access an item you then have to cast it to the expected type.</p> <p>With the old Pipelime, this quickly became a problem and lead to the creation of <code>Entity</code> and <code>Action</code> classes, that provide a type-safe alternative, but unfortunately integrate poorly with the rest of the library, failing to completely remove the need for casts or <code>type: ignore</code> directives. </p> <p>Example</p> <p>The type-checker fails to infer the type of the retrieved item: <pre><code>sample = TypelessSample(**dictionary_of_items)\n\n# When accessing the \"image\" item, the type checker cannot possibly know that the \n# item named \"image\" is an item that accesses an image represented by a numpy array.\nimage_item = sample[\"image\"]\n\n# Thus the need for casting (type-unsafe)\nimage_data = cast(np.ndarray, image_item())\n</code></pre></p> <p>Despite this limitation, <code>TypelessSample</code> allow you to use Pipewine in a quick-and-dirty way that allows for faster experimentation without worrying about type-safety.</p> <p>Example</p> <p>At any moment, you can convert any <code>Sample</code> into a <code>TypelessSample</code>, dropping all typing information by calling the <code>typeless</code> method:</p> <pre><code>sample: MySample\n\n# Construct a typeless copy of the sample\ntl_sample = sample.typeless()\n</code></pre>"},{"location":"tutorial/data/#typedsample","title":"TypedSample","text":"<p><code>TypedSample</code> is the type-safe alternative for samples. It allows you to construct samples that retain information on the type of each item contained within them, making your static type-checker happy.</p> <p><code>TypedSample</code> on its own does not do anything, to use it you always need to define a class that defines the names and the type of the items. This process is very similar to the definition of a Python dataclass, with minimal boilerplate.</p> <p>What you get in return:</p> <ul> <li>No need for <code>t.cast</code> or <code>type: ignore</code> directives that make your code cluttered and error-prone.</li> <li>The type-checker will complain when something is wrong with the way you use your <code>TypedSample</code>, effectively preventing many potential bugs.</li> <li>Intellisense automatically suggests field and method names for auto-completion.</li> <li>Want to rename an item? Any modern IDE is able to quickly rename all occurrences of a <code>TypedSample</code> field without breaking anything.</li> </ul> <p>Example</p> <p>Example creation and usage of a custom <code>TypedSample</code>:</p> <pre><code>class MySample(TypedSample):\n    image_left: Item[np.ndarray]\n    image_right: Item[np.ndarray]\n    category: Item[str]\n\nmy_sample = MySample(\n    image_left=image_left_item,\n    image_right=image_right_item,\n    category=category_item,\n)\n\nimage_left_item = my_sample.image_left # Type-checker infers type Item[np.ndarray]\nimage_left_item = my_sample[\"image_left\"] # Equivalent type-unsafe\n\nimage_right_item = my_sample.image_right # Type-checker infers type Item[np.ndarray]\nimage_right_item = my_sample[\"image_right\"] # Equivalent type-unsafe\n\ncategory_item = my_sample.category # Type-checker infers type Item[str]\ncategory_item = my_sample[\"category\"] # Equivalent type-unsafe\n</code></pre> <p>Warning</p> <p>Beware of naming conflicts when using <code>TypedSample</code>. You should avoid item names conflicting with the methods of the <code>Sample</code> class.  </p>"},{"location":"tutorial/data/#item","title":"Item","text":"<p><code>Item</code> objects represent a single serializable unit of data. They are not the data itself, instead, they only have access to the underlying data.</p> <p>Items do not implement any specific Python abstract type, since they are at the lowest level of the hierarchy and do not need to manage any collection of objects.</p> <p>All items can be provided with typing information about the type of the data they have access to. This enables the type-checker to automatically infer the type of the data when accessed. </p> <p>All <code>Item</code> objects have a <code>Parser</code> inside of them, an object that is responsible to encode/decode the data when reading or writing. These <code>Parser</code> objects are detailed later on.</p> <p>Furthermore, items can be flagged as \"shared\", enabling Pipelime to perform some optimizations when reading/writing them, but essentially leaving their behavior unchanged.</p> <p>Example</p> <p>Example usage of an <code>Item</code>:</p> <pre><code># Given an item that accesses a string\nitem: Item[str]\n\n# Get the actual data by calling the item ()\nactual_data = item()\n\n# Create a copy of the item with data replaced by something else\nnew_item = item.with_value(\"new_string\")\n\n# Get the parser of the item\nparser = item.parser\n\n# Create a copy of the item with another parser\nnew_item = item.with_parser(new_parser)\n\n# Get the sharedness of the the item\nis_shared = item.is_shared\n\n# Set the item as shared/unshared\nnew_item = item.with_sharedness(True)\n</code></pre> <p>Pipewine provides three <code>Item</code> variants, that differ in the way data is accessed or stored.</p>"},{"location":"tutorial/data/#memoryitem","title":"MemoryItem","text":"<p><code>MemoryItem</code> instances are items that directly contain data they are associated with. Accessing data is immediate as it is always loaded in memory and ready to be returned.</p> <p>Tip</p> <p>Use <code>MemoryItem</code> to contain \"new\" data that is the result of a computation. E.g. the output of a complex DL model.</p> <p>Example</p> <p>To create a <code>MemoryItem</code>, you just need to pass the data as-is and the <code>Parser</code> object:</p> <pre><code># Given a numpy array representing an image that is the output of an expensive\n# computation\nimage_data = np.array([[[...]]])\n\n# Create a MemoryItem that contains the data and explicitly tells Pipewine to always\n# encode the data as a JPEG image.\nmy_item = MemoryItem(image_data, JpegParser())\n</code></pre>"},{"location":"tutorial/data/#storeditem","title":"StoredItem","text":"<p><code>StoredItem</code> instances are items that point to external data stored elsewhere. Upon calling the item, the data is read from the storage, parsed and returned. </p> <p><code>StoredItem</code> objects use both <code>Parser</code> and <code>Reader</code> objects to retrieve the data. A <code>Reader</code> is an object that exposes a <code>read</code> method that returns data as bytes.</p> <p>Currently Pipewine provides a <code>Reader</code> for locally available files called <code>LocalFileReader</code>, that essentially all it does is <code>open(path, \"rb\").read()</code>.</p> <p>Tip</p> <p>Use <code>StoredItem</code> to contain data that is yet to be loaded. E.g. when creating a dataset that reads from a DB, do not perform all the loading upfront, use <code>StoredItem</code> to lazily load the data only when requested.</p> <p>Example</p> <p>To create a <code>StoredItem</code>, you need to </p> <pre><code># The reader object responsible for reading the data as bytes\nreader = LocalFileReader(Path(\"/my/file.png\"))\n\n# Create a StoredItem that is able able to read and parse the data when requested.\nmy_item = StoredItem(reader, PngParser())\n</code></pre> <p>Warning</p> <p>Contrary to old Pipelime items, <code>StoredItem</code> do not offer any kind of automatic caching mechanism: if you retrieve the data multiple times, you will perform a full read each time. </p> <p>To counteract this, you need to use Pipewine cache operations. </p>"},{"location":"tutorial/data/#cacheditem","title":"CachedItem","text":"<p><code>CachedItem</code> objects are items that offer a caching mechanism to avoid calling expensive read operations multiple times when the underlying data is left unchanged. </p> <p>To create a <code>CachedItem</code>, you just need to pass an <code>Item</code> of your choice to the <code>CachedItem</code> constructor.</p> <p>Example</p> <p>Example usage of a <code>CachedItem</code>:</p> <pre><code># Suppose we have an item that reads a high resolution BMP image from an old HDD. \nreader = LocalFileReader(Path(\"/extremely/large/file.bmp\"))\nitem = StoredItem(reader, BmpParser())\n\n# Reading data takes ages, and does not get faster if done multiple times.\ndata1 = item() # Slow\ndata2 = item() # Slow\ndata3 = item() # Slow\n\n# With CachedItem, we can memoize the data after the first access, making subsequent\n# accesses immediate\ncached_item = CachedItem(item)\n\ndata1 = cached_item() # Slow\ndata2 = cached_item() # Fast\ndata3 = cached_item() # Fast\n</code></pre>"},{"location":"tutorial/data/#parser","title":"Parser","text":"<p>Pipewine <code>Parser</code> objects are responsible for implementing the serialization/deserialization functions for data:</p> <ul> <li><code>parse</code> transforms bytes into python objects of your choice.</li> <li><code>dump</code> transforms python objects into bytes.</li> </ul>"},{"location":"tutorial/data/#built-in-parsers","title":"Built-in Parsers","text":"<p>Pipewine has some built-in parsers for commonly used data encodings: </p> <ul> <li> <p><code>PickleParser</code>: de/serializes data using Pickle, a binary protocol that can be used to de/serialize most Python objects. Key pros/cons:</p> <ul> <li>\u2705 <code>pickle</code> can efficiently serialize pretty much any python object.</li> <li>\u274c <code>pickle</code> is not secure: you can end up executing malicious code when reading data. </li> <li>\u274c <code>pickle</code> only works with Python, preventing interoperability with other systems.</li> <li>\u274c There are no guarantees that <code>pickle</code> data written today can be correctly read by future python interpreters.  </li> </ul> </li> <li> <p><code>JSONParser</code> and <code>YAMLParser</code> de/serializes data using JSON or YAML, two popular human-readable data serialization languages that support tree-like structures of data that strongly resemble Python builtin types.</p> <ul> <li>\u2705 Both JSON and YAML are interoperable with many existing systems.</li> <li>\u2705 Both JSON and YAML are standard formats that guarantee backward compatibility.</li> <li>\u26a0\ufe0f JSON and YAML only support a limited set of types such as <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>dict</code>, <code>list</code>. </li> <li>\u2705 <code>JSONParser</code> and <code>YAMLParser</code> interoperate with pydantic <code>BaseModel</code> objects, automatically calling pydantic parsing, validation and dumping when reding/writing. </li> <li>\u274c Both JSON and YAML trade efficiency off for human readability. You may want to use different formats when dealing with large data that you don't care to manually read.</li> </ul> </li> <li> <p><code>NumpyNpyParser</code> de/serializes numpy arrays into binary files. </p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type</li> <li>\u274c Only works with Python and Numpy.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> </ul> </li> <li> <p><code>TiffParser</code> de/serializes numpy arrays into TIFF files.</p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type.</li> <li>\u2705 Produces files that can be read outside of Python.</li> <li>\u2705 Applies zlib lossless compression to reduce the file size. </li> </ul> </li> <li> <p><code>BmpParser</code> de/serializes numpy arrays into BMP files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> <li>\u2705 Fast de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>PngParser</code> de/serializes numpy arrays into PNG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u2705 Produces smaller files due to image compression. </li> <li>\u274c Slow de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>JpegParser</code> de/serializes numpy arrays into JPEG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale and RGB uint8 images.</li> <li>\u2705 Produces very small files due to image compression. </li> <li>\u2705 Fast de/serialization.</li> <li>\u274c Lossy.</li> </ul> </li> </ul>"},{"location":"tutorial/data/#custom-parsers","title":"Custom Parsers","text":"<p>With Pipewine you are not limited to use the built-in Parsers, you can implement your own and use it seamlessly as if it were provided by the library.</p> <p>Example</p> <p>Let's create a <code>TrimeshParser</code> that is able to handle 3D meshes using the popular library Trimesh</p> <pre><code>class TrimeshParser(Parser[tm.Trimesh]):\n    def parse(self, data: bytes) -&gt; tm.Trimesh:\n        # Create a binary buffer with the binary data\n        buffer = io.BytesIO(data)\n\n        # Read the buffer and let trimesh load the 3D mesh object\n        return tm.load(buffer, file_type=\"obj\")\n\n    def dump(self, data: tm.Trimesh) -&gt; bytes:\n        # Create an empty buffer\n        buffer = io.BytesIO()\n\n        # Export the mesh to the buffer\n        data.export(buffer, file_type=\"obj\")\n\n        # Return the contents of the buffer\n        return buffer.read()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        # This tells pipewine that it can automatically use this parses whenever a \n        # file with .obj extension is found and needs to be parsed.\n        return [\"obj\"]\n</code></pre>"},{"location":"tutorial/data/#immutability","title":"Immutability","text":"<p>All data model types are immutable. Their inner state is hidden in private fields and methods and should never be modified in-place. Instead, they provide public methods that return copies with altered values, leaving the original object intact.</p> <p>With immutability, a design decision inherited by the old Pipelime, we can be certain that every object is in the correct state everytime, since it cannot possibly change, and this prevents many issues when the same function is run multiple times, possibly in non-deterministic order.</p> <p>Example</p> <p>Let's say you have a sample containing an item named <code>image</code> with an RGB image. You want to resize the image reducing the resolution to 50% of the original size.</p> <p>To change the image in a sample, you need to create a new sample in which the <code>image</code> item contains the resized image.</p> <pre><code>def half_res(image: np.ndarray) -&gt; np.ndarray:\n    # Some code that downscales an image by 50%\n    ...\n\n# Read the image (more details later)\nimage = sample[\"image\"]()\n\n# Downscale the image\nhalf_image = half_res(image)\n\n# Create a new sample with the new (downscaled) image\nnew_sample = sample.with_value(\"image\", half_image)\n</code></pre> <p>At the end of the snippet above, the <code>sample</code> variable will still contain the original full-size image. Instead, <code>new_sample</code> will contain the new resized image.</p> <p>There are only two exceptions to this immutability rule:</p> <ol> <li>Caches: They need to change their state to save time when the result of a computation is already known. Since all other data is immutable, caches never need to be invalidated.</li> <li>Inner data: While all pipewine data objects are immutable, this may not be true for the data contained within them. If your item contains mutable objects, you are able to modify them implace. But never do that! </li> </ol> <p>Python, unlike other languages, has no mechanism to enforce read-only access to an object, the only way to do so would be to perform a deep-copy whenever an object is accessed, but that would be a complete disaster performance-wise.</p> <p>So, when dealing with mutable data structures inside your items, make sure you either:</p> <ul> <li>Access the data without applying changes.</li> <li>Create a deep copy of the data before applying in-place changes.</li> </ul> <p>Danger</p> <p>Never do this!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Apply some in-place changes to image\nimage += 1\nimage *= 0.9 \nimage += 1   \n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present both in the old and new sample, violating the immutability rule.</p> <p>Success</p> <p>Do this instead!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Create a copy of the image with modified data\nimage = image + 1\n\n# Since image is now a copy of the original data, you can now apply all \n# the in-place changes you like now. \nimage *= 0.9 # Perfectly safe\nimage += 1   # Perfectly safe\n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present only in the new sample.</p>"},{"location":"tutorial/installation/","title":"\ud83d\udee0\ufe0f Installation","text":""},{"location":"tutorial/installation/#basic-installation","title":"Basic Installation","text":"<p>Before installing make sure you have:</p> <ul> <li>A Python3.12+ interpreter installed on your system.</li> <li>A Virtual Environment, which is highly recommended to avoid messing up your system-level python environment.</li> <li> <p>A relatively up-to-date version of <code>pip</code>. You can upgrade to the latest version using</p> <pre><code>pip install -U pip\n</code></pre> </li> </ul> <p>To install Pipewine, run:</p> <pre><code>pip install pipewine\n</code></pre> <p>You can then verify whether <code>pipewine</code> was correctly installed by calling the CLI:</p> <pre><code>pipewine --version\n</code></pre> <p>Success</p> <p>In case the installation is successful, you should see the version of the current Pipewine installation, e.g:</p> <pre><code>0.1.0\n</code></pre> <p>Failure</p> <p>In case something went wrong, you should see something like (this may vary based on your shell type):</p> <pre><code>bash: command not found: pipewine\n</code></pre> <p>In this case, do the following:</p> <ol> <li>Check for any <code>pip</code> error messages during installation.</li> <li>Go back through all steps and check whether you followed them correctly.</li> <li>Open a GitHub issue describing the installation problem. </li> </ol>"},{"location":"tutorial/installation/#dev-installation","title":"Dev Installation","text":"<p>If you are a dev and want to install Pipewine for development purposes, it's recommended you follow these steps instead:</p> <ol> <li>Clone the github repo in a folder of your choice:     <pre><code>git clone https://github.com/lucabonfiglioli/pipewine.git\ncd pipewine\n</code></pre></li> <li>Create a new virtual environment:     <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate \n</code></pre></li> <li>Update pip:     <pre><code>pip install -U pip\n</code></pre></li> <li>Install pipewine in edit mode:     <pre><code>pip install -e .\n</code></pre></li> <li> <p>Install pipewine optional dependencies     <pre><code>pip install .[dev] .[docs]\n</code></pre></p> <p>Warning</p> <p>With some shells (like <code>zsh</code>), you may need to escape the square brackets e.g: <code>.\\[dev\\]</code> or <code>.\\[docs\\]</code>.</p> </li> </ol>"},{"location":"tutorial/overview/","title":"\ud83d\udca0 Overview","text":""},{"location":"tutorial/overview/#high-level","title":"High Level","text":"<p>Pipewine provides you with tools to help decouple what you do with data from the way data is represented and stored. It does so by providing a set of abstractions for many aspects of your data pipeline:</p> <ul> <li><code>Dataset</code>, <code>Sample</code>, <code>Item</code> define how the data is structured, how many data samples are there, in which order, what is their content, how are they accessed etc...</li> <li>More low-level abstractions such as <code>Parser</code> and <code>Reader</code> define how data is encoded and stored. </li> <li><code>DatasetSource</code>, <code>DatasetSink</code>, <code>DatasetOperator</code> define how the data is read, written and transformed, and consistitute the base building blocks for workflows.</li> <li><code>Workflow</code> defines how a set of operators are interconnected. They can be seen as DAGs (Directed Acyclic Graph) in which nodes are sources, sinks or operators, and edges are datasets. </li> </ul> <p>All of these components are designed to allow the user to easily create custom implementations that can be seamlessly integrated with all the built-in blocks.</p> <p>By doing so, Pipewine (much like Pipelime) encourages you to write components that are likely to be highly re-usable.</p>"},{"location":"tutorial/overview/#extendibility","title":"Extendibility","text":"<p>Pipewine is completely agnostic on the following aspects of your data:</p> <ul> <li>Storage location: you can store data anywhere you want, on the file system, on a DB of your choice, on the device memory, on a remote source. You just need to implement the necessary components. </li> <li>Data encoding: By default Pipewine supports some popular image encodings, JSON/YAML metadata, numpy encoding for array data and Pickle encoding for generic python objects. You can easily add custom encodings to read/write data as you like.</li> <li>Data format: By default Pipewine supports the same built-in dataset format as Pipelime, a file system based format called \"Underfolder\" that is flexible to most use-cases but has a few limitations. Dataset formats are highly dependent on the application, thus Pipewine allows you to fully take control on how to structure your datasets.</li> <li>Data operators: As mentioned previously, you can define custom operators that do all sorts of things with your data. Built-in operators cover some common things you may want to do at some point such as concatenating two or more datasets, filtering samples based on a criterion, splitting datasets into smaller chunks, apply the same function (called <code>Mapper</code>) to all samples of a dataset.  </li> </ul>"},{"location":"tutorial/overview/#a-note-on-performance","title":"A Note on Performance","text":"<p>Pipewine is a python package and it's currently 100% python, therefore it's certainly going to be orders of magnitude slower than it could be if written in another language.</p> <p>Having said that, Pipewine still tries its best to maximize efficiency by leveraging:</p> <ul> <li>Caching: Results of computations can be cached to avoid being computed multiple times. This was also done by Pipelime, but they way cache works underwent many changes in the rewrite.</li> <li>Parallelism: Many operations are automatically run in parallel with a multi-processing pool of workers. </li> <li>Linking: When writing to file system, Pipewine automatically attempts to leverage hard-links where possible to avoid serializing and writing the same file multiple times.</li> <li>Vectorization: Where possible, Pipewine uses Numpy to perform vectorized computation on batches of data, achieving better performance if compared to plain python code.</li> </ul> <p>Furthermore, when performing complex operations such as image processing, inference with AI models, 3D data processing, the performance overhead of Pipewine will likely become negligible if compared to the complexity of the individual operations.</p>"},{"location":"tutorial/overview/#a-note-on-scalability","title":"A Note on Scalability","text":"<p>Pipewine - and its predecessor Pipelime - are meant to quickly let you manipulate data without either having to:</p> <ul> <li>Coding everything from scratch and come up with meaningful abstractions yourself. </li> <li>Setting up complex and expensive frameworks that can run data pipelines on distributed systems with many nodes.</li> </ul> <p>Warning</p> <p>If you are running data pipelines on petabytes of data, in distributed systems, with strong consistency requirements and the need for data replication at each step, Pipewine is not what you are looking for.</p> <p>Success</p> <p>If you need to run data pipelines on small/medium datasets (in the order of gigabytes) and want a flexible tool to help you do that, then Pipewine might be what you are looking for.</p>"},{"location":"tutorial/workflows/","title":"\u267b\ufe0f Workflows","text":""},{"location":"tutorial/workflows/#overwiew","title":"Overwiew","text":"<p>So far in this tutorial we always focused on simple cases where we only needed to apply a single operation to our data, with the only exception of a few cache-related examples where we saw the interactions and side-effects of two lazy operations applied sequentially. </p> <p>While applying a single operation to your data may be useful in some situations, more often than not you will need to apply multiple operations in a repeatable and organized way. One way of doing this is to write a python script that calls the right operations in the right order and that's it, but with a little extra effort you can turn your script into a Pipewine <code>Workflow</code>: a Directed Acyclic Graph (DAG) where nodes represent Actions (either a source, operator or sinks) and edges represent the dependencies between them.</p> <p>When using Pipewine workflows, you have the following advantages:</p> <ul> <li>Pipewine can automatically draw a 2D representation of your workflow to help you (and others) understand what it does without reading your code. </li> <li>Pipewine automatically attaches callbacks to all operations to track the progress of your workflow while it's running. Progress updates can be visualized live in a TUI (Text-based User Interface) to monitor the progress of long workflows.</li> <li>The code is transformed into a data structure that can be inspected before running it. </li> <li>Pipewine can inject logic into your code (e.g. caches or checkpoints) without you having to write them manually.  </li> </ul>"},{"location":"tutorial/workflows/#example-workflow","title":"Example Workflow","text":"<p>Instead of going through every single workflow component, we will instead focus on how to create and run workflows at a higher level, with an example use case where we perform some operations to augment and rearrange the same old dataset with letters.</p> <p>Here is a summary of all the operations we want to perform in our example DAG:</p> <ol> <li>Read a \"Letter\" dataset - used many times as an example toy dataset in this tutorial. </li> <li>Repeat the dataset 10 times.</li> <li>Apply random color jitter to each sample.</li> <li>Group letters by their type (either 'vowel' or 'consonant').</li> <li>Concatenate the two (vowels and consonants) splits into a single dataset.</li> <li>Write the dataset (A).</li> <li>Sort the vowels by average color brightness.</li> <li>Sort the consonants by average color brightness.</li> <li>Contatenate the sorted splits into a single dataset.</li> <li>Write the dataset (B).</li> </ol> <p>A graphical representation of the operations involved in the workflow (this image was generated using a Pipewine utility to draw workflows).  </p> <p>We start by defining the schema of our data, in this case it's the same as the one used previously in this tutorial:</p> <pre><code>class LetterMetadata(pydantic.BaseModel):\n    letter: str\n    color: str\n\nclass SharedMetadata(pydantic.BaseModel):\n    vowels: list[str]\n    consonants: list[str]\n\nclass LetterSample(TypedSample):\n    image: Item[np.ndarray]\n    metadata: Item[LetterMetadata]\n    shared: Item[SharedMetadata]\n</code></pre> <p>Some steps of our workflow need a custom implementation, or are simply not provided by Pipewine, namely:</p> <ul> <li>The mapper that applies random color jittering (very rudimental): <pre><code>class ColorJitter(Mapper[LetterSample, LetterSample]):\n    def __call__(self, idx: int, x: LetterSample) -&gt; LetterSample:\n        image = x.image()\n        col = np.random.randint(0, 255, (1, 1, 3))\n        alpha = np.random.uniform(0.1, 0.9, [])\n        image = (image * alpha + col * (1 - alpha)).clip(0, 255).astype(np.uint8)\n        return x.with_values(image=image)\n</code></pre></li> <li>The group-by function that separates vowels from consonants: <pre><code>def group_fn(idx: int, sample: LetterSample) -&gt; str:\n    return \"vowel\" if sample.metadata().letter in \"aeiou\" else \"consonant\"\n</code></pre></li> <li>The sort function needed to sort samples by image brightness: <pre><code>def sort_fn(idx: int, sample: LetterSample) -&gt; float:\n    return float(sample.image().mean())\n</code></pre></li> <li>An operation to concatenate all datasets in a dictionary of datasets. Needed because <code>CatOp</code> accepts a sequence of datasets, not a mapping.  <pre><code>class GroupCat(DatasetOperator[Mapping[str, Dataset], Dataset]):\n    def __call__(self, x: Mapping[str, Dataset]) -&gt; Dataset:\n        return CatOp()(list(x.values()))\n</code></pre></li> </ul> <p>Let's pretend we don't know anything about workflows and just write some Python code to apply the operations in the correct order:</p> <pre><code># Input, outputs, grabber\ninput_folder: Path(\"tests/sample_data/underfolders/underfolder_0\")\noutput_a: Path(\"/tmp/out_a\")\noutput_b: Path(\"/tmp/out_b\")\ngrabber = Grabber(8, 50)\n\n# (1) Read the data\ndata = UnderfolderSource(input_folder, sample_type=LetterSample)()\n\n# (2) Repeat the dataset 10 times\ndata = RepeatOp(10)(data)\n\n# (3) Apply random color jitter \n# We need a checkpoint after this operation to avoid recomputing a random function! \ndata = MapOp(ColorJitter())(data)\nUnderfolderSink(ckpt_path := Path(\"/tmp/checkpoint\"), grabber=grabber)\ndata = UnderfolderSource(ckpt_path, sample_type=LetterSample)()\n\n# (4) Group letters by their type\ngroups = GroupByOp(group_fn)(data)\n\n# (5) Concatenate the two types\ndata = GroupCat()(groups)\n\n# (6) Write the dataset A\nUnderfolderSink(out_a, grabber=grabber)(data)\n\n# (7, 8) Sort the two splits by average color brightness\nvowels = SortOp(sort_fn)(groups[\"vowel\"])\nconsonants = SortOp(sort_fn)(groups[\"consonant\"])\n\n# (9) Concatenate the sorted splits \ndata = CatOp()([vowels, consonants])\n\n# (10) Write the dataset B\nUnderfolderSink(out_b, grabber=grabber)(data)\n</code></pre> <p>Now, let's re-write our code using workflows, by applying the following changes:</p> <ol> <li>Create an empty <code>Workflow</code> object named <code>wf</code> at the beginning of our code.</li> <li>Wrap each action call using <code>wf.node()</code>. </li> <li>Call the <code>run_workflow</code> function at the end of our code.</li> </ol> <pre><code># Input, outputs, grabber\ninput_folder: Path(\"tests/sample_data/underfolders/underfolder_0\")\noutput_a: Path(\"/tmp/out_a\")\noutput_b: Path(\"/tmp/out_b\")\ngrabber = Grabber(8, 50)\n\n# Create the worfklow object\nwf = Workflow(WfOptions(checkpoint_grabber=grabber))\n\n# (1) Read the data\ndata = wf.node(UnderfolderSource(input_folder, sample_type=LetterSample))()\n\n# (2) Repeat the dataset 10 times\ndata = wf.node(RepeatOp(10))(data)\n\n# (3) Apply random color jitter (with checkpointing)\ndata = wf.node(MapOp(ColorJitter()), options=WfOptions(checkpoint=True))(data)\n\n# (4) Group letters by their type\ngroups = wf.node(GroupByOp(group_fn))(data)\n\n# (5) Concatenate the two types\ndata = wf.node(GroupCat())(groups)\n\n# (6) Write the dataset A\nwf.node(UnderfolderSink(out_a, grabber=grabber))(data)\n\n# (7, 8) Sort the two splits by average color brightness\nvowels = wf.node(SortOp(sort_fn))(groups[\"vowel\"])\nconsonants = wf.node(SortOp(sort_fn))(groups[\"consonant\"])\n\n# (9) Concatenate the sorted splits \ndata = wf.node(CatOp())([vowels, consonants])\n\n# (10) Write the dataset B\nwf.node(UnderfolderSink(out_b, grabber=grabber))(data)\n\n# Run the workflow\nrun_workflow(wf)\n</code></pre> <p>This code is very similar to the previous and it behaves the same way if executed. What changed is that instead of being executed as soon as operators are called, they are first converted into a DAG, then executed upon calling the <code>run_workflow</code> function. Nothing is done until the last line of code. </p>"},{"location":"tutorial/workflows/#workflow-creation","title":"Workflow Creation","text":"<p>Workflow objects can be created by simply calling the <code>Workflow()</code> constructor. However, you can customize some aspects of your workflow by customizing the workflow options.</p> <p>Workflow options are contained into the <code>WfOptions</code> class and currently include some settings to add a cache or a checkpoint after the operation, automatically collect garbage after an operation completes etc... You can find all details in the API reference. </p> <p>All of these options do not provide a default value, but instead default to an object <code>Default</code>, which is simply a marker that acts as a sentinel. The reason behind this is that you can use <code>WfOptions</code> at the moment in which the workflow is created, but also when adding individual nodes. In these cases, the options of a node always override the ones of the workflow. </p> <p>The true default values of the workflow options is instead specific of the <code>WorkflowExecutor</code>, a component (detailed later) that is responsible for scheduling and executing the pipeline. </p> <p>The order in which the option values are resolved is as follows:</p> <ol> <li>The value specified in the node options, if not <code>Default</code>.</li> <li>The value specified in the workflow options, if not <code>Default</code>.</li> <li>The default value for this <code>WorkflowExecutor</code>. </li> </ol> <p>This is useful because if we want to always write a checkpoint after executing a node, except for a few places, we can write less code by setting <code>checkpoint=True</code> in the workflow options and then setting it to false for the one or two nodes that don't need it.</p>"},{"location":"tutorial/workflows/#workflow-components","title":"Workflow Components","text":"<p>As demonstrated in the previous example, nothing really happens until the <code>run_workflow</code> function is called, despite our code looking pretty much the same as the example without workflows, but how does that happen in practice?</p> <p>When calling the <code>node()</code> method, the action (source, sink or operation) that is passed as first argument is memorized in a data structure inside the <code>Workflow</code> object and a mocked version of it is returned. This mocked version retains exactly the same interface as the true version, but when called it will only record that the operation needs to be called with the given inputs, without actually executing it.</p> <p>Inputs and outputs accepted and returned by the mocked actions are not actual datasets, but only <code>Proxy</code> objects whose only purpose is to serve as a placeholder to create new connections when a new action is called. As actions can accept/return a dataset, a sequence, a tuple, a mapping or a bundle of datasets, mocked actions accept/return proxies for every type of data. </p> <p>Failure</p> <p>With proxies you cannot do the things you would normally do with the actual outputs of an node. E.g. accessing a sample of a proxy dataset, or trying to get its length will raise an error.</p> <p>The dataset is not actually been computed yet, there is no way to know its length in advance!</p> <p>Complete list of things that can or cannot be done with proxies:</p> <ul> <li> <p>Proxy <code>Dataset</code>:</p> <ul> <li>\u274c Getting the length using <code>__len__</code>.</li> <li>\u274c Accessing a sample or a slice using <code>__getitem__</code>.</li> <li>\u274c Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow, either directly or through a list/tuple/mapping/bundle.</li> </ul> </li> <li> <p>Proxy <code>Sequence[Dataset]</code> or <code>tuple[Dataset, ...]</code> where the number of elements is not statically known:</p> <ul> <li>\u274c Getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element using <code>__getitem__</code>. In this case a proxy dataset will be returned.</li> <li>\u274c Extracting a slice using <code>__getitem__</code>. </li> <li>\u274c Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>tuple[Dataset]</code> where the number of elements is statically known: </p> <ul> <li>\u2705 Getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element or a slice using <code>__getitem__</code>. In this case a proxy dataset or a tuple of proxy datasets will be returned.</li> <li>\u2705 Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>Mapping[str, Dataset]</code>:</p> <ul> <li>\u274c getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element using <code>__getitem__</code>. In this case a proxy dataset will be returned.</li> <li>\u274c Iterate through keys, values or items using <code>keys()</code>, <code>values()</code>, <code>items()</code> or <code>__iter__</code>.</li> <li>\u274c Determine whether the mapping contains a given key.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>Bundle[Dataset]</code>:</p> <ul> <li>\u2705 Accessing an element using the <code>__getattr__</code> (dot notation). In this case a proxy dataset will be returned.</li> <li>\u2705 Convert it to a regular dictionary of proxy datasets with the <code>as_dict()</code> method.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> </ul> <p>Important</p> <p>As you can see, there are many limitations to what can be done with these proxy objects and this may seem like a huge limitation. </p> <p>In reality, these limitations only apply during the workflow definition phase: nothing prevents you from accessing the real length of a list of datasets inside a node of the workflow. </p> <p>These limitations are only apparent. Think about it: you must execute the workflow to get its results, and to execute it you must first construct it. It would not make any sense if in order to construct the workflow we would first need to take decisions based on its results, that would be a circular definition!</p> <p>Furthermore, while Pipewine makes these limitations apparent, they are not something new. In fact, even with the old Pipelime, you cannot take decision on the outputs of nodes before executing a DAG. A typical example of this is when splitting a dataset: you must know in advance the number of splits in order to use them in a DAG.</p> <p>Pipewine actually has less limitations in that regard, because lists (or mappings) of datasets can be passed as-is to other nodes of the workflow, even if their length and content is completely unknown, this was not possible using Pipelime DAGs.</p> <p>Tip</p> <p>If you ever need to know the result of a workflow in order to construct it, it may help to:</p> <ul> <li>Statically define certain aspects of the workflow using constants: e.g. if you know in advance the length of a list of datasets as a constant, it makes no sense to compute it using <code>__len__</code>, use the constant value instead.</li> <li>Restructure your code, maybe you are giving a responsibility to the workflow  that should be given to one or more <code>DatasetOperator</code>. </li> <li>As a last resort, avoid using a workflow and instead use a plain python function.</li> </ul>"},{"location":"tutorial/workflows/#workflow-execution","title":"Workflow Execution","text":"<p>Workflows on their own do not do much: they only consist of the graph data structure that, plus some methods to conveniently add new nodes and edges or to inspect them.</p> <p>To execute a workflow you need a <code>WorkflowExecutor</code>, an object whose main responsibility is running the actions contained in the <code>Workflow</code> object on the correct data inputs and in the correct order. It also manages caches, checkpoints, cleanup and sends events to whoever is listening.</p> <p>The only implementation provided (so far) by Pipewine is a very naive executor that simply topologically sorts the workflow and runs each action sequentially, hence the name <code>SequentialWorkflowExecutor</code>. </p> <p>Using it is very simple: you just have to construct the executor and pass your workflow to the <code>execute</code> method: </p> <pre><code>wf: Workflow\n\nexecutor = SequentialWorkflowExecutor()\nexecutor.execute(wf)\n</code></pre> <p>More conveniently, you can simply call the <code>run_workflow</code> function that does something similar under the hood. If not specified, the workflow executor will default to a new instance of <code>SequentialWorkflowExecutor</code>, to spare you the need to construct it and call it explicitly.</p> <pre><code>wf: Workflow\n\nrun_workflow(wf)\n</code></pre>"},{"location":"tutorial/workflows/#worfklow-progress-tracking","title":"Worfklow Progress Tracking","text":"<p>Each node of the workflow, which corresponds to an action, can start zero or more tasks. By \"task\" we mean a finite, ordered collection of units of work that are executed in a single call to an action.</p> <p>Example</p> <p>In the <code>GroupByOp</code>, an operator that splits the input dataset into many parts that have in common a specific key, we need to iterate over the input dataset and retrieve the value associated to the group-by key. This is a task: </p> <ul> <li>It starts after the <code>GroupByOp</code> is called.</li> <li>It has a number of units equal to the length of the input dataset. </li> <li>The i-th unit corresponds to the computation of the group-by key in the i-th sample.</li> <li>It finishes before the <code>GroupByOp</code> returns.</li> </ul> <p>Note</p> <p>Lazy operators do not typically create tasks when called.</p> <p>Tip</p> <p>The <code>self.loop()</code> method of a Pipewine action will automatically invoke callbacks to start, update and finish a new task each time it is called. </p> <p>If a <code>Grabber</code> is used (to speed up computation through parallelism), all worker sub-processes will be configured to automatically emit events each time they complete a unit of work.</p> <p>Workflow executors can be attached to an <code>EventQueue</code>, a FIFO queue that collects any progress update event from the executor and lets whoever is listening to it, typically a <code>Tracker</code>, receive these updates in the correct order. </p> <p><code>EventQueue</code> objects are designed so that the same queue can receive updates from many emitters, possibly located on different processes. This is crucial to enable progress tracking when using a <code>Grabber</code>. The only <code>EventQueue</code> implementation provided so far by Pipewine is the <code>ProcessSharedEventQueue</code>, built on top of a <code>multiprocessing.Pipe</code> object.</p> <p><code>Tracker</code> objects are instead consumers of progress update events. They constantly listen for new updates and display them somewhere. The only <code>Tracker</code> implementation provided by Pipewine is <code>CursesTracker</code>, that conveniently displays progress updates in a ncurses TUI (Text-based User Interface) for immediate reading. </p> <p>To connect all these things together, you need to do the following things:</p> <ol> <li>Construct and populate a <code>Workflow</code>.</li> <li>Construct a <code>WorkflowExecutor</code>.</li> <li>Construct a <code>EventQueue</code>.</li> <li>Construct a <code>Tracker</code>.</li> <li>Call the <code>start()</code> method of the <code>EventQueue</code>.</li> <li>Attach the executor to the running event queue.</li> <li>Attach the tracker to the running event queue.</li> <li>Execute the workflow.</li> <li>Detach the executor from the event queue.</li> <li>Detach the tracker from the event queue.</li> <li>Close the event queue.</li> </ol> <pre><code># Workflows and other components\nworkflow: Workflow\nexecutor = SequentialWorkflowExecutor()\nevent_queue = ProcessSharedEventQueue()\ntracker = CursesTracker()\n\n# Setup\nevent_queue.start()\nexecutor.attach(event_queue)\ntracker.attach(event_queue)\n\n# Execution\nexecutor.run(workflow)\n\n# Teardown\nexecutor.detach()\ntracker.detach()\nevent_queue.close()\n</code></pre> <p>This is kind of complicated right? To simplify things a bit on your side, the <code>run_workflow</code> function does all this complicated stuff for you. Equivalently to the code above:</p> <pre><code>workflow: Workflow\ntracker = CursesTracker()\n\nrun_workflow(workflow, tracker=tracker)\n</code></pre> <p></p>"},{"location":"tutorial/workflows/#workflow-drawing","title":"Workflow Drawing","text":"<p>Workflows can be drawn using the <code>draw_workflow</code> function:</p> <pre><code>workflow: Workflow\n\ndraw_workflow(workflow, Path(\"/tmp/workflow.svg\"))\n</code></pre> <p>Under the hood, this function calls two different components:</p> <ul> <li><code>Layout</code>: responsible for determining the position and size of workflow nodes in a 2D image plane.</li> <li><code>Drawer</code>: responsible for taking an already laid out workflow and writing a file that can be visualized by external software such as a web browser or an image viewer.</li> </ul> <p>As for the layout part, Pipewine provides <code>OptimizedLayout</code>, a class that attempts to position the nodes on a 2D grid such that the overall flow can be read from left to right such that:</p> <ul> <li>Nodes are not too much spread out, nor too close to each other.</li> <li>Excessively long edges are penalized.</li> <li>Edge-edge intersections are minimized.</li> <li>Edge-node intersections are minimized.</li> <li>Edges are directed towards the right side of the image.</li> </ul> <p>This is achieved using a very simple evolutionary approach. The implementation is kind of difficult to follow due to heavy use of vectorization to eliminate all Python for loops and control flow, crucial to ensure acceptable performance (&lt;1000ms execution time). </p> <p><code>SVGDrawer</code> takes the laid-out graph and writes it into a SVG file that can be opened in any modern web browser and embedded pretty much everywhere.</p>"}]}