{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipewine","text":"<p>Pipewine is a complete rewrite of the Pipelime library and intends to serve the same purpose: provide a set of tools to manipulate multi-modal small/medium-sized datasets, mainly for research purposes in the CV/ML domain.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Unified access pattern to datasets of various formats, origin and content.</li> <li>Underfolder, a quick and easy filesystem-based dataset format good for small/medium datasets.</li> <li>Common data encoding formats for images, text and metadata.</li> <li>Common operations to manipulate existing datasets.</li> <li>Workflows that transform data in complex DAGs (Directed Acyclic Graph) pipelines.</li> <li>CLI (Command Line Interface) to perform operations without writing a full python script.</li> <li>Extendibility, allowing the user to easily extend the library in many aspects, adding components that seamlessly integrate with the built-in ones:<ul> <li>Add custom dataset formats</li> <li>Add custom data encoding formats</li> <li>Add custom operations on datasets</li> <li>Register components to the CLI</li> </ul> </li> </ul>"},{"location":"#rationale","title":"\u2b50\ufe0f Rationale","text":"<p>Pipewine started from a refactoring of some core components of the Pipelime library, but it soon turned into a complete rewrite that aims at solving some architectural issues of Pipelime, namely:</p> <ul> <li>Dependency Hell: Pipelime had so many of dependencies and contstraints that made it very hard to install in many environments due to conflicts and incompatibilities. Outdated dependencies at the core of the library (e.g. Pydantic 1) make it incompatible with new python packages that use Pydantic 2+. Upgrading these dependencies is currently impossible without breaking everything.</li> <li>Over-reliance on Pydantic: Making most library components inherit from Pydantic <code>BaseModel</code>'s (expecially with the 1.0 major) completely violate type-safety and result in type-checking errors if used with any modern python type-checker like MyPy or PyLance. As a result, IntelliSense and autocompletion do not work properly, requiring the user to fill their code with <code>type: ignore</code> directives. Furthermore, Pydantic runtime validation has some serious performance issues, slowing down the computation significantly (especially before rewriting the whole library in Rust).</li> <li>Lack of typed abstractions: Many components of Pipelime (e.g. <code>SamplesSequence</code>, <code>Sample</code>, <code>SampleStage</code>, <code>PipedSequence</code> and <code>PipelimeCommand</code>) do not carry any information on the type of data they operate on, thus making the usage of Pipelime error-prone: the user cannot rely on the type checker to know what type of data is currently processing. Some later components like <code>Entity</code> and <code>Action</code> aim at mitigating this problem by encapsulating samples inside a Pydantic model, but they ended up being almost unused because they integrate poorly with the rest of the library.</li> <li>Confusing CLI: Pipelime CLI was built with the purpose of being able to directly instantiate any (possibly nested) Pydantic model. This resulted in a very powerful CLI that could do pretty much anything but was also very confusing to use (and even worse to maintain). As a contributor of the pipelime library, I never met anybody able to read and understand the CLI validation errors.</li> <li>Everything is CLI: All Pipelime components are also CLI components. This has the positive effect of eliminating the need to manually write CLI hooks for data operations, but also couples the CLI with the API, requiring everything to be serializable as a list of key-value pairs of (possibly nested) builtin values. We solved this using Pydantic, but was it really worth the cost?</li> <li>Feature Creep: Lots of features like streams, remotes, choixe add a lot of complexity but are mostly unused. </li> </ul> <p>Key development decisions behind Pipewine:</p> <ul> <li>Minimal Dependencies: Rely on the standard library as much as possible, only depend on widely-used and maintained 3rd-party libraries. </li> <li>Type-Safety: The library heavily relies on Python type annotations to achieve a desirable level of type safety at development-time. Runtime type checking is limited to external data validation to not hinder the performance too much. The user should be able to rely on any modern static type checker to notice and correct bugs. </li> <li>Pydantic: Limit the use of Pydantic for stuff that is not strictly external data validation. When serialization and runtime validation are not needed, plain dataclasses are a perfect alternative.</li> <li>CLI Segregation: The CLI is merely a tool to quickly access some of the core library functionalities, no core component should ever depend on it. </li> <li>Limited Compatibility Pipewine should be able to read data written by Pipelime and potentially be used alonside it, but it is not intended to be a backward-compatible update, it is in fact a separate project with a separate development cycle.</li> <li>Feature Pruning Avoid including complex features that no one is going to use, instead, focus on keeping the library easy to extend. </li> </ul>"},{"location":"tutorial/cli/","title":"\ud83d\udda5\ufe0f CLI","text":""},{"location":"tutorial/data/","title":"\ud83d\uddc3\ufe0f Data Model","text":""},{"location":"tutorial/data/#overview","title":"Overview","text":"<p>Pipewine data model is composed of three main abstractions: </p> <ul> <li>Dataset - A Sequence of <code>Sample</code> instances, where \"sequence\" means an ordered collection that supports indexing, slicing and iteration.</li> <li>Sample - A Mapping of strings to <code>Item</code> instances, where \"mapping\" means a set of key-value pairs that supports indexing and iteration. </li> <li>Item - An object that has access to the underlying data unit. E.g. images, text, structured metadata, numpy arrays, and whatever serializable object you may want to include in your dataset.</li> </ul> <p>Plus, some lower level components that are detailed later on. You can disregard them for now:</p> <ul> <li>Parser - Defines how an item should encode/decode the associated data.</li> <li>ReadStorage - Defines how an item should access data stored elsewhere.</li> </ul>"},{"location":"tutorial/data/#immutability","title":"Immutability","text":"<p>All data model types are immutable. Their inner state is hidden in private fields and methods and should never be modified in-place. Instead, they provide public methods that return copies with altered values, leaving the original object intact.</p> <p>With immutability, a design decision inherited by the old Pipelime, we can be certain that every object is in the correct state everytime, since it cannot possibly change, and this prevents many issues when the same function is run multiple times, possibly in non-deterministic order.</p> <p>Example</p> <p>Let's say you have a sample containing an item named <code>image</code> with an RGB image. You want to resize the image reducing the resolution to 50% of the original size.</p> <p>To change the image in a sample, you need to create a new sample in which the <code>image</code> item contains the resized image.</p> <pre><code>def half_res(image: np.ndarray) -&gt; np.ndarray:\n    # Some code that downscales an image by 50%\n    ...\n\n# Read the image (more details later)\nimage = sample[\"image\"]()\n\n# Downscale the image\nhalf_image = half_res(image)\n\n# Create a new sample with the new (downscaled) image\nnew_sample = sample.with_value(\"image\", half_image)\n</code></pre> <p>At the end of the snippet above, the <code>sample</code> variable will still contain the original full-size image. Instead, <code>new_sample</code> will contain the new resized image.</p> <p>There are only two exceptions to this immutability rule:</p> <ol> <li>Caches: They need to change their state to save time when the result of a computation is already known. Since all other data is immutable, caches never need to be invalidated.</li> <li>Inner data: While all pipewine data objects are immutable, this may not be true for the data contained within them. If your item contains mutable objects, you are able to modify them implace. But never do that! </li> </ol> <p>When dealing with mutable data structures inside your items, make sure you either:</p> <ul> <li>Access the data without applying changes.</li> <li>Create a copy of the data before applying in-place changes.</li> </ul> <p>Danger</p> <p>Never do this!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]()\n\n# Apply some in-place changes to image\nimage += 1\nimage *= 0.9 \nimage += 1   \n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present both in the old and new sample, violating the immutability rule.</p> <p>Success</p> <p>Do this instead!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]()\n\n# Create a copy of the image with modified data\nimage = image + 1\n\n# Since image is now a copy of the original data, you can now apply all \n# the in-place changes you like now. \nimage *= 0.9 # Perfectly safe\nimage += 1   # Perfectly safe\n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present only in the new sample.</p>"},{"location":"tutorial/data/#dataset","title":"Dataset","text":""},{"location":"tutorial/data/#sample","title":"Sample","text":""},{"location":"tutorial/data/#item","title":"Item","text":""},{"location":"tutorial/data/#parser","title":"Parser","text":""},{"location":"tutorial/data/#readstorage","title":"ReadStorage","text":""},{"location":"tutorial/installation/","title":"\ud83d\udee0\ufe0f Installation","text":""},{"location":"tutorial/installation/#basic-installation","title":"Basic Installation","text":"<p>Before installing make sure you have:</p> <ul> <li>A Python3.12+ interpreter installed on your system.</li> <li>A Virtual Environment, which is highly recommended to avoid messing up your system-level python environment.</li> <li> <p>A relatively up-to-date version of <code>pip</code>. You can upgrade to the latest version using</p> <pre><code>pip install -U pip\n</code></pre> </li> </ul> <p>To install Pipewine, run:</p> <pre><code>pip install pipewine\n</code></pre> <p>You can then verify whether <code>pipewine</code> was correctly installed by calling the CLI:</p> <pre><code>pipewine --version\n</code></pre> <p>Success</p> <p>In case the installation is successful, you should see the version of the current Pipewine installation, e.g:</p> <pre><code>0.1.0\n</code></pre> <p>Failure</p> <p>In case something went wrong, you should see something like (this may vary based on your shell type):</p> <pre><code>bash: command not found: pipewine\n</code></pre> <p>In this case, do the following:</p> <ol> <li>Check for any <code>pip</code> error messages during installation.</li> <li>Go back through all steps and check whether you followed them correctly.</li> <li>Open a GitHub issue describing the installation problem. </li> </ol>"},{"location":"tutorial/installation/#dev-installation","title":"Dev Installation","text":"<p>If you are a dev and want to install Pipewine for development purposes, it's recommended you follow these steps instead:</p> <ol> <li>Clone the github repo in a folder of your choice:     <pre><code>git clone https://github.com/lucabonfiglioli/pipewine.git\ncd pipewine\n</code></pre></li> <li>Create a new virtual environment:     <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate \n</code></pre></li> <li>Update pip:     <pre><code>pip install -U pip\n</code></pre></li> <li>Install pipewine in edit mode:     <pre><code>pip install -e .\n</code></pre></li> <li> <p>Install pipewine optional dependencies     <pre><code>pip install .[dev] .[docs]\n</code></pre></p> <p>Warning</p> <p>With some shells (like <code>zsh</code>), you may need to escape the square brackets e.g: <code>.\\[dev\\]</code> or <code>.\\[docs\\]</code>.</p> </li> </ol>"},{"location":"tutorial/operators/","title":"\u2699\ufe0f Operators","text":""},{"location":"tutorial/overview/","title":"\ud83d\udca0 Overview","text":""},{"location":"tutorial/overview/#high-level","title":"High Level","text":"<p>Pipewine provides you with tools to help decouple what you do with data from the way data is represented and stored. It does so by providing a set of abstractions for many aspects of your data pipeline:</p> <ul> <li><code>Dataset</code>, <code>Sample</code>, <code>Item</code> define how the data is structured, how many data samples are there, in which order, what is their content, how are they accessed etc...</li> <li>More low-level abstractions such as <code>Parser</code> and <code>ReadStorage</code> define how data is encoded and stored. </li> <li><code>DatasetSource</code>, <code>DatasetSink</code>, <code>DatasetOperator</code> define how the data is read, written and transformed, and consistitute the base building blocks for workflows.</li> <li><code>Workflow</code> defines how a set of operators are interconnected. They can be seen as DAGs (Directed Acyclic Graph) in which nodes are sources, sinks or operators, and edges are datasets. </li> </ul> <p>All of these components are designed to allow the user to easily create custom implementations that can be seamlessly integrated with all the built-in blocks.</p> <p>By doing so, Pipewine (much like Pipelime) encourages you to write components that are likely to be highly re-usable.</p>"},{"location":"tutorial/overview/#extendibility","title":"Extendibility","text":"<p>Pipewine is completely agnostic on the following aspects of your data:</p> <ul> <li>Storage location: you can store data anywhere you want, on the file system, on a DB of your choice, on the device memory, on a remote source. You just need to implement the necessary components. </li> <li>Data encoding: By default Pipewine supports some popular image encodings, JSON/YAML metadata, numpy encoding for array data and Pickle encoding for generic python objects. You can easily add custom encodings to read/write data as you like.</li> <li>Data format: By default Pipewine supports the same built-in dataset format as Pipelime, a file system based format called \"Underfolder\" that is flexible to most use-cases but has a few limitations. Dataset formats are highly dependent on the application, thus Pipewine allows you to fully take control on how to structure your datasets.</li> <li>Data operators: As mentioned previously, you can define custom operators that do all sorts of things with your data. Built-in operators cover some common things you may want to do at some point such as concatenating two or more datasets, filtering samples based on a criterion, splitting datasets into smaller chunks, apply the same function (called <code>Mapper</code>) to all samples of a dataset.  </li> </ul>"},{"location":"tutorial/overview/#a-note-on-performance","title":"A Note on Performance","text":"<p>Pipewine is a python package and it's currently 100% python, therefore it's certainly going to be orders of magnitude slower than it could be if written in another language.</p> <p>Having said that, Pipewine still tries its best to maximize efficiency by leveraging:</p> <ul> <li>Caching: Results of computations can be cached to avoid being computed multiple times. This was also done by Pipelime, but they way cache works underwent many changes in the rewrite.</li> <li>Parallelism: Many operations are automatically run in parallel with a multi-processing pool of workers. </li> <li>Linking: When writing to file system, Pipewine automatically attempts to leverage hard-links where possible to avoid serializing and writing the same file multiple times.</li> <li>Vectorization: Where possible, Pipewine uses Numpy to perform vectorized computation on batches of data, achieving better performance if compared to plain python code.</li> </ul> <p>Furthermore, when performing complex operations such as image processing, inference with AI models, 3D data processing, the performance overhead of Pipewine will likely become negligible if compared to the complexity of the individual operations.</p>"},{"location":"tutorial/overview/#a-note-on-scalability","title":"A Note on Scalability","text":"<p>Pipewine - and its predecessor Pipelime - are meant to quickly let you manipulate data without either having to:</p> <ul> <li>Coding everything from scratch and come up with meaningful abstractions yourself. </li> <li>Setting up complex and expensive frameworks that can run data pipelines on distributed systems with many nodes.</li> </ul> <p>Warning</p> <p>If you are running data pipelines on petabytes of data, in distributed systems, with strong consistency requirements and the need for data replication at each step, Pipewine is not what you are looking for.</p> <p>Success</p> <p>If you need to run data pipelines on small/medium datasets (in the order of gigabytes) and want a flexible tool to help you do that, then Pipewine might be what you are looking for.</p>"},{"location":"tutorial/sinks/","title":"\u2b55 Sinks","text":""},{"location":"tutorial/sources/","title":"\u2733\ufe0f Sources","text":""},{"location":"tutorial/workflows/","title":"\u267b\ufe0f Workflows","text":""}]}