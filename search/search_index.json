{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipewine","text":"<p>Pipewine is a complete rewrite of the Pipelime library and intends to serve the same purpose: provide a set of tools to manipulate multi-modal small/medium-sized datasets, mainly for research purposes in the CV/ML domain.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Unified access pattern to datasets of various formats, origin and content.</li> <li>Underfolder, a quick and easy filesystem-based dataset format good for small/medium datasets.</li> <li>Common data encoding formats for images, text and metadata.</li> <li>Common operators to manipulate existing datasets.</li> <li>Workflows that transform data in complex DAGs (Directed Acyclic Graph) pipelines.</li> <li>CLI (Command Line Interface) to perform operators without writing a full python script.</li> <li>Extendibility, allowing the user to easily extend the library in many aspects, adding components that seamlessly integrate with the built-in ones:<ul> <li>Add custom dataset formats</li> <li>Add custom data encoding formats</li> <li>Add custom operators on datasets</li> <li>Register components to the CLI</li> </ul> </li> </ul>"},{"location":"#rationale","title":"\u2b50\ufe0f Rationale","text":"<p>Pipewine started from a refactoring of some core components of the Pipelime library, but it soon turned into a complete rewrite that aims at solving some architectural issues of Pipelime, namely:</p> <ul> <li>Dependency Hell: Pipelime had so many of dependencies and contstraints that made it very hard to install in many environments due to conflicts and incompatibilities. Outdated dependencies at the core of the library (e.g. Pydantic 1) make it incompatible with new python packages that use Pydantic 2+. Upgrading these dependencies is currently impossible without breaking everything.</li> <li>Over-reliance on Pydantic: Making most library components inherit from Pydantic <code>BaseModel</code>'s (expecially with the 1.0 major) completely violate type-safety and result in type-checking errors if used with any modern python type-checker like MyPy or PyLance. As a result, IntelliSense and autocompletion do not work properly, requiring the user to fill their code with <code>type: ignore</code> directives. Furthermore, Pydantic runtime validation has some serious performance issues, slowing down the computation significantly (especially before rewriting the whole library in Rust).</li> <li>Lack of typed abstractions: Many components of Pipelime (e.g. <code>SamplesSequence</code>, <code>Sample</code>, <code>SampleStage</code>, <code>PipedSequence</code> and <code>PipelimeCommand</code>) do not carry any information on the type of data they operate on, thus making the usage of Pipelime error-prone: the user cannot rely on the type checker to know what type of data is currently processing. Some later components like <code>Entity</code> and <code>Action</code> aim at mitigating this problem by encapsulating samples inside a Pydantic model, but they ended up being almost unused because they integrate poorly with the rest of the library.</li> <li>Confusing CLI: Pipelime CLI was built with the purpose of being able to directly instantiate any (possibly nested) Pydantic model. This resulted in a very powerful CLI that could do pretty much anything but was also very confusing to use (and even worse to maintain). As a contributor of the pipelime library, I never met anybody able to read and understand the CLI validation errors.</li> <li>Everything is CLI: All Pipelime components are also CLI components. This has the positive effect of eliminating the need to manually write CLI hooks for data operators, but also couples the CLI with the API, requiring everything to be serializable as a list of key-value pairs of (possibly nested) builtin values. We solved this using Pydantic, but was it really worth the cost?</li> <li>Feature Creep: Lots of features like streams, remotes, choixe add a lot of complexity but are mostly unused. </li> </ul> <p>Key development decisions behind Pipewine:</p> <ul> <li>Minimal Dependencies: Rely on the standard library as much as possible, only depend on widely-used and maintained 3rd-party libraries. </li> <li>Type-Safety: The library heavily relies on Python type annotations to achieve a desirable level of type safety at development-time. Runtime type checking is limited to external data validation to not hinder the performance too much. The user should be able to rely on any modern static type checker to notice and correct bugs. </li> <li>Pydantic: Limit the use of Pydantic for stuff that is not strictly external data validation. When serialization and runtime validation are not needed, plain dataclasses are a perfect alternative.</li> <li>CLI Segregation: The CLI is merely a tool to quickly access some of the core library functionalities, no core component should ever depend on it. </li> <li>Limited Compatibility Pipewine should be able to read data written by Pipelime and potentially be used alonside it, but it is not intended to be a backward-compatible update, it is in fact a separate project with a separate development cycle.</li> <li>Feature Pruning Avoid including complex features that no one is going to use, instead, focus on keeping the library easy to extend. </li> </ul>"},{"location":"tutorial/actions/","title":"\u2699\ufe0f Actions","text":""},{"location":"tutorial/actions/#overview","title":"Overview","text":"<p>In this section you will learn what are the main operational abstractions that define how Pipewine transforms data, how to interact with them and how to build re-usable building blocks for your data pipelines.</p> <p>Note</p> <p>This section assumes you already know the basics on the way the data model is structured. If you haven't already, go take a look at the Data Model section. </p> <p>The main components that are explained in this section include:</p> <ul> <li>Sources - Action that creates Pipewine datasets from external storages.</li> <li>Sinks - Action that saves Pipewine datasets to external storages.</li> <li>Operators - Action that transforms Pipewine dataset/s into Pipewine dataset/s.</li> <li>Mappers - Applies the same operation to every sample of a dataset.</li> <li>Grabber - Distributes work to a multi-processing pool of workers.</li> </ul>"},{"location":"tutorial/actions/#types-of-actions","title":"Types of Actions","text":"<p>Pipewine uses the broad term \"Action\" to denote any of the following components:</p> <ul> <li><code>DatasetSource</code> - A component that is able to create one or more <code>Dataset</code> objects.  </li> <li><code>DatasetSink</code> - A component that is able to consume one or more <code>Dataset</code> objects. </li> <li><code>DatasetOperator</code> - A components that takes as input and returns one or more <code>Dataset</code> objects.</li> </ul> <p>In general, actions can receive and return one of the following:</p> <ul> <li>A single <code>Dataset</code> object.</li> <li>A <code>Sequence</code> of datasets.</li> <li>A <code>tuple</code> of datasets.</li> <li>A <code>Mapping</code> of strings to datasets.</li> <li>A <code>Bundle</code> of datasets.</li> </ul> <p>Specific types of actions statically declare the type of inputs and outputs they accept/return to ensure that their type is always known at development time (as soon as you write the code).</p> <p>Example</p> <p><code>CatOp</code> is a type of <code>DatasetOperation</code> that concatenates one or more datasets into a single one, therefore, its input type is <code>Sequence[Dataset]</code> and its output is <code>Dataset</code>.</p> <p>Any modern static type checker such as <code>mypy</code> or <code>PyLance</code>  will complain if you try to pass anything else to it, preventing such errors at dev time.</p> <p>Tip</p> <p>As a general rule of thumb of what type of input/output to choose when implementing custom actions:</p> <ul> <li>Use <code>Dataset</code> when you want to force that the action accepts/returns exactly one dataset.</li> <li>Use <code>Sequence</code> when you need the operation to accept/return more than one dataset, but you don't know a priory how many, their order or their type.</li> <li>Use <code>tuple</code> in settings similar to the <code>Sequence</code> case but you also know at dev time the exact number of datasets, their order and their type. Tuples, contrary to <code>Sequences</code>, also allow you to specify the type of each individual element.</li> <li>Use <code>Mapping</code> when you need to accept/return collections of datasets that do not have any ordering relationship, but are instead associated with an arbitrary (not known a priori) set of string keys.</li> <li>Use <code>Bundle</code> in settings similar to the <code>Mapping</code> case but you also know at dev time the exact number of datasets, the name of their keys and their type. More details on <code>Bundle</code> in the next sub-section. </li> </ul>"},{"location":"tutorial/actions/#bundle-objects","title":"Bundle objects","text":"<p>Pipewine <code>Bundle[T]</code> are objects that have the following characteristics:</p> <ul> <li>They behave like a Python <code>dataclass</code>.</li> <li>The type of all fields is bound to <code>T</code>.</li> <li>Field names and types are statically known and cannot be modified.</li> <li>They are anemic objects: they only act as a data container and define no methods.</li> </ul> <p>Note</p> <p>You may already have encountered the <code>Bundle</code> type without noticing when we introduced the TypedSample. <code>TypedSample</code> is a <code>Bundle[Item]</code>.</p> <p>Example</p> <p>Creating a <code>Bundle</code> is super easy:</p> <pre><code># Inherit from Bundle[T] and declare some fields\nclass MyBundle(Bundle[str]):\n    foo: str\n    bar: str\n\n# Constructs like a dataclass\nmy_bundle = MyBundle(foo=\"hello\", bar=\"world\")\n\n# Access content with dot notation\nmy_bundle.foo # &gt;&gt;&gt; hello\nmy_bundle.bar # &gt;&gt;&gt; world\n\n# Convert to a plain dict[str, T] when needed\nmy_bundle.as_dict() # &gt;&gt;&gt; {\"foo\": \"hello\", \"bar\": \"world\"}\n\n# Create from a dictionary\nmy_bundle_2 = MyBundle.from_dict({\"foo\": \"hello\", \"bar\": \"world\"})\n</code></pre> <p>Note</p> <p>Despite their similarity <code>Bundle</code> objects do not rely on Pydantic and do not inherit from <code>BaseModel</code>. Do not expect them to provide validation, parsing or dumping functionalities, as they are essentially dataclasses plus some constraints.</p>"},{"location":"tutorial/actions/#eagerlazy-behavior","title":"Eager/Lazy Behavior","text":"<p>Suppose we need to load and display N images stored as files in a directory, comparing the two approaches:</p> <ul> <li>Eager - Every computation is carried out as soon as the necessary data is ready.</li> <li>Lazy - Every operation is performed only when it cannot be delayed anymore.</li> </ul> <p>Let's explore the trade-off between the two approaches:</p> Eager Lazy \u274c Long initialization time: all images must be loaded before the first one is displayed. \u2705 Short initialization time: no loading is done during initialization. \u2705 When the user requests an image, it is immediately available. \u274c When the user requests an image, it must wait for the application to load it before viewing it. \u274c High (and unbounded) memory usage, necessary to keep all images loaded at the same time. \u2705 Always keep at most one image loaded into memory. \u274c Risk of performing unnecessary computation: what if the user only wants to display a small subset of images? \u2705 If the user is not interested in a particular image, it won't be loaded at all. \u2705 Accessing an image multiple times has no additional cost. \u274c Accessing an image multiple times has a cost proportional to the number of times it is accessed. \u2705 Optimizations may be easier and more effective on batched operations. \u274c Optimizations are harder to perform or less effective on small batches accessed out-of-order. <p>As you can see, there are positive and negative aspects in both approaches. You may be inclined to think that for this silly example of loading and displaying images, the lazy approach is clearly better. After all, who would use a software that needs to load the entire photo album of thousands of photos just to display a single image?</p> <p>As you may have noticed in the previous section, Pipewine allows you to choose both approaches, but when there is no clear reason to prefer the eager behavior, its components are implemented so that they behave lazily: <code>Item</code> only reads data when requested to do so, <code>LazyDataset</code> creates the actual <code>Sample</code> object upon indexing. As you will see, most Pipewine actions follow a similar pattern.</p> <p>This decision, inherited from the old Pipelime library, is motivated mainly by the following reasons:</p> <ul> <li>Data pipelines may operate on large amounts of data that we cannot afford to load upfront and keep loaded into system memory. Even small datasets with hundreds of images will impose a significant memory cost on any modern machine. Larger datasets may not be possible to load at all!</li> <li>Until a time machine is invented, there is no way to undo wasteful computation. Suppose you have an image classification dataset and you want to select all samples with the category \"horse\": there is no point in loading an image once you know that it's not what you are looking for. Lazy behavior prevents this kind of unnecessary computation by design.</li> <li>The risk of performing some computation twice can be mitigated by using caches with the right eviction policy.</li> </ul>"},{"location":"tutorial/actions/#parallelism","title":"Parallelism","text":"<p>Pipewine provides you with a <code>Grabber</code> object, a simple tool allows you to iterate over a sequence with a parallel pool of multi-processing workers.</p> <p>Why multi-processing and not multi-threading? Because of GIL (Global Interpreter Lock), multi-threading in Python allows concurrency but not parallelism, effectively granting the same speed as a single-threaded program.</p> <p>To effectively parallelize our code we are left with two options:</p> <ol> <li>Avoid using Python, and instead write components in other languages such as C/C++, then create bindings that allow the usage from Python. </li> <li>Bite the bullet and use multi-processing instead of multi-threading. If compared with threads, processes are a lot more expensive to create and manage, often requiring to serialize and deserialize the whole execution stack upon creation.</li> </ol> <p>The second option is clearly better: </p> <ul> <li>You still get the option to parallelize work in a relatively easy way without having to integrate complex bindings for execution environments other than the Python interpreter. </li> <li>Process creation and communication overhead becomes negligible when dealing with large amounts of relatively independent jobs that don't need much synchronization. This is a relatively common scenario in data pipelines.</li> <li>It does not prevent you to implement optimized multi-threaded components in other languages. Think about it: whenever you use tools like Numpy inside a Pipewine action, you are doing exactly this. </li> </ul>"},{"location":"tutorial/actions/#grabber","title":"Grabber","text":"<p>To parallelize tasks with Pipewine you can use a <code>Grabber</code> object: a simple wrapper around a multiprocessing <code>Pool</code> of workers iterate through an ordered sequence of objects.</p> <p>Note</p> <p><code>Grabber</code> objects can work on arbitrary sequences of objects, provided they are serializable.</p> <p>The <code>Grabber</code> object also does a few nice things for you:</p> <ul> <li>Manages the lifecycle of the underlying pool of processes.</li> <li>Handles exceptions freeing resources and returning the errors transparently to the parent process.</li> <li>Automatically invokes a callback function whenever a task is completed. The callback is run by the individual workers.</li> </ul> <p>When creating a <code>Grabber</code> you will need to choose three main parameters that govern the execution:</p> <ul> <li><code>num_workers</code>: The number of concurrent workers. Ideally, in a magic world where all workload is perfectly distributed and communication overhead is zero, the total time is proportional to <code>num_jobs / num_workers</code>, so the higher the better. In reality:<ul> <li>Work is not perfectly distributed. If you need to run 100 jobs with 3 workers, 2 workers will be assigned 33 jobs each, but the 3rd worker is left with 34 jobs. </li> <li>Different jobs may have different computational costs. If you need to run 100 jobs with 4 workers, equal splits of 25 each may not guarantee that the actual workload is evenly split: a \"lucky\" worker may get the 25 easiest jobs and complete them way before the others have finished running. </li> <li>Processes are expensive to create. With small enough workloads, a single process may actually be faster than a concurrent pool.</li> <li>Everytime an object is passed from one process to the other it needs to be serialized and then de-serialized. This can become quite expensive when dealing with large data structures.</li> <li>Processes need synchronization mechanisms that temporarily halt the computation.</li> <li>Sometimes the bottleneck is not the computation itself: if a single process reading data from the network completely saturates your bandwidth, adding more processes won't fix the problem.</li> <li>Sometimes multiprocessing can be much slower than single processing. A typical example is when concurrently writing to a mechanical HDD, causing it to waste a lot of time going back and forth the writing locations.  </li> <li>Your code may already be parallelized. Whenever you use libraries like Numpy, PyTorch, OpenCV (and many more), you are calling C/C++ bindings that run very efficient multi-threaded code outside of the Python interpreter, or even code that runs on devices other than your CPU (e.g. CUDA-enabled GPUs). Adding multiprocessing in these cases will only add overhead costs to something that already uses your computational resources nearly optimally.</li> <li>Due to memory constraints the number of processes you can keep running without swapping (and thus severely compromising the execution speed) may be limited to a small number. E.g. if each process needs to keep loaded a 10GB deep learning model and the available memory is just 32GB, the maximum amount of processes you can run without swapping is 3. </li> </ul> </li> <li><code>prefetch</code>: The number of tasks that are assigned to each worker whenever they are ready. It's easier to explain this with an analogy. Imagine you need to deliver 1000 products to customers and you have 4 couriers ready to deliver them. How inefficient would it be if every courier delivered one product at a time, returning to the warehouse whenever they complete a delivery? You would still parallelize work across 4 couriers, but would incur in massive synchronization costs (the extra time it takes for the courier to return to the warehouse each time). A smarter solution would be to assign a larger batch of deliveries to each courier (e.g. 50) so that they would have to return to the warehouse less often. </li> <li><code>keep_order</code>: Sometimes, the order in which the operations are performed is not that relevant. Executing tasks out-of-order requires even less synchronization, usually resulting in faster overall execution.</li> </ul> <p>Let's see an example of how a <code>Grabber</code> works. </p> <p>Example</p> <p>We want to iterate through a sequence of objects that, whenever indexed, performs some expensive operation. In this example we simulate it using a <code>time.sleep()</code>.</p> <p>Here is our <code>SlowSequence</code>:</p> <pre><code>class SlowSequence(Sequence[int]):\n\"\"\"Return the numbers in range [A, B), slowly.\"\"\"\n\n    def __init__(self, start: int, stop: int) -&gt; None:\n        self._start = start\n        self._stop = stop\n\n    def __len__(self) -&gt; int:\n        return self._stop - self._start\n\n    @overload\n    def __getitem__(self, idx: int) -&gt; int: ...\n    @overload\n    def __getitem__(self, idx: slice) -&gt; \"SlowSequence\": ...\n    def __getitem__(self, idx: int | slice) -&gt; \"SlowSequence | int\":\n        # Let's not care about slicing for now\n        if isinstance(idx, slice):\n            raise NotImplementedError()\n\n        # Raise if index is out of bounds\n        if idx &gt;= len(self):\n            raise IndexError(idx)\n\n        # Simulate some slow operation and return\n        time.sleep(0.1)\n        return idx + self._start\n</code></pre> <p>Let's create an instance of <code>SlowSequence</code> and define a callback function that we need to call every time we iterate on it:</p> <pre><code>def my_callback(index: int) -&gt; None:\n    print(f\"Callback {index} invoked by process {os.getpid()}\")\n\n# The sequence object of which we want to compute the sum\nsequence = SlowSequence(100, 200)\n</code></pre> <p>Next, let's try to compute the total sum of a <code>SlowSequence</code> with a simple for loop.</p> <pre><code># Compute the sum, and measure total running time\nt1 = time.perf_counter()\n\ntotal = 0\nfor i, x in enumerate(sequence):\n    my_callback(i)\n    total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 10.008610311000666\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total running time is roughly 10s, checks out, since every iteration takes approximately 0.1s and we iterate 100 times. The PID printed by the callback function is always the same, since all computation is performed by a single process.</p> <p>Let's do the same but with with a <code>Grabber</code> with 5 concurrent workers</p> <pre><code>t1 = time.perf_counter()\n\ntotal = 0\ngrabber = Grabber(num_workers=5)\nwith grabber(sequence, callback=my_callback) as par_sequence:\n    for i, x in par_sequence:\n        total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 2.011717862998921\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total time is now near to 2s, the result of evenly splitting among 5 parallel workers. As you can see from the console, the callback is being called by 5 different PIDs:</p> <pre><code>Callback 0 invoked by process 21219\nCallback 2 invoked by process 21220\nCallback 4 invoked by process 21221\nCallback 6 invoked by process 21222\nCallback 8 invoked by process 21223\n...\n</code></pre> <p>Important note: what <code>Grabber</code> parallelizes just the call to the <code>__getitem__</code> method and the callback function. The body of the for loop (the summation) is not parallelized and it's executed by the parent process.</p>"},{"location":"tutorial/actions/#sources-and-sinks","title":"Sources and Sinks","text":"<p><code>DatasetSource</code> and <code>DatasetSink</code> are the two base classes for every Pipewine action that respectively reads or writes datasets (or collections of datasets) from/to external storages.</p> <p>A <code>DatasetSource</code> creates and returns instances of Pipewine <code>Datasets</code>. A <code>DatasetSink</code> consumes instances of Pipewine <code>Datasets</code> </p>"},{"location":"tutorial/actions/#underfolder","title":"Underfolder","text":""},{"location":"tutorial/actions/#custom-formats","title":"Custom Formats","text":""},{"location":"tutorial/actions/#operators","title":"Operators","text":""},{"location":"tutorial/actions/#built-in-operators","title":"Built-in Operators","text":""},{"location":"tutorial/actions/#custom-operators","title":"Custom Operators","text":""},{"location":"tutorial/actions/#mappers","title":"Mappers","text":""},{"location":"tutorial/actions/#built-in-mappers","title":"Built-in Mappers","text":""},{"location":"tutorial/actions/#custom-mappers","title":"Custom Mappers","text":""},{"location":"tutorial/cli/","title":"\ud83d\udda5\ufe0f CLI","text":""},{"location":"tutorial/data/","title":"\ud83d\uddc3\ufe0f Data Model","text":""},{"location":"tutorial/data/#overview","title":"Overview","text":"<p>In this section your will learn what are the main data abstractions upon which Pipewine is built, how to interact with them and extend them according to your needs.</p> <p>Pipewine data model is composed of three main abstractions: </p> <ul> <li>Dataset - A Sequence of <code>Sample</code> instances, where \"sequence\" means an ordered collection that supports indexing, slicing and iteration.</li> <li>Sample - A Mapping of strings to <code>Item</code> instances, where \"mapping\" means a set of key-value pairs that supports indexing and iteration. </li> <li>Item - An object that has access to the underlying data unit. E.g. images, text, structured metadata, numpy arrays, and whatever serializable object you may want to include in your dataset.</li> </ul> <p>Plus, some lower level components that are detailed later on. You can disregard them for now:</p> <ul> <li>Parser - Defines how an item should encode/decode the associated data.</li> <li>Reader - Defines how an item should access data stored elsewhere.</li> </ul>"},{"location":"tutorial/data/#dataset","title":"Dataset","text":"<p><code>Dataset</code> is the highest-level container and manages the following information:</p> <ul> <li>How many samples it contains</li> <li>In which order</li> </ul> <p>It provides methods to access individual samples or slices of datasets, as in Python slice.</p> <p>Note</p> <p>A <code>Dataset</code> is an immutable Python Sequence, supporting all its methods.</p> <p>All <code>Dataset</code> objects are Generics, meaning that they can be hinted with information about the type of samples they contain. This is especially useful if you are using a static type checker.</p> <p>Example</p> <p>Example usage of a <code>Dataset</code> object:</p> <pre><code># Given a Dataset of MySample's \ndataset: Dataset[MySample]\n\n# Supports len\nnumber_of_samples = len(dataset)\n\n# Supports indexing\nsample_0 = dataset[0]    # The type checker infers the type: MySample\nsample_51 = dataset[51]  # The type checker infers the type: MySample\n\n# Suppors slicing\nsub_dataset = dataset[10:20] # The type checker infers the type: Dataset[MySample]\n\n# Supports iteration\nfor sample in dataset:\n    ...\n</code></pre> <p>By default Pipewine provides two implementations of the <code>Dataset</code> interface: </p> <ul> <li><code>ListDataset</code></li> <li><code>LazyDataset</code> </li> </ul>"},{"location":"tutorial/data/#listdataset","title":"ListDataset","text":"<p>A <code>ListDataset</code> is basically a wrapper around a Python <code>list</code>, such that, whenever indexed, the result is immediately available. </p> <p>To achieve this, it has two fundamental requirements:</p> <ol> <li>All samples must be known at creation time.</li> <li>All samples must be always loaded into memory.</li> </ol> <p>Due to these limitations, it's rarely used in the built-in operations, since the lazy alternative <code>LazyDataset</code> combined with caching provides a better trade-off, but it may be handy to have when:</p> <ul> <li>The number of samples is small.</li> <li>Samples are lightweight (i.e. no images, 3d data, huge tensors etc...)</li> </ul> <p>Example</p> <p>Example of how to construct a <code>ListDataset</code>:</p> <pre><code># Create a list of samples\nsamples = [ ... ] \n\n# Wrap it in a ListDataset\ndataset = ListDataset(samples)\n</code></pre> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(N) (including the construction of the list)</li> <li>Length - O(1)</li> <li>Indexing - O(1)</li> <li>Slicing - O(N)</li> </ul>"},{"location":"tutorial/data/#lazydataset","title":"LazyDataset","text":"<p>The smarter alternative is <code>LazyDataset</code>, a type of <code>Dataset</code> that defers the computation of the samples as late as possible. That's right, when using a <code>LazyDataset</code> samples are created when it is indexed, using a user-defined function that is passed at creation time.  </p> <p>This has some implications:</p> <ul> <li>Samples are not required to be known at creation time, meaning that you can create a <code>LazyDataset</code> in virually zero time.</li> <li>Samples are not required to be kept loaded into memory the whole time, meaning that the memory required by <code>LazyDataset</code> is constant.</li> <li>Constant-time slicing.</li> <li>The computatonal cost shifts to the indexing part, which now carries the burden of creating and returning samples. </li> </ul> <p>Example</p> <p>Let's see an example of how to create and use a <code>LazyDataset</code>: </p> <pre><code># Define a function that creates samples from an integer index.\ndef get_sample_fn(idx: int) -&gt; Sample:\n    print(f\"Called with index: {idx}\")\n\n    sample = ... # Omitted \n    return sample\n\n# Create a LazyDataset of length 10\ndataset = LazyDataset(10, get_sample_fn)\n\n# Do some indexing\nsample_0 = dataset[0] # Prints 'Called with index: 0'\nsample_1 = dataset[1] # Prints 'Called with index: 1'\nsample_2 = dataset[2] # Prints 'Called with index: 2'\n\n# Indexing the same sample multiple times calls the function multiple times\nsample_1 = dataset[1] # Prints 'Called with index: 1'\n</code></pre> <p>Warning</p> <p>What if my function is very expensive to compute? Is <code>LazyDataset</code> going to call it every time the dataset is indexed?</p> <p>Yes, but that can be avoided by using Caches, which are not managed by the <code>LazyDataset</code> class.</p> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(1)</li> <li>Length - O(1)</li> <li>Indexing - Depends on <code>get_sample_fn</code> and <code>index_fn</code>.</li> <li>Slicing - O(1)</li> </ul>"},{"location":"tutorial/data/#sample","title":"Sample","text":"<p><code>Sample</code> is a mapping-like container of <code>Item</code> objects. If dataset were tables (as in a SQL database), samples would be individual rows. Contrary to samples in a dataset, items in a sample do not have any ordering relationship and instead of being indexed with an integer, they are indexed by key.</p> <p>Note</p> <p>A <code>Sample</code> is an immutable Python Mapping, supporting all its methods.</p> <p>Example</p> <p>Let's see an example on how to use a <code>Sample</code> object as a python mapping:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Get the number of items inside the sample\nnumber_of_items = len(sample) \n\n# Retrieve an item named \"image\".\n# This does not return the actual image, but merely an Item that has access to it.\n# This will be explained in detail later.\nitem_image = sample[\"image\"] \n\n# Retrieve an item named \"metadata\"\nitem_metadata = sample[\"metadata\"]\n\n# Iterate on all keys\nfor key in sample.keys():\n    ...\n\n# Iterate on all items\nfor item in sample.values():\n    ...\n\n# Iterate on all key-item pairs\nfor key, item in sample.items():\n    ...\n</code></pre> <p>In addition to all <code>Mapping</code> methods, <code>Sample</code> provides a set of utility methods to create modified copies (samples are immutable) where new items are added, removed or have their content replaced by new values.</p> <p>Example</p> <p>Example showing how to manipulate <code>Sample</code> objects using utility methods:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Add/Replace the item named \"image\" with another item\nnew_sample = sample.with_item(\"image\", new_image_item)\n\n# Add/Replace multiple items at once\nnew_sample = sample.with_items(image=new_image_item, metadata=new_metadata_item)\n\n# Replace the contents of the item named \"image\" with new data\nnew_sample = sample.with_value(\"image\", np.array([[[...]]]))\n\n# Replace the contents of multiple items at once\nnew_sample = sample.with_values(image=np.array([[[...]]]), metadata={\"foo\": 42})\n\n# Remove one or more items\nnew_sample = sample.without(\"image\")\nnew_sample = sample.without(\"image\", \"metadata\") \n\n# Remove everything but one or more items\nnew_sample = sample.with_only(\"image\")\nnew_sample = sample.with_only(\"image\", \"metadata\")\n\n# Rename items\nnew_sample = sample.remap({\"image\": \"THE_IMAGE\", \"metadata\": \"THE_METADATA\"})\n</code></pre> <p>In contrast with <code>Datasets</code>, pipewine does not offer a lazy version of samples, meaning that the all items are always kept memorized. Usually, you want to keep the number of items per sample bound to a constant number.</p> <p>Pipewine provides two main <code>Sample</code> implementations that differ in the way they handle typing information.</p>"},{"location":"tutorial/data/#typelesssample","title":"TypelessSample","text":"<p>The most basic type of <code>Sample</code> is <code>TypelessSample</code>, akin to the old Pipelime <code>Sample</code>.  This class is basically a wrapper around a dictionary of items of unknown type.</p> <p>When using <code>TypelessSample</code> it's your responsibility to know what is the type of each item, meaning that if you access an item you then have to cast it to the expected type.</p> <p>With the old Pipelime, this quickly became a problem and lead to the creation of <code>Entity</code> and <code>Action</code> classes, that provide a type-safe alternative, but unfortunately integrate poorly with the rest of the library, failing to completely remove the need for casts or <code>type: ignore</code> directives. </p> <p>Example</p> <p>The type-checker fails to infer the type of the retrieved item: <pre><code>sample = TypelessSample(**dictionary_of_items)\n\n# When accessing the \"image\" item, the type checker cannot possibly know that the \n# item named \"image\" is an item that accesses an image represented by a numpy array.\nimage_item = sample[\"image\"]\n\n# Thus the need for casting (type-unsafe)\nimage_data = cast(np.ndarray, image_item())\n</code></pre></p> <p>Despite this limitation, <code>TypelessSample</code> allow you to use Pipewine in a quick-and-dirty way that allows for faster experimentation without worrying about type-safety.</p> <p>Example</p> <p>At any moment, you can convert any <code>Sample</code> into a <code>TypelessSample</code>, dropping all typing information by calling the <code>typeless</code> method:</p> <pre><code>sample: MySample\n\n# Construct a typeless copy of the sample\ntl_sample = sample.typeless()\n</code></pre>"},{"location":"tutorial/data/#typedsample","title":"TypedSample","text":"<p><code>TypedSample</code> is the type-safe alternative for samples. It allows you to construct samples that retain information on the type of each item contained within them, making your static type-checker happy.</p> <p><code>TypedSample</code> on its own does not do anything, to use it you always need to define a class that defines the names and the type of the items. This process is very similar to the definition of a Python dataclass, with minimal boilerplate.</p> <p>What you get in return:</p> <ul> <li>No need for <code>t.cast</code> or <code>type: ignore</code> directives that make your code cluttered and error-prone.</li> <li>The type-checker will complain when something is wrong with the way you use your <code>TypedSample</code>, effectively preventing many potential bugs.</li> <li>Intellisense automatically suggests field and method names for auto-completion.</li> <li>Want to rename an item? Any modern IDE is able to quickly rename all occurrences of a <code>TypedSample</code> field without breaking anything.</li> </ul> <p>Example</p> <p>Example creation and usage of a custom <code>TypedSample</code>:</p> <pre><code>class MySample(TypedSample):\n    image_left: Item[np.ndarray]\n    image_right: Item[np.ndarray]\n    category: Item[str]\n\nmy_sample = MySample(\n    image_left=image_left_item,\n    image_right=image_right_item,\n    category=category_item,\n)\n\nimage_left_item = my_sample.image_left # Type-checker infers type Item[np.ndarray]\nimage_left_item = my_sample[\"image_left\"] # Equivalent type-unsafe\n\nimage_right_item = my_sample.image_right # Type-checker infers type Item[np.ndarray]\nimage_right_item = my_sample[\"image_right\"] # Equivalent type-unsafe\n\ncategory_item = my_sample.category # Type-checker infers type Item[str]\ncategory_item = my_sample[\"category\"] # Equivalent type-unsafe\n</code></pre> <p>Warning</p> <p>Beware of naming conflicts when using <code>TypedSample</code>. You should avoid item names conflicting with the methods of the <code>Sample</code> class.  </p>"},{"location":"tutorial/data/#item","title":"Item","text":"<p><code>Item</code> objects represent a single serializable unit of data. They are not the data itself, instead, they only have access to the underlying data.</p> <p>Items do not implement any specific Python abstract type, since they are at the lowest level of the hierarchy and do not need to manage any collection of objects.</p> <p>All items can be provided with typing information about the type of the data they have access to. This enables the type-checker to automatically infer the type of the data when accessed. </p> <p>All <code>Item</code> objects have a <code>Parser</code> inside of them, an object that is responsible to encode/decode the data when reading or writing. These <code>Parser</code> objects are detailed later on.</p> <p>Furthermore, items can be flagged as \"shared\", enabling Pipelime to perform some optimizations when reading/writing them, but essentially leaving their behavior unchanged.</p> <p>Example</p> <p>Example usage of an <code>Item</code>:</p> <pre><code># Given an item that accesses a string\nitem: Item[str]\n\n# Get the actual data by calling the item ()\nactual_data = item()\n\n# Create a copy of the item with data replaced by something else\nnew_item = item.with_value(\"new_string\")\n\n# Get the parser of the item\nparser = item.parser\n\n# Create a copy of the item with another parser\nnew_item = item.with_parser(new_parser)\n\n# Get the sharedness of the the item\nis_shared = item.is_shared\n\n# Set the item as shared/unshared\nnew_item = item.with_sharedness(True)\n</code></pre> <p>Pipewine provides three <code>Item</code> variants, that differ in the way data is accessed or stored.</p>"},{"location":"tutorial/data/#memoryitem","title":"MemoryItem","text":"<p><code>MemoryItem</code> instances are items that directly contain data they are associated with. Accessing data is immediate as it is always loaded in memory and ready to be returned.</p> <p>Tip</p> <p>Use <code>MemoryItem</code> to contain \"new\" data that is the result of a computation. E.g. the output of a complex DL model.</p> <p>Example</p> <p>To create a <code>MemoryItem</code>, you just need to pass the data as-is and the <code>Parser</code> object:</p> <pre><code># Given a numpy array representing an image that is the output of an expensive\n# computation\nimage_data = np.array([[[...]]])\n\n# Create a MemoryItem that contains the data and explicitly tells Pipewine to always\n# encode the data as a JPEG image.\nmy_item = MemoryItem(image_data, JpegParser())\n</code></pre>"},{"location":"tutorial/data/#storeditem","title":"StoredItem","text":"<p><code>StoredItem</code> instances are items that point to external data stored elsewhere. Upon calling the item, the data is read from the storage, parsed and returned. </p> <p><code>StoredItem</code> objects use both <code>Parser</code> and <code>Reader</code> objects to retrieve the data. A <code>Reader</code> is an object that exposes a <code>read</code> method that returns data as bytes.</p> <p>Currently Pipewine provides a <code>Reader</code> for locally available files called <code>LocalFileReader</code>, that essentially all it does is <code>open(path, \"rb\").read()</code>.</p> <p>Tip</p> <p>Use <code>StoredItem</code> to contain data that is yet to be loaded. E.g. when creating a dataset that reads from a DB, do not perform all the loading upfront, use <code>StoredItem</code> to lazily load the data only when requested.</p> <p>Example</p> <p>To create a <code>StoredItem</code>, you need to </p> <pre><code># The reader object responsible for reading the data as bytes\nreader = LocalFileReader(Path(\"/my/file.png\"))\n\n# Create a StoredItem that is able able to read and parse the data when requested.\nmy_item = StoredItem(reader, PngParser())\n</code></pre> <p>Warning</p> <p>Contrary to old Pipelime items, <code>StoredItem</code> do not offer any kind of automatic caching mechanism: if you retrieve the data multiple times, you will perform a full read each time. </p> <p>To counteract this, you need to use Pipewine cache operations. </p>"},{"location":"tutorial/data/#cacheditem","title":"CachedItem","text":"<p><code>CachedItem</code> objects are items that offer a caching mechanism to avoid calling expensive read operations multiple times when the underlying data is left unchanged. </p> <p>Warning</p> <p>Caching everything at the item level is an exceptionally bad idea that is guaranteed to fill up your system memory in no time. Instead of using <code>CachedItem</code> directly, use Pipewine <code>CacheOp</code> operations that provide a smarter mechanism that mixes sample-level and item-level caches. </p> <p>This is an important lesson learnt from the old Pipelime library, where caching was always done at the item level and was enabled by default. </p> <p>To create a <code>CachedItem</code>, you just need to pass an <code>Item</code> of your choice to the <code>CachedItem</code> constructor.</p> <p>Example</p> <p>Example usage of a <code>CachedItem</code>:</p> <pre><code># Suppose we have an item that reads a high resolution BMP image from an old HDD. \nreader = LocalFileReader(Path(\"/extremely/large/file.bmp\"))\nitem = StoredItem(reader, BmpParser())\n\n# Reading data takes ages, and does not get faster if done multiple times.\ndata1 = item() # Slow\ndata2 = item() # Slow\ndata3 = item() # Slow\n\n# With CachedItem, we can memoize the data after the first access, making subsequent\n# accesses immediate\ncached_item = CachedItem(item)\n\ndata1 = cached_item() # Slow\ndata2 = cached_item() # Fast\ndata3 = cached_item() # Fast\n</code></pre>"},{"location":"tutorial/data/#parser","title":"Parser","text":"<p>Pipewine <code>Parser</code> objects are responsible for implementing the serialization/deserialization functions for data:</p> <ul> <li><code>parse</code> transforms bytes into python objects of your choice.</li> <li><code>dump</code> transforms python objects into bytes.</li> </ul>"},{"location":"tutorial/data/#built-in-parsers","title":"Built-in Parsers","text":"<p>Pipewine has some built-in parsers for commonly used data encodings: </p> <ul> <li> <p><code>PickleParser</code>: de/serializes data using Pickle, a binary protocol that can be used to de/serialize most Python objects. Key pros/cons:</p> <ul> <li>\u2705 <code>pickle</code> can efficiently serialize pretty much any python object.</li> <li>\u274c <code>pickle</code> is not secure: you can end up executing malicious code when reading data. </li> <li>\u274c <code>pickle</code> only works with Python, preventing interoperability with other systems.</li> <li>\u274c There are no guarantees that <code>pickle</code> data written today can be correctly read by future python interpreters.  </li> </ul> </li> <li> <p><code>JSONParser</code> and <code>YAMLParser</code> de/serializes data using JSON or YAML, two popular human-readable data serialization languages that support tree-like structures of data that strongly resemble Python builtin types.</p> <ul> <li>\u2705 Both JSON and YAML are interoperable with many existing systems.</li> <li>\u2705 Both JSON and YAML are standard formats that guarantee backward compatibility.</li> <li>\u26a0\ufe0f JSON and YAML only support a limited set of types such as <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>dict</code>, <code>list</code>. </li> <li>\u2705 <code>JSONParser</code> and <code>YAMLParser</code> interoperate with pydantic <code>BaseModel</code> objects, automatically calling pydantic parsing, validation and dumping when reding/writing. </li> <li>\u274c Both JSON and YAML trade efficiency off for human readability. You may want to use different formats when dealing with large data that you don't care to manually read.</li> </ul> </li> <li> <p><code>NumpyNpyParser</code> de/serializes numpy arrays into binary files. </p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type</li> <li>\u274c Only works with Python and Numpy.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> </ul> </li> <li> <p><code>TiffParser</code> de/serializes numpy arrays into TIFF files.</p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type.</li> <li>\u2705 Produces files that can be read outside of Python.</li> <li>\u2705 Applies zlib lossless compression to reduce the file size. </li> </ul> </li> <li> <p><code>BmpParser</code> de/serializes numpy arrays into BMP files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> <li>\u2705 Fast de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>PngParser</code> de/serializes numpy arrays into PNG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u2705 Produces smaller files due to image compression. </li> <li>\u274c Slow de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>JpegParser</code> de/serializes numpy arrays into JPEG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale and RGB uint8 images.</li> <li>\u2705 Produces very small files due to image compression. </li> <li>\u2705 Fast de/serialization.</li> <li>\u274c Lossy.</li> </ul> </li> </ul>"},{"location":"tutorial/data/#custom-parsers","title":"Custom Parsers","text":"<p>With Pipewine you are not limited to use the built-in Parsers, you can implement your own and use it seamlessly as if it were provided by the library.</p> <p>Example</p> <p>Let's create a <code>TrimeshParser</code> that is able to handle 3D meshes using the popular library Trimesh</p> <pre><code>class TrimeshParser(Parser[tm.Trimesh]):\n    def parse(self, data: bytes) -&gt; tm.Trimesh:\n        # Create a binary buffer with the binary data\n        buffer = io.BytesIO(data)\n\n        # Read the buffer and let trimesh load the 3D mesh object\n        return tm.load(buffer, file_type=\"obj\")\n\n    def dump(self, data: tm.Trimesh) -&gt; bytes:\n        # Create an empty buffer\n        buffer = io.BytesIO()\n\n        # Export the mesh to the buffer\n        data.export(buffer, file_type=\"obj\")\n\n        # Return the contents of the buffer\n        return buffer.read()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        # This tells pipewine that it can automatically use this parses whenever a \n        # file with .obj extension is found and needs to be parsed.\n        return [\"obj\"]\n</code></pre>"},{"location":"tutorial/data/#immutability","title":"Immutability","text":"<p>All data model types are immutable. Their inner state is hidden in private fields and methods and should never be modified in-place. Instead, they provide public methods that return copies with altered values, leaving the original object intact.</p> <p>With immutability, a design decision inherited by the old Pipelime, we can be certain that every object is in the correct state everytime, since it cannot possibly change, and this prevents many issues when the same function is run multiple times, possibly in non-deterministic order.</p> <p>Example</p> <p>Let's say you have a sample containing an item named <code>image</code> with an RGB image. You want to resize the image reducing the resolution to 50% of the original size.</p> <p>To change the image in a sample, you need to create a new sample in which the <code>image</code> item contains the resized image.</p> <pre><code>def half_res(image: np.ndarray) -&gt; np.ndarray:\n    # Some code that downscales an image by 50%\n    ...\n\n# Read the image (more details later)\nimage = sample[\"image\"]()\n\n# Downscale the image\nhalf_image = half_res(image)\n\n# Create a new sample with the new (downscaled) image\nnew_sample = sample.with_value(\"image\", half_image)\n</code></pre> <p>At the end of the snippet above, the <code>sample</code> variable will still contain the original full-size image. Instead, <code>new_sample</code> will contain the new resized image.</p> <p>There are only two exceptions to this immutability rule:</p> <ol> <li>Caches: They need to change their state to save time when the result of a computation is already known. Since all other data is immutable, caches never need to be invalidated.</li> <li>Inner data: While all pipewine data objects are immutable, this may not be true for the data contained within them. If your item contains mutable objects, you are able to modify them implace. But never do that! </li> </ol> <p>Python, unlike other languages, has no mechanism to enforce read-only access to an object, the only way to do so would be to perform a deep-copy whenever an object is accessed, but that would be a complete disaster performance-wise.</p> <p>So, when dealing with mutable data structures inside your items, make sure you either:</p> <ul> <li>Access the data without applying changes.</li> <li>Create a deep copy of the data before applying in-place changes.</li> </ul> <p>Danger</p> <p>Never do this!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Apply some in-place changes to image\nimage += 1\nimage *= 0.9 \nimage += 1   \n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present both in the old and new sample, violating the immutability rule.</p> <p>Success</p> <p>Do this instead!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Create a copy of the image with modified data\nimage = image + 1\n\n# Since image is now a copy of the original data, you can now apply all \n# the in-place changes you like now. \nimage *= 0.9 # Perfectly safe\nimage += 1   # Perfectly safe\n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present only in the new sample.</p>"},{"location":"tutorial/installation/","title":"\ud83d\udee0\ufe0f Installation","text":""},{"location":"tutorial/installation/#basic-installation","title":"Basic Installation","text":"<p>Before installing make sure you have:</p> <ul> <li>A Python3.12+ interpreter installed on your system.</li> <li>A Virtual Environment, which is highly recommended to avoid messing up your system-level python environment.</li> <li> <p>A relatively up-to-date version of <code>pip</code>. You can upgrade to the latest version using</p> <pre><code>pip install -U pip\n</code></pre> </li> </ul> <p>To install Pipewine, run:</p> <pre><code>pip install pipewine\n</code></pre> <p>You can then verify whether <code>pipewine</code> was correctly installed by calling the CLI:</p> <pre><code>pipewine --version\n</code></pre> <p>Success</p> <p>In case the installation is successful, you should see the version of the current Pipewine installation, e.g:</p> <pre><code>0.1.0\n</code></pre> <p>Failure</p> <p>In case something went wrong, you should see something like (this may vary based on your shell type):</p> <pre><code>bash: command not found: pipewine\n</code></pre> <p>In this case, do the following:</p> <ol> <li>Check for any <code>pip</code> error messages during installation.</li> <li>Go back through all steps and check whether you followed them correctly.</li> <li>Open a GitHub issue describing the installation problem. </li> </ol>"},{"location":"tutorial/installation/#dev-installation","title":"Dev Installation","text":"<p>If you are a dev and want to install Pipewine for development purposes, it's recommended you follow these steps instead:</p> <ol> <li>Clone the github repo in a folder of your choice:     <pre><code>git clone https://github.com/lucabonfiglioli/pipewine.git\ncd pipewine\n</code></pre></li> <li>Create a new virtual environment:     <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate \n</code></pre></li> <li>Update pip:     <pre><code>pip install -U pip\n</code></pre></li> <li>Install pipewine in edit mode:     <pre><code>pip install -e .\n</code></pre></li> <li> <p>Install pipewine optional dependencies     <pre><code>pip install .[dev] .[docs]\n</code></pre></p> <p>Warning</p> <p>With some shells (like <code>zsh</code>), you may need to escape the square brackets e.g: <code>.\\[dev\\]</code> or <code>.\\[docs\\]</code>.</p> </li> </ol>"},{"location":"tutorial/overview/","title":"\ud83d\udca0 Overview","text":""},{"location":"tutorial/overview/#high-level","title":"High Level","text":"<p>Pipewine provides you with tools to help decouple what you do with data from the way data is represented and stored. It does so by providing a set of abstractions for many aspects of your data pipeline:</p> <ul> <li><code>Dataset</code>, <code>Sample</code>, <code>Item</code> define how the data is structured, how many data samples are there, in which order, what is their content, how are they accessed etc...</li> <li>More low-level abstractions such as <code>Parser</code> and <code>Reader</code> define how data is encoded and stored. </li> <li><code>DatasetSource</code>, <code>DatasetSink</code>, <code>DatasetOperator</code> define how the data is read, written and transformed, and consistitute the base building blocks for workflows.</li> <li><code>Workflow</code> defines how a set of operators are interconnected. They can be seen as DAGs (Directed Acyclic Graph) in which nodes are sources, sinks or operators, and edges are datasets. </li> </ul> <p>All of these components are designed to allow the user to easily create custom implementations that can be seamlessly integrated with all the built-in blocks.</p> <p>By doing so, Pipewine (much like Pipelime) encourages you to write components that are likely to be highly re-usable.</p>"},{"location":"tutorial/overview/#extendibility","title":"Extendibility","text":"<p>Pipewine is completely agnostic on the following aspects of your data:</p> <ul> <li>Storage location: you can store data anywhere you want, on the file system, on a DB of your choice, on the device memory, on a remote source. You just need to implement the necessary components. </li> <li>Data encoding: By default Pipewine supports some popular image encodings, JSON/YAML metadata, numpy encoding for array data and Pickle encoding for generic python objects. You can easily add custom encodings to read/write data as you like.</li> <li>Data format: By default Pipewine supports the same built-in dataset format as Pipelime, a file system based format called \"Underfolder\" that is flexible to most use-cases but has a few limitations. Dataset formats are highly dependent on the application, thus Pipewine allows you to fully take control on how to structure your datasets.</li> <li>Data operators: As mentioned previously, you can define custom operators that do all sorts of things with your data. Built-in operators cover some common things you may want to do at some point such as concatenating two or more datasets, filtering samples based on a criterion, splitting datasets into smaller chunks, apply the same function (called <code>Mapper</code>) to all samples of a dataset.  </li> </ul>"},{"location":"tutorial/overview/#a-note-on-performance","title":"A Note on Performance","text":"<p>Pipewine is a python package and it's currently 100% python, therefore it's certainly going to be orders of magnitude slower than it could be if written in another language.</p> <p>Having said that, Pipewine still tries its best to maximize efficiency by leveraging:</p> <ul> <li>Caching: Results of computations can be cached to avoid being computed multiple times. This was also done by Pipelime, but they way cache works underwent many changes in the rewrite.</li> <li>Parallelism: Many operations are automatically run in parallel with a multi-processing pool of workers. </li> <li>Linking: When writing to file system, Pipewine automatically attempts to leverage hard-links where possible to avoid serializing and writing the same file multiple times.</li> <li>Vectorization: Where possible, Pipewine uses Numpy to perform vectorized computation on batches of data, achieving better performance if compared to plain python code.</li> </ul> <p>Furthermore, when performing complex operations such as image processing, inference with AI models, 3D data processing, the performance overhead of Pipewine will likely become negligible if compared to the complexity of the individual operations.</p>"},{"location":"tutorial/overview/#a-note-on-scalability","title":"A Note on Scalability","text":"<p>Pipewine - and its predecessor Pipelime - are meant to quickly let you manipulate data without either having to:</p> <ul> <li>Coding everything from scratch and come up with meaningful abstractions yourself. </li> <li>Setting up complex and expensive frameworks that can run data pipelines on distributed systems with many nodes.</li> </ul> <p>Warning</p> <p>If you are running data pipelines on petabytes of data, in distributed systems, with strong consistency requirements and the need for data replication at each step, Pipewine is not what you are looking for.</p> <p>Success</p> <p>If you need to run data pipelines on small/medium datasets (in the order of gigabytes) and want a flexible tool to help you do that, then Pipewine might be what you are looking for.</p>"},{"location":"tutorial/workflows/","title":"\u267b\ufe0f Workflows","text":""}]}