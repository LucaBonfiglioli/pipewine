{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pipewine","text":"<p>Pipewine is a complete rewrite of the Pipelime library and intends to serve the same purpose: provide a set of tools to manipulate multi-modal small/medium-sized datasets, mainly for research purposes in the CV/ML domain.</p>"},{"location":"#features","title":"\ud83d\ude80 Features","text":"<ul> <li>Unified access pattern to datasets of various formats, origin and content.</li> <li>Underfolder, a quick and easy filesystem-based dataset format good for small/medium datasets.</li> <li>Common data encoding formats for images, text and metadata.</li> <li>Common operators to manipulate existing datasets.</li> <li>Workflows that transform data in complex DAGs (Directed Acyclic Graph) pipelines.</li> <li>CLI (Command Line Interface) to quickly run simple workflows without writing a full python script.</li> <li>Extendibility, allowing the user to easily extend the library in many aspects, adding components that seamlessly integrate with the built-in ones:<ul> <li>Add custom dataset formats</li> <li>Add custom data encoding formats</li> <li>Add custom operators on datasets</li> <li>Register components to the CLI</li> </ul> </li> </ul>"},{"location":"#rationale","title":"\u2b50\ufe0f Rationale","text":"<p>Pipewine started from a refactoring of some core components of the Pipelime library, but it soon turned into a complete rewrite that aims at solving some architectural issues of Pipelime, namely:</p> <ul> <li>Dependency Hell: Pipelime had so many of dependencies and contstraints that made it very hard to install in many environments due to conflicts and incompatibilities. Outdated dependencies at the core of the library (e.g. Pydantic 1) make it incompatible with new python packages that use Pydantic 2+. Upgrading these dependencies is currently impossible without breaking everything.</li> <li>Over-reliance on Pydantic: Making most library components inherit from Pydantic <code>BaseModel</code>'s (expecially with the 1.0 major) completely violate type-safety and result in type-checking errors if used with any modern python type-checker like MyPy or PyLance. As a result, IntelliSense and autocompletion do not work properly, requiring the user to fill their code with <code>type: ignore</code> directives. Furthermore, Pydantic runtime validation has some serious performance issues, slowing down the computation significantly (especially before rewriting the whole library in Rust).</li> <li>Lack of typed abstractions: Many components of Pipelime (e.g. <code>SamplesSequence</code>, <code>Sample</code>, <code>SampleStage</code>, <code>PipedSequence</code> and <code>PipelimeCommand</code>) do not carry any information on the type of data they operate on, thus making the usage of Pipelime error-prone: the user cannot rely on the type checker to know what type of data is currently processing. Some later components like <code>Entity</code> and <code>Action</code> aim at mitigating this problem by encapsulating samples inside a Pydantic model, but they ended up being almost unused because they integrate poorly with the rest of the library.</li> <li>Confusing CLI: Pipelime CLI was built with the purpose of being able to directly instantiate any (possibly nested) Pydantic model. This resulted in a very powerful CLI that could do pretty much anything but was also very confusing to use (and even worse to maintain). As a contributor of the pipelime library, I never met anybody able to read and understand the CLI validation errors.</li> <li>Everything is CLI: All Pipelime components are also CLI components. This has the positive effect of eliminating the need to manually write CLI hooks for data operators, but also couples the CLI with the API, requiring everything to be serializable as a list of key-value pairs of (possibly nested) builtin values. We solved this using Pydantic, but was it really worth the cost?</li> <li>Feature Creep: Lots of features like streams, remotes, choixe add a lot of complexity but are mostly unused. </li> </ul> <p>Key development decisions behind Pipewine:</p> <ul> <li>Minimal Dependencies: Rely on the standard library as much as possible, only depend on widely-used and maintained 3rd-party libraries. </li> <li>Type-Safety: The library heavily relies on Python type annotations to achieve a desirable level of type safety at development-time. Runtime type checking is limited to external data validation to not hinder the performance too much. The user should be able to rely on any modern static type checker to notice and correct bugs. </li> <li>Pydantic: Limit the use of Pydantic for stuff that is not strictly external data validation. When serialization and runtime validation are not needed, plain dataclasses are a perfect alternative.</li> <li>CLI Segregation: The CLI is merely a tool to quickly access some of the core library functionalities, no core component should ever depend on it. </li> <li>Limited Compatibility Pipewine should be able to read data written by Pipelime and potentially be used alonside it, but it is not intended to be a backward-compatible update, it is in fact a separate project with a separate development cycle.</li> <li>Feature Pruning Avoid including complex features that no one is going to use, instead, focus on keeping the library easy to extend. </li> </ul>"},{"location":"tutorial/actions/","title":"\u2699\ufe0f Actions","text":""},{"location":"tutorial/actions/#overview","title":"Overview","text":"<p>In this section you will learn what are the main operational abstractions that define how Pipewine transforms data, how to interact with them and how to build re-usable building blocks for your data pipelines.</p> <p>Note</p> <p>This section assumes you already know the basics on the way the data model is structured. If you haven't already, go take a look at the Data Model section. </p> <p>The main components that are explained in this section include:</p> <ul> <li>Sources - Action that creates Pipewine datasets from external storages.</li> <li>Sinks - Action that saves Pipewine datasets to external storages.</li> <li>Operators - Action that transforms Pipewine dataset/s into Pipewine dataset/s.</li> <li>Mappers - Applies the same operation to every sample of a dataset.</li> <li>Grabber - Distributes work to a multi-processing pool of workers.</li> </ul>"},{"location":"tutorial/actions/#types-of-actions","title":"Types of Actions","text":"<p>Pipewine uses the broad term \"Action\" to denote any of the following components:</p> <ul> <li><code>DatasetSource</code> - A component that is able to create one or more <code>Dataset</code> objects.  </li> <li><code>DatasetSink</code> - A component that is able to consume one or more <code>Dataset</code> objects. </li> <li><code>DatasetOperator</code> - A component that accepts and returns one or more <code>Dataset</code> objects.</li> </ul> <p>In general, actions can accept and return one of the following:</p> <ul> <li>A single <code>Dataset</code> object.</li> <li>A <code>Sequence</code> of datasets.</li> <li>A <code>tuple</code> of datasets.</li> <li>A <code>Mapping</code> of strings to datasets.</li> <li>A <code>Bundle</code> of datasets.</li> </ul> <p>Specific types of actions statically declare the type of inputs and outputs they accept/return to ensure that their type is always known at development time (as soon as you write the code).</p> <p>Example</p> <p><code>CatOp</code> is a type of <code>DatasetOperation</code> that concatenates one or more datasets into a single one, therefore, its input type is <code>Sequence[Dataset]</code> and its output is <code>Dataset</code>.</p> <p>Any modern static type checker such as <code>mypy</code> or <code>PyLance</code>  will complain if you try to pass anything else to it, preventing such errors at dev time.</p> <p>Tip</p> <p>As a general rule of thumb of what type of input/output to choose when implementing custom actions:</p> <ul> <li>Use <code>Dataset</code> when you want to force that the action accepts/returns exactly one dataset.</li> <li>Use <code>Sequence</code> when you need the operation to accept/return more than one dataset, but you don't know a priory how many, their order or their type.</li> <li>Use <code>tuple</code> in settings similar to the <code>Sequence</code> case but you also know at dev time the exact number of datasets, their order and their type. Tuples, contrary to <code>Sequences</code>, also allow you to specify the type of each individual element.</li> <li>Use <code>Mapping</code> when you need to accept/return collections of datasets that do not have any ordering relationship, but are instead associated with an arbitrary (not known a priori) set of string keys.</li> <li>Use <code>Bundle</code> in settings similar to the <code>Mapping</code> case but you also know at dev time the exact number of datasets, the name of their keys and their type. More details on <code>Bundle</code> in the next sub-section. </li> </ul>"},{"location":"tutorial/actions/#bundle-objects","title":"Bundle objects","text":"<p>Pipewine <code>Bundle[T]</code> are objects that have the following characteristics:</p> <ul> <li>They behave like a Python <code>dataclass</code>.</li> <li>The type of all fields is bound to <code>T</code>.</li> <li>Field names and types are statically known and cannot be modified.</li> <li>They are anemic objects: they only act as a data container and define no methods.</li> </ul> <p>Note</p> <p>You may already have encountered the <code>Bundle</code> type without noticing when we introduced the TypedSample. <code>TypedSample</code> is a <code>Bundle[Item]</code>.</p> <p>Example</p> <p>Creating a <code>Bundle</code> is super easy:</p> <pre><code># Inherit from Bundle[T] and declare some fields\nclass MyBundle(Bundle[str]):\n    foo: str\n    bar: str\n\n# Constructs like a dataclass\nmy_bundle = MyBundle(foo=\"hello\", bar=\"world\")\n\n# Access content with dot notation\nmy_bundle.foo # &gt;&gt;&gt; hello\nmy_bundle.bar # &gt;&gt;&gt; world\n\n# Convert to a plain dict[str, T] when needed\nmy_bundle.as_dict() # &gt;&gt;&gt; {\"foo\": \"hello\", \"bar\": \"world\"}\n\n# Create from a dictionary\nmy_bundle_2 = MyBundle.from_dict({\"foo\": \"hello\", \"bar\": \"world\"})\n</code></pre> <p>Note</p> <p>Despite their similarity <code>Bundle</code> objects do not rely on Pydantic and do not inherit from <code>BaseModel</code>. Do not expect them to provide validation, parsing or dumping functionalities, as they are essentially dataclasses plus some constraints.</p>"},{"location":"tutorial/actions/#eagerlazy-behavior","title":"Eager/Lazy Behavior","text":"<p>Suppose we need to load and display N images stored as files in a directory, comparing the two approaches:</p> <ul> <li>Eager - Every computation is carried out as soon as the necessary data is ready.</li> <li>Lazy - Every operation is performed only when it cannot be delayed anymore.</li> </ul> <p>Let's explore the trade-off between the two approaches:</p> Eager Lazy \u274c Long initialization time: all images must be loaded before the first one is displayed. \u2705 Short initialization time: no loading is done during initialization. \u2705 When the user requests an image, it is immediately available. \u274c When the user requests an image, it must wait for the application to load it before viewing it. \u274c High (and unbounded) memory usage, necessary to keep all images loaded at the same time. \u2705 Always keep at most one image loaded into memory. \u274c Risk of performing unnecessary computation: what if the user only wants to display a small subset of images? \u2705 If the user is not interested in a particular image, it won't be loaded at all. \u2705 Accessing an image multiple times has no additional cost. \u274c Accessing an image multiple times has a cost proportional to the number of times it is accessed. \u2705 Optimizations may be easier and more effective on batched operations. \u274c Optimizations are harder to perform or less effective on small batches accessed out-of-order. <p>As you can see, there are positive and negative aspects in both approaches. You may be inclined to think that for this silly example of loading and displaying images, the lazy approach is clearly better. After all, who would use a software that needs to load the entire photo album of thousands of photos just to display a single image?</p> <p>As you may have noticed in the previous section, Pipewine allows you to choose both approaches, but when there is no clear reason to prefer the eager behavior, its components are implemented so that they behave lazily: <code>Item</code> only reads data when requested to do so, <code>LazyDataset</code> creates the actual <code>Sample</code> object upon indexing. As you will see, most Pipewine actions follow a similar pattern.</p> <p>This decision, inherited from the old Pipelime library, is motivated mainly by the following reasons:</p> <ul> <li>Data pipelines may operate on large amounts of data that we cannot afford to load upfront and keep loaded into system memory. Even small datasets with hundreds of images will impose a significant memory cost on any modern machine. Larger datasets may not be possible to load at all!</li> <li>Until a time machine is invented, there is no way to undo wasteful computation. Suppose you have an image classification dataset and you want to select all samples with the category \"horse\": there is no point in loading an image once you know that it's not what you are looking for. Lazy behavior prevents this kind of unnecessary computation by design.</li> <li>The risk of performing some computation twice can be mitigated by using caches with the right eviction policy.</li> </ul>"},{"location":"tutorial/actions/#parallelism","title":"Parallelism","text":"<p>Pipewine provides you with a <code>Grabber</code> object, a simple tool allows you to iterate over a sequence with a parallel pool of multi-processing workers.</p> <p>Why multi-processing and not multi-threading? Because of GIL (Global Interpreter Lock), multi-threading in Python allows concurrency but not parallelism, effectively granting the same speed as a single-threaded program.</p> <p>To effectively parallelize our code we are left with two options:</p> <ol> <li>Avoid using Python, and instead write components in other languages such as C/C++, then create bindings that allow the usage from Python. </li> <li>Bite the bullet and use multi-processing instead of multi-threading. If compared with threads, processes are a lot more expensive to create and manage, often requiring to serialize and deserialize the whole execution stack upon creation.</li> </ol> <p>The second option is clearly better: </p> <ul> <li>You still get the option to parallelize work in a relatively easy way without having to integrate complex bindings for execution environments other than the Python interpreter. </li> <li>Process creation and communication overhead becomes negligible when dealing with large amounts of relatively independent jobs that don't need much synchronization. This is a relatively common scenario in data pipelines.</li> <li>It does not prevent you from implementing optimized multi-threaded components in other languages. Think about it: whenever you use tools like Numpy inside a Pipewine action, you are doing exactly this. </li> </ul>"},{"location":"tutorial/actions/#grabber","title":"Grabber","text":"<p>To parallelize tasks with Pipewine you can use a <code>Grabber</code> object: a simple wrapper around a multiprocessing <code>Pool</code> of workers iterate through an ordered sequence of objects.</p> <p>Note</p> <p><code>Grabber</code> objects can work on arbitrary sequences of objects, provided they are serializable.</p> <p>The <code>Grabber</code> object also does a few nice things for you:</p> <ul> <li>Manages the lifecycle of the underlying pool of processes.</li> <li>Handles exceptions freeing resources and returning the errors transparently to the parent process.</li> <li>Automatically invokes a callback function whenever a task is completed. The callback is run by the individual workers.</li> </ul> <p>When creating a <code>Grabber</code> you need to choose three main parameters that govern the execution:</p> <ul> <li><code>num_workers</code>: The number of concurrent workers. Ideally, in a magic world where all workload is perfectly distributed and communication overhead is zero, the total time is proportional to <code>num_jobs / num_workers</code>, so the higher the better. In reality, there are many factors that make the returns of adding more processes strongly diminishing, and sometimes even negative:<ul> <li>Work is not perfectly distributed and it's not complete until the slowest worker hasn't finished. </li> <li>Processes are expensive to create. With small enough workloads, a single process may actually be faster than a concurrent pool.</li> <li>Everytime an object is passed from one process to the other it needs to be serialized and then de-serialized. This can become quite expensive when dealing with large data structures.</li> <li>Processes need synchronization mechanisms that temporarily halt the computation.</li> <li>Sometimes the bottleneck is not the computation itself: if a single process reading data from the network completely saturates your bandwidth, adding more processes won't fix the issue.</li> <li>Sometimes multiprocessing can be much slower than single processing. A typical example is when concurrently writing to a mechanical HDD, causing it to waste a lot of time going back and forth the writing locations.  </li> <li>Your code may already be parallelized. Whenever you use libraries like Numpy, PyTorch, OpenCV (and many more), you are calling C/C++ bindings that run very efficient multi-threaded code outside of the Python interpreter, or even code that runs on devices other than your CPU (e.g. CUDA-enabled GPUs). Adding multiprocessing in these cases will only add overhead costs to something that already uses your computational resources nearly optimally.</li> <li>Due to memory constraints the number of processes you can keep running may be limited to a small number. E.g. if each process needs to keep loaded a 10GB chunk of data and the available memory is just 32GB, the maximum amount of processes you can run without swapping is 3. Adding a 4th process will make the system start swapping, greatly deteriorating the overall execution speed.</li> </ul> </li> <li><code>prefetch</code>: The number of tasks that are assigned to each worker whenever they are ready. It's easier to explain this with an analogy. Imagine you need to deliver 1000 products to customers and you have 4 couriers ready to deliver them. How inefficient would it be if every courier delivered one product at a time, returning to the warehouse whenever they complete a delivery? You would still parallelize work across 4 couriers, but would also incur in massive synchronization costs (the extra time it takes for the courier to return to the warehouse each time). A smarter solution would be to assign a larger batch of deliveries to each courier (e.g. 50) so that they would have to return to the warehouse less frequently. </li> <li><code>keep_order</code>: Sometimes, the order in which the operations are performed is not that relevant. Executing tasks out-of-order requires even less synchronization, usually resulting in faster overall execution.</li> </ul> <p>Let's see an example of how a <code>Grabber</code> works. </p> <p>Example</p> <p>We want to iterate through a sequence of objects that, whenever indexed, performs some expensive operation. In this example we simulate it using a <code>time.sleep()</code>.</p> <p>Here is our <code>SlowSequence</code>:</p> <pre><code>class SlowSequence(Sequence[int]):\n\"\"\"Return the numbers in range [A, B), slowly.\"\"\"\n\n    def __init__(self, start: int, stop: int) -&gt; None:\n        self._start = start\n        self._stop = stop\n\n    def __len__(self) -&gt; int:\n        return self._stop - self._start\n\n    @overload\n    def __getitem__(self, idx: int) -&gt; int: ...\n    @overload\n    def __getitem__(self, idx: slice) -&gt; \"SlowSequence\": ...\n    def __getitem__(self, idx: int | slice) -&gt; \"SlowSequence | int\":\n        # Let's not worry about slicing for now\n        if isinstance(idx, slice):\n            raise NotImplementedError()\n\n        # Raise if index is out of bounds\n        if idx &gt;= len(self):\n            raise IndexError(idx)\n\n        # Simulate a slow operation and return\n        time.sleep(0.1)\n        return idx + self._start\n</code></pre> <p>Let's create an instance of <code>SlowSequence</code> and define a callback function that we need to call every time we iterate on it:</p> <pre><code>def my_callback(index: int) -&gt; None:\n    print(f\"Callback {index} invoked by process {os.getpid()}\")\n\n# The sequence object of which we want to compute the sum\nsequence = SlowSequence(100, 200)\n</code></pre> <p>Next, let's try to compute the total sum of a <code>SlowSequence</code> with a simple for loop.</p> <pre><code># Compute the sum, and measure total running time\nt1 = time.perf_counter()\n\ntotal = 0\nfor i, x in enumerate(sequence):\n    my_callback(i)\n    total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 10.008610311000666\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total running time is roughly 10s, checks out, since every iteration takes approximately 0.1s and we iterate 100 times. The PID printed by the callback function is always the same, since all computation is performed by a single process.</p> <p>Let's do the same but with with a <code>Grabber</code> with 5 concurrent workers</p> <pre><code>t1 = time.perf_counter()\n\ntotal = 0\ngrabber = Grabber(num_workers=5)\nwith grabber(sequence, callback=my_callback) as par_sequence:\n    for i, x in par_sequence:\n        total += x\n\nt2 = time.perf_counter()\nprint(\"Time (s):\", t2 - t1) # &gt;&gt;&gt; Time (s): 2.011717862998921\nprint(\"Total:\", total)      # &gt;&gt;&gt; Total: 14950\n</code></pre> <p>The total time is now near to 2s, the result of evenly splitting among 5 parallel workers. As you can see from the console, the callback is being called by 5 different PIDs:</p> <pre><code>Callback 0 invoked by process 21219\nCallback 2 invoked by process 21220\nCallback 4 invoked by process 21221\nCallback 6 invoked by process 21222\nCallback 8 invoked by process 21223\n...\n</code></pre> <p>Important note: what <code>Grabber</code> just parallelizes the <code>__getitem__</code> method and the callback function. The body of the for loop (the summation) is not parallelized and it's executed by the parent process.</p>"},{"location":"tutorial/actions/#sources-and-sinks","title":"Sources and Sinks","text":"<p><code>DatasetSource</code> and <code>DatasetSink</code> are the two base classes for every Pipewine action that respectively reads or writes datasets (or collections of datasets) from/to external storages.</p> <p>Pipewine offers built-in support for Underfolder datasets, a flexible dataset format inherited by Pipelime that works well with many small-sized multi-modal datasets with arbitrary content. </p> <p>While Underfolder is the only format supported by Pipewine (currently), you are strongly encouraged to create custom dataset sources and sinks for whatever format you like.</p>"},{"location":"tutorial/actions/#underfolder","title":"Underfolder","text":"<p>An underfolder dataset is a directory located anywhere on the file system, with no constraint on its name. Every underfolder must contain a subfolder named <code>data</code>, plus some additional files.</p> <p>Every file contained in the <code>data</code> subfolder corresponds to an item and must be named: <code>$INDEX_$KEY.$EXTENSION</code>, where:</p> <ul> <li><code>$INDEX</code> is a non-negative integer number representing the index of the sample in the dataset, prefixed with an arbitrary number of <code>0</code> characters that are ignored. </li> <li><code>$KEY</code>  is a string representing the key of the item in the sample. Every name that can be used as a Python variable name is a valid key.</li> <li><code>$EXTENSION</code> is the file extension you would normally use. Pipewine can only read files if there is a registered <code>Parser</code> class that supports its extension.  </li> </ul> <p>Every file outside of the <code>data</code> folder (also called \"root file\") represents a shared item that every sample in the dataset will inherit.</p> <p>Warning</p> <p>All sample indices must be contiguous: if a sample with index N is present, then all the samples from 0 to N-1 must also be present in the dataset.</p> <p>Pros and cons of the Underfolder dataset format:</p> <ul> <li>\u2705 No need to setup databases, making them extremely easy to create and access. You can even create them by renaming a bunch of files using your favourite file explorer or a shell script.</li> <li>\u2705 You can easily inspect and edit the content of an underfolder dataset just by opening the individual files you are interested in with your favourite tools.</li> <li>\u2705 It's a very flexible format that you can use in many scenarios. </li> <li>\u274c Databases exist for a reason. Putting 100.000 files in a local directory and expecting it to be efficient is pure fantasy. You should avoid using underfolders when dealing with large datasets or when performance is critical.</li> <li>\u274c There is an issue when used with datasets where all items are shared (an edge case that's very rare in practice). In these cases, the original length of the dataset is lost after writing. Although fixing this issue is quite easy, it would break the forward-compatibility (reading data written by a future version of the library), so it will likely remain unfixed.</li> </ul> <p>Example</p> <p>You can see an example of an Underfolder dataset here. The dataset contains samples corresponding to the 26 letters of the english alphabet, with RGB images and metadata.</p> <p>Basic <code>UnderfolderSource</code> usage with no typing information:</p> <pre><code># Create the source object from an existing directory Path.\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\nsource = UnderfolderSource(path)\n\n# Call the source object to create a new Dataset instance.\ndataset = source()\n\n# Do stuff with the dataset\nsample = dataset[4]\nprint(sample[\"image\"]().reshape(-1, 3).mean(0))  # &gt;&gt;&gt; [244.4, 231.4, 221.7]\nprint(sample[\"metadata\"]()[\"color\"])  #            &gt;&gt;&gt; \"orange\"\nprint(sample[\"shared\"]()[\"vowels\"])  #             &gt;&gt;&gt; [\"a\", \"e\", \"i\", \"o\", \"u\"]\n</code></pre> <p>Fully typed usage:</p> <pre><code>class LetterMetadata(pydantic.BaseModel):\n    letter: str\n    color: str\n\nclass SharedMetadata(pydantic.BaseModel):\n    vowels: list[str]\n    consonants: list[str]\n\nclass LetterSample(TypedSample):\n    image: Item[np.ndarray]\n    metadata: Item[LetterMetadata]\n    shared: Item[SharedMetadata]\n\n# Create the source object from an existing directory Path.\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\nsource = UnderfolderSource(path, sample_type=LetterSample)\n\n# Call the source object to create a new Dataset instance.\ndataset = source()\n\n# Do stuff with the dataset\nsample = dataset[4]\nprint(sample.image().reshape(-1, 3).mean(0))  # &gt;&gt;&gt; [244.4, 231.4, 221.7]\nprint(sample.metadata().color)  #               &gt;&gt;&gt; \"orange\"\nprint(sample.shared().vowels)  #                &gt;&gt;&gt; [\"a\", \"e\", \"i\", \"o\", \"u\"]\n</code></pre> <p>You can use <code>UnderfolderSink</code> to write any Pipewine dataset, even if it wasn't previously read as an underfolder.</p> <pre><code># Write the dataset with an underfolder sink\noutput_path = Path(gettempdir()) / \"underfolder_write_example\"\nsink = UnderfolderSink(output_path)\nsink(dataset) # &lt;-- Writes data to the specified path.\n</code></pre>"},{"location":"tutorial/actions/#custom-formats","title":"Custom Formats","text":"<p>You can add custom implementations of the <code>DatasetSource</code> and <code>DatasetSink</code> interfaces that allow reading and writing datasets with custom formats. To better illustrate how to do so, this section will walk you through an example use case of a source and a sink for folders containing JPEG image files.</p> <p>Before you start, you should always think of the type of datasets you are going to read. Some formats may be flexible enough to support any kind of dataset, while others may be restricted to only a specific type. Pipewine gives you the tools to choose any of the two options in a type-safe way, but also to completely ignore all the typing part and to always return un-typed structures as it was with the old Pipelime.</p> <p>Example</p> <p>In this example case it's very simple: we only want to read samples that contain a single item named \"image\" that loads as a numpy array:</p> <pre><code>class ImageSample(TypedSample):\n    image: Item[np.ndarray]\n</code></pre> <p>Next, let's implement the dataset source:</p> <ol> <li>Inherit from <code>DatasetSource</code> and specify the type of data that will be read.</li> <li>[Optional] Implement an <code>__init__</code> method. </li> <li>Implement the <code>__call__</code> method, accepting no arguments and returning an instance of chosen type of dataset. You can choose to load the whole dataset upfront (eager), or to return a <code>LazyDataset</code> that will load the samples only when needed.</li> </ol> <p>Tip</p> <p>When overriding the <code>__init__</code> method, always remember to call <code>super().__init__()</code>, otherwise the object won't be correctly initialized.</p> <p>Example</p> <pre><code>class ImagesFolderSource(DatasetSource[Dataset[ImageSample]]):\n    def __init__(self, folder: Path) -&gt; None:\n        super().__init__()  # Always call the superclass constructor!\n        self._folder = folder\n        self._files: list[Path]\n\n    def __call__(self) -&gt; Dataset[ImageSample]:\n        # Find all JPEG files in the folder in lexicographic order.\n        jpeg_files = filter(lambda x: x.suffix == \".jpeg\", self._folder.iterdir())\n        self._files = sorted(list(jpeg_files))\n\n        # Return a lazy dataset thet loads the samples with the _get_sample method\n        return LazyDataset(len(self._files), self._get_sample)\n\n    def _get_sample(self, idx: int) -&gt; ImageSample:\n        # Create an Item that reads a JPEG image from the i-th file.\n        reader = LocalFileReader(self._files[idx])\n        parser = JpegParser()\n        image_item = StoredItem(reader, parser)\n\n        # Return an ImageSample that only contains the image item.\n        return ImageSample(image=image_item)\n</code></pre> <p>Next, let's implement the dataset sink, which is somewhat specular to the source:</p> <ol> <li>Inherit from <code>DatasetSink</code> and specify the type of data that will be written.</li> <li>[Optional] Implement an <code>__init__</code> method. </li> <li>Implement the <code>__call__</code> method, accepting an instance of the chosen type of dataset and returning nothing. </li> </ol> <p>Tip</p> <p>Since sinks are always placed at the end of pipelines, computation cannot be delayed any further, giving them no option but to loop over the data in an eager way inside the <code>__call__</code> method. We recommend doing this using the <code>self.loop</code> method with a grabber. Doing this has two advantages:</p> <ol> <li>It loops over the data using a grabber, meaning that if there are some lazy operations that still need to be computed, they can be run efficiently in parallel.</li> <li>It automatically invokes callbacks that send progress updates to whoever is listening to them, enabling live progress tracking in long-running jobs.</li> </ol> <p>Example</p> <pre><code>class ImagesFolderSink(DatasetSink[Dataset[ImageSample]]):\n    def __init__(self, folder: Path, grabber: Grabber | None = None) -&gt; None:\n        super().__init__()  # Always call the superclass constructor!\n        self._folder = folder\n        self._grabber = grabber or Grabber()\n\n    def __call__(self, data: Dataset[ImageSample]) -&gt; None:\n        self._folder.mkdir(parents=True, exist_ok=True)\n\n        # Compute the amount of 0-padding to preserve lexicographic order.\n        zpadding = len(str(len(data)))\n\n        # Iterate over the dataset and write each sample.\n        for i, sample in self.loop(data, self._grabber, name=\"Writing\"):\n            fname = self._folder / f\"{str(i).zfill(zpadding)}.jpeg\"\n            write_item_to_file(sample.image, fname)\n</code></pre> <p>Custom dataset sources and sinks can be registered to the Pipewine CLI to allow you to manipulate your own datasets using the same commands you would normally use for every other dataset format. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/actions/#operators","title":"Operators","text":"<p>We have seen how to read and write datasets using dataset sources and sinks, now it's time to see how to apply operations that transform datasets into other datasets. This is done by implementations of the <code>DatasetOperator</code> base class. </p> <p>Similarly to sources and sinks, operators must statically declare the type of data they accept as input and the one they return, for a total of 25 (5x5) possible combinations of input and output types.</p> <p>Example</p> <p>For example, a dataset operator that splits a single dataset into exactly 3 parts, should specify:</p> <ul> <li>Input type: <code>Dataset</code></li> <li>Output type: <code>tuple[Dataset, Dataset, Dataset]</code></li> </ul> <p>Operators are constructed as plain Python objects, then they behave like a Python <code>Callable</code> accepting and returning either a dataset or a collection of datasets.</p> <p>Example</p> <p>Here is an example of how to use a <code>DatasetOperator</code> that concatenates two or more datasets into a single one.</p> <pre><code># Given N datasets\ndataset1: Dataset\ndataset2: Dataset\ndataset3: Dataset\n\n# Construct the operator object\nconcatenate = CatOp()\n\n# Call the object\nfull_dataset = concatenate([dataset1, dataset2, dataset3])\n</code></pre> <p>Just like most Pipewine components, dataset operators can either be:</p> <ul> <li>Eager: performing all the computation when called and returning dataset/s containing the computation results. This is similar to the way old Pipelime's <code>PipelimeCommand</code> objects behave.</li> <li>Lazy: the call immediately returns a <code>LazyDataset</code> instance that performs the actual computation when requested. </li> </ul> <p>Tip</p> <p>In reality, it often makes sense to use a mix of the two approaches. You can perform some eager computation upfront then return a lazy dataset that uses the result of the previous computation.  </p>"},{"location":"tutorial/actions/#built-in-operators","title":"Built-in Operators","text":"<p>Pipewine has some useful generic operators that are commonly used in many workflows. Here is a list of the currently available built-in operators with a brief explanation. More in-depth documentation in the API reference.</p> <p>Iteration operators: operators that operate on single datasets changing their length or the order of samples.</p> <ul> <li><code>SliceOp</code>: return a slice (as in Python slices) of a dataset.</li> <li><code>RepeatOp</code>: replicate the samples of a dataset N times.</li> <li><code>CycleOp</code>: replicate the samples of a dataset until the desired number of samples is reached.</li> <li><code>IndexOp</code>: select a sub-sequence of samples of a dataset with arbitrary order.</li> <li><code>ReverseOp</code>: invert the order of samples in a dataset (same as <code>SliceOp(step=-1)</code>).</li> <li><code>PadOp</code>: extend a dataset to a desired length by replicating the i-th sample.</li> </ul> <p>Merge operators: operators that merge a collection of datasets into a single one.</p> <ul> <li><code>CatOp</code>: concatenate one or more datasets into a single dataset preserving the order of samples.</li> <li><code>ZipOp</code>: zip two or more dataset with the same length by merging the contents of the individual samples.</li> </ul> <p>Split operators: operators that split a single dataset into a collection of datasets.</p> <ul> <li><code>BatchOp</code>: split a dataset into many datasets of the desired size.</li> <li><code>ChunkOp</code>: split a dataset into a desired number of chunks (of approximately the same size).</li> <li><code>SplitOp</code>: split a dataset into an arbitrary amount of splits with arbitrary size.</li> </ul> <p>Functional operators: operators that transform datasets based on the result of a user-defined function.</p> <ul> <li><code>FilterOp</code>: keep only (or discard) samples that verify an arbitrary predicate.</li> <li><code>GroupByOp</code>: split a dataset grouping together samples that evaluate to the same value of a given function.</li> <li><code>SortOp</code>: sort a dataset with a user-defined sorting key function.</li> <li><code>MapOp</code>: apply a user-defined function (<code>Mapper</code>) to each sample of a dataset.</li> </ul> <p>Random operators: operators that apply non-deterministic random transformations.</p> <ul> <li><code>ShuffleOp</code>: sort the samples of a dataset in random order.</li> </ul> <p>Cache operators: operators that do not apply any transformation to the actual data, bu only change the way they are accessed.</p> <ul> <li><code>CacheOp</code>: adds a caching layer that memorizes samples to avoid computing them multiple times.</li> </ul>"},{"location":"tutorial/actions/#custom-operators","title":"Custom Operators","text":"<p>As with sources and sinks, you can implement your own operators that manipulate data as you wish. To help you understand how to do so, this section will walk you through the implementation of an example operator that normalizes images.</p> <p>To implement the dataset operator:</p> <ol> <li>Inherit from <code>DatasetOperator</code> and specify both the input and output types. You can choose to implement operators that work with specific types of data or to allow usage with arbitrary types.</li> <li>[Optional] Implement an <code>__init__</code> method.</li> <li>Implement the <code>__call__</code> method, accepting and returning the previously specified types of data. </li> </ol> <p>Example</p> <p>Here is the code for the <code>NormalizeOp</code>, an example operator that applies a channel-wise z-score normalization to all images of a dataset. </p> <p>The implementation is realatively naive: reading and stacking all images in a dataset is not a good strategy memory-wise. In a real-world scenario, you want to compute mean and standard deviation using constant-memory approaches. However, for the sake of this tutorial we can disregard this aspect and focus on other aspects of the code.</p> <p>Important things to consider:</p> <ul> <li>Since we want to be able to use this operator with many types of dataset, we inherit from <code>DatasetOperator[Dataset, Dataset]</code>, leaving the type of dataset unspecified for now.</li> <li>The <code>__call__</code> method has a typevar <code>T</code> that is used to tell the type-checker that the type of samples contained in the input dataset is preserved in the output dataset, allowing us to use this operator with any dataset we want.</li> <li>In the first part of the <code>__call__</code> method, we eagerly compute mu and sigma vectors iterating over the whole dataset.</li> <li>In the second part of the <code>__call__</code> method we return a lazy dataset instance that applies the normalization.</li> </ul> <pre><code>class NormalizeOp(DatasetOperator[Dataset, Dataset]):\n    def __init__(self, grabber: Grabber | None = None) -&gt; None:\n        super().__init__()\n        self._grabber = grabber or Grabber()\n\n    def _normalize[\n        T: Sample\n    ](self, dataset: Dataset[T], mu: np.ndarray, sigma: np.ndarray, i: int) -&gt; T:\n        # Get the image of the i-th sample\n        sample = dataset[i]\n        image = sample[\"image\"]().astype(np.float32)\n\n        # Apply normalization then bring values back to the [0, 255] range, \n        # clipping values below -sigma to 0 and above sigma to 255.\n        image = (image - mu) / sigma\n        image = (image * 255 / 2 + 255 / 2).clip(0, 255).astype(np.uint8)\n\n        # Replace the image item value\n        return sample.with_value(\"image\", image)\n\n    def __call__[T: Sample](self, x: Dataset[T]) -&gt; Dataset[T]:\n        # Compute mu and sigma on the dataset (eager)\n        all_images = []\n        for _, sample in self.loop(x, self._grabber, name=\"Computing stats\"):\n            all_images.append(sample[\"image\"]())\n        all_images_np = np.concatenate(all_images)\n        mu = all_images_np.mean((0, 1), keepdims=True)\n        sigma = all_images_np.std((0, 1), keepdims=True)\n\n        # Lazily apply the normalization with precomputed mu and sigma\n        return LazyDataset(len(x), partial(self._normalize, x, mu, sigma))\n</code></pre> <p>Custom dataset operators can be registered to the Pipewine CLI to allow you to apply custom transformations to your datasets using a common CLI. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/actions/#mappers","title":"Mappers","text":"<p>Pipewine <code>Mapper</code> objects (essentially the same as Pipelime Stages), allow you to quickly implement dataset operators by defining a function that transforms individual samples.</p> <p>This allows you to write way less code when all the following conditions apply:</p> <ul> <li>The operation accepts and returns a single dataset.</li> <li>The input and output datasets have the same length.</li> <li>The i-th sample of the output dataset can be computed as a pure function of the i-th sample of the input dataset.</li> </ul> <p>Danger</p> <p>Never use mappers when the function that transforms samples is stateful. Not only Pipewine does not preserve the order of samples when calling mappers, but the execution may be run concurrently in different processes that do not share memory.</p> <p>In these cases, use a <code>DatasetOperator</code> instead of a <code>Mapper</code>.</p> <p>Mappers must be used in combination with <code>MapOp</code>, a built-in dataset operator that lazily applies a mapper to every sample of a dataset.</p> <p>Example</p> <p>Example usage of a mapper that renames items in a dataset:</p> <pre><code># Given a dataset\ndataset: Dataset\n\n# Rename all items named \"image\" into \"my_image\"\nop = MapOp(RenameMapper({\"image\": \"my_image\"}))\n\n# Apply the mapper\ndataset = op(dataset)\n</code></pre> <p>If you need to apply multiple mappers, instead of applying them individually using multiple <code>MapOp</code>, you should compose the mappers into a single one using the built-in <code>ComposeMapper</code> and then apply it with a single <code>MapOp</code>.</p> <p>All mappers are statically annotated with two type variables representing the type of input and output samples they accept/return, enabling static type checking. E.g. <code>Mapper[SampleA, SampleB]</code> accepts samples of type <code>SampleA</code> and returns samples of type <code>SampleB</code>. When composed in a <code>ComposeMapper</code>, it will automatically detect the input and output type from the sequence of mappers it is constructed with.</p> <p>Example</p> <p>Here, the static type-checker automatically infers type <code>Mapper[SampleA, SampleE]</code> for the variable <code>composed</code>:</p> <pre><code>mapper_ab: Mapper[SampleA, SampleB]\nmapper_bc: Mapper[SampleB, SampleC]\nmapper_cd: Mapper[SampleC, SampleD]\nmapper_de: Mapper[SampleD, SampleE]\n\ncomposed = ComposeMapper((mapper_ab, mapper_bc, mapper_cd, mapper_de))\n</code></pre>"},{"location":"tutorial/actions/#built-in-mappers","title":"Built-in Mappers","text":"<p>Pipewine has some useful generic mappers that are commonly used in many workflows. Here is a list of the currently available built-in mappers with a brief explanation. More in-depth documentation in the API reference.</p> <p>Key transformation mappers: modify the samples by adding, removing and renaming keys.</p> <ul> <li><code>DuplicateItemMapper</code>: create a copy of an existing item and give it a different name.</li> <li><code>FormatKeysMapper</code>: rename items using a format string.</li> <li><code>RenameMapper</code>: rename items using a mapping from old to new keys.</li> <li><code>FilterKeysMapper</code>: keep or discard a subset of items.</li> </ul> <p>Item transformation mappers: modify item properties such as parser and sharedness.</p> <ul> <li><code>ConvertMapper</code>: change the parser of a subset of items, e.g. convert PNG to JPEG.</li> <li><code>ShareMapper</code>: change the sharedness of a subset of items.</li> </ul> <p>Cryptography mappers: currently contains only <code>HashMapper</code>.</p> <ul> <li><code>HashMapper</code>: computes a secure hash of a subset of items, useful to perform deduplication or integrity checks.</li> </ul> <p>Special mappers:</p> <ul> <li><code>CacheMapper</code>: converts all items to <code>CachedItem</code>. </li> <li><code>ComposeMapper</code>: composes many mappers into a single object that applies all functions sequentially. </li> </ul>"},{"location":"tutorial/actions/#custom-mappers","title":"Custom Mappers","text":"<p>To implement a <code>Mapper</code>:</p> <ol> <li>Inherit from <code>Mapper</code> and specify both the input and output sample types.</li> <li>[Optional] Implement an <code>__init__</code> method.</li> <li>Implement the <code>__call__</code> method, accepting an integer and the input sample, and returning the output sample.</li> </ol> <p>Danger</p> <p>Remember: mappers are stateless. Never use object fields to store state between subsequent calls.</p> <p>Example</p> <p>In this example we will implement a mapper that inverts the colors of an image.</p> <pre><code>class ImageSample(TypedSample):\n    image: Item[np.ndarray]\n\nclass InvertRGBMapper(Mapper[ImageSample, ImageSample]):\n    def __call__(self, idx: int, x: ImageSample) -&gt; ImageSample:\n        return x.with_value(\"image\", 255 - x.image())\n</code></pre> <p>Custom mappers can be registered to the Pipewine CLI to allow you to apply custom transformations to your datasets using a common CLI. This is covered in the CLI tutorial. </p>"},{"location":"tutorial/cache/","title":"\ud83d\udcbe Cache","text":""},{"location":"tutorial/cache/#overwiew","title":"Overwiew","text":"<p>By default, accessing a Pipewine dataset via the <code>__getitem__</code> method often implies calling a function that performs some computation and then constructs a new Sample object. Furthermore, accessing items on a sample may require a read from external storages e.g. a file system or a remote DB. </p> <p>When dealing with deterministic operations (i.e. same inputs imply the same outputs) it makes little sense to perform these computations and/or reads more than once: we could store the output of the first call and then, for every successive call, just look it up and return it. </p> <p>That's a technique called Memoization that while being very effective at speeding computation up, it also has no bound on the number of elements it keeps stored, meaning that it will eventually end up using all the available memory on our system, if the data we are trying to store does not entirely fit in system memory.</p> <p>Note that if we had the guarantee that every dataset could fit in the system memory, we could just apply every operation eagerly and avoid dealing with this caching headache altogether, but unfortunately (a) it's surprisingly common to deal with datasets that cannot fit in your system's RAM and (b) even if they did, would you be ok with Pipewine using 95% of your system memory?</p> <p>Pipewine has some built-in tools to provide a reasonable trade-off between (a) wasting time performing the same computation multiple times but minimizing the memory usage and (b) eliminating all waste but keeping everything memorized, because often, something in between these two extreme cases is highly preferable. </p> <p>Pipewine allows you to cache data in three different ways:</p> <ul> <li>Cache at the item level, to accelerate consecutive accesses to the same item object, mainly to avoid multiple I/O for the same data.</li> <li>Cache at the dataset level, to accelerate consecutive accesses (<code>__getitem__</code>) to the same dataset, avoiding the re-computation of lazy operations.  </li> <li>Checkpoints, to save all intermediate results to disk (applying all the lazy operations) and then read the results.</li> </ul> <p>Note</p> <p>There is no cache at the sample level, because sample objects receive all their items at construction time and they always own a reference to all items they contain, making them always immediately available. </p>"},{"location":"tutorial/cache/#item-cache","title":"Item Cache","text":"<p>Item cache converts all items of a sample into <code>CachedItem</code> instances. As you may have seen on the previous section of this tutorial, <code>CachedItem</code> is a wrapper of a regular <code>Item</code> store the result of the <code>__call__</code> method when called for the first time, then, every subsequent call simply returns the stored value without calling the inner item twice. <code>CachedItem</code> are lightweight and computationally inexpensive, they merely store a reference and return it.</p> <p>The <code>__call__</code> method of a stored item implies a read from disk (or other sources), calling it multiple times results in the slow read operation being repeated!</p> <p>Example</p> <p>Very slow code, without item cache every time we access the \"image\" item we read it from disk!</p> <p>This runs in approximately 130 ms on my system. </p> <pre><code>dataset = UnderfolderSource(Path(\"my_dataset\"))()\nsample = dataset[0]\n\nfor _ in range(1000):\n    sample[\"image\"]()\n</code></pre> <p>We can improve this situation by using item caches, storing the result of the first read, then immediately return it in every subsequent call.</p> <p>Example</p> <p>Same code with item caches, this time it takes only 0.2 ms!</p> <pre><code>dataset: Dataset = UnderfolderSource(Path(\"my_dataset\"))()\ndataset = ItemCacheOp()(dataset)\nsample = dataset[0]\n\nfor _ in range(1000):\n    sample[\"image\"]()\n</code></pre> <p>Contrary to Pipelime, item caches die with the sample object: whenever we index the dataset, we get a brand new sample object with newly created (thus empty) <code>CachedItem</code> instances.</p> <p>Warning</p> <p>Even though we call <code>ItemCacheOp</code>, this is as slow as the first example where we did not use item caching at all! </p> <p>This is because the sample is re-created every time we index the dataset. <pre><code>dataset: Dataset = UnderfolderSource(Path(\"my_dataset\"))()\ndataset = ItemCacheOp()(dataset)\n\nfor _ in range(1000):\n    dataset[0][\"image\"]()\n</code></pre></p> <p>This also means that unless you keep references to samples, you cannot possibly blow up your memory using item cache.</p> <p>A few tips on item caches:</p> <p>Tip</p> <p>Use <code>ItemCacheOp</code> when you need to access the same item on the same sample multiple times and you want to avoid reading from disk.</p> <p>The following code runs in approximately 19ms on my system. Without item caches, it would take 29ms, roughly 50% longer!</p> <pre><code>def function1(sample: Sample) -&gt; None:\n    print(\"Average Color: \", sample[\"image\"]().mean((0, 1)))\n\n\ndef function2(sample: Sample) -&gt; None:\n    print(\"Color Standard Deviation: \", sample[\"image\"]().std((0, 1)))\n\n\nif __name__ == \"__main__\":\n    path = Path(\"tests/sample_data/underfolders/underfolder_0\")\n\n    dataset = UnderfolderSource(path)()\n    dataset = ItemCacheOp()(dataset)\n\n    for sample in dataset:\n        function1(sample)\n        function2(sample)\n</code></pre> <p>Tip</p> <p>Results of previous operations are always stored inside <code>MemoryItems</code>. It makes no sense to cache those.</p> <p>Tip</p> <p>If you only access an item once for every sample in the dataset, your code won't benefit from using item cache.</p>"},{"location":"tutorial/cache/#dataset-cache","title":"Dataset Cache","text":"<p>Item caches are very simple, inexpensive and relatively safe to use, but they cannot do anything about repeated computations of lazy operations. </p> <p>Let's consider the following example: </p> <p>Example</p> <p>We have a dataset of 26 samples, for each of them we compute statistics about an item named \"image\", like the average color, the standard deviation, the minimum and maximum pixel values. We then repeat the results 100 times to artificially increase the dataset length.</p> <pre><code>class ComputeStatsMapper(Mapper[Sample, Sample]):\n    def __call__(self, idx: int, x: Sample) -&gt; Sample:\n        image: np.ndarray = x[\"image\"]()\n        mean = image.mean((0, 1)).tolist()\n        std = image.std((0, 1)).tolist()\n        min_, max_ = int(image.min()), int(image.max())\n        stats_item = MemoryItem(\n            {\"mean\": mean, \"std\": std, \"min\": min_, \"max\": max_}, YAMLParser()\n        )\n        return x.with_item(\"stats\", stats_item)\n\n\npath = Path(\"tests/sample_data/underfolders/underfolder_0\")\n\ndataset = UnderfolderSource(path)()\ndataset = MapOp(ComputeStatsMapper())(dataset)\ndataset = RepeatOp(100)(dataset)\n\nfor sample in dataset: \n    print(sample[\"stats\"]())\n</code></pre> <p>The code above runs (on my system) in about 750 ms. That's a very long time, even considering the slow speed of the Python interpreter. That's because <code>RepeatOp</code> is lazy, and the i-th sample is only computed when the dataset is indexed. In total, the dataset is indexed 26 * 100 = 2600 times, each time computing the image stats from scratch:</p> <ul> <li>User requests sample 0 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0</li> <li>User requests sample 1 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1</li> <li>...</li> <li>User requests sample 26 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0</li> <li>User requests sample 27 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1</li> <li>...</li> </ul> <p>If the program were run eagerly (that's not the case with Pipewine), the stats were computed on the 26 original images, then the results would be repeated 100 times with virtually no cost.</p> <p>To solve this problem we can use <code>CacheOp</code>, a special operator that adds a layer of cache to the result of an operation, combined with <code>MemoCache</code>, a very simple cache that memorizes everything with no bound on the number of cached elements.</p> <pre><code>dataset = UnderfolderSource(path)()\ndataset = MapOp(ComputeStatsMapper())(dataset)\ndataset = CacheOp(MemoCache)(dataset)  # &lt;- Activates dataset caching\ndataset = RepeatOp(100)(dataset)\n\nfor sample in dataset: \n    print(sample[\"stats\"]())\n</code></pre> <p>The code now runs in 45 ms and the stats computation is only performed 26 times:</p> <ul> <li>User requests sample 0 -&gt; maps to sample 0 of original dataset -&gt; compute stats on image 0 -&gt; cache result 0</li> <li>User requests sample 1 -&gt; maps to sample 1 of original dataset -&gt; compute stats on image 1 -&gt; cache result 1</li> <li>...</li> <li>User requests sample 26 -&gt; maps to sample 0 of original dataset -&gt; return cached sample 0</li> <li>User requests sample 27 -&gt; maps to sample 1 of original dataset -&gt; return cached sample 1</li> <li>...</li> </ul> <p>As you might have noticed, <code>CacheOp</code> can be parametrized with the type of cache to use, and optionally implementation-specific parameters to further customize the way data is cached. There is no silver bullet here, and the type of cache you should use strongly depends on the order the data is accessed, and if chosen wrongly it can lead to 0% hit rate: the worst case scenario where you pay the price of increased memory utilization, additional overhead, and get nothing in return.</p> <p>Note</p> <p><code>CacheOp</code>, regardless of the policy, always applies item-level caching and inherits all its benefits.</p> <p>This is useful because if you want dataset-level caching, you probably want item-level caching as well, sparing you the effort of always applying two <code>ItemCacheOp</code> after <code>CacheOp</code>.</p> <p>For Pipewine, <code>Cache</code> objects are anything that behaves like this:</p> <pre><code>class Cache[K, V]:\n    def clear(self) -&gt; None: ... # &lt;- Removes everything from the cache.\n    def get(self, key: K) -&gt; V | None: ... # Return the value of key, if any.\n    def put(self, key: K, value: V) -&gt; None: ... # Assign a value to a key.\n</code></pre> <p>Let'see the available cache types, together with their pros and cons.</p>"},{"location":"tutorial/cache/#memocache","title":"MemoCache","text":"<p><code>MemoCache</code> is a basic memoization. It can only grow in size since it never evicts elements and has no upper bound to the number of elements it can keep memorized.</p> <p>Access and insertion are both O(1) and are the fastest among all the cache types implemented in Pipewine.</p> <p>Success</p> <p>Best used to cache the output of long computations where either:</p> <ul> <li>The number of samples in the dataset is small (better if known a-priori).</li> <li>The size of the cached data is relatively small. </li> <li>You don't care about memory, you just want to always guarantee the maximum possible cache hit rate by brute-force.</li> </ul> <p>Failure</p> <p><code>MemoCache</code> is unbounded: it may end up using all the available memory on your system.</p>"},{"location":"tutorial/cache/#rrcache","title":"RRCache","text":"<p><code>RRCache</code> is a bounded cache that follows a stochastic replacement rule. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting a random element.</p> <p>Access and insertion are both O(1). Access is exactly as fast as with <code>MemoCache</code>, insertion includes additional overhead due to the RNG.</p> <p>Success</p> <p>You can use a random replacement cache when the order in which samples are accessed is unknown, but still you want to statistically do better than the worst case scenario (0% hit rate).</p> <p>Failure</p> <p>Random replacement cache is not optimal if:</p> <ul> <li>You need deterministic running time.</li> <li>You know the order in which samples are accessed and you can select a more appropriate cache type.</li> </ul>"},{"location":"tutorial/cache/#fifocache","title":"FIFOCache","text":"<p><code>FIFOCache</code> is a bounded cache that stores element in a queue data structure. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting the element that was least recently inserted in the cache (not to be confused with LRU caches), regardless of how many times they were accessed before.</p> <p>Basically, the first element that is inserted is going to be the first that gets evicted.</p> <p>Access and insertion are both O(1). Access is exactly as fast as with <code>MemoCache</code>, insertion includes additional overhead due the eviction policy implementation.</p> <p>Success</p> <p>Use <code>FIFOCache</code> when an element is likely going to be accessed multiple times shortly after being inserted in the cache.</p> <p>Failure</p> <p>The FIFO eviction policy guarantees a 0% hit rate when the elements are accessed in order from first to last in multiple cycles. </p>"},{"location":"tutorial/cache/#lifocache","title":"LIFOCache","text":"<p><code>LIFOCache</code> is a bounded cache that stores element in a stack data structure. Whenever an element needs to be added to the cache and no space is available, the cache will make space by evicting the element that was most recently inserted in the cache (not to be confused with MRU caches), regardless of how many times they were accessed before.</p> <p>Basically, the first element that is inserted is going to be the last that gets evicted.</p> <p>Access and insertion are both O(1), same cost as with <code>FIFOCache</code>. </p> <p>Success</p> <p>Use <code>LIFOCache</code> when:</p> <ul> <li>Elements are accessed in order from first to last in multiple cycles.  </li> <li>The first elements to be inserted are likely going to be accessed more frequently.  </li> </ul> <p>Failure</p> <p>The LIFO eviction policy performs terribly when recently inserted elements are likely going to be accessed in subsequent, but not immediately consecutive, calls.</p>"},{"location":"tutorial/cache/#lrucache","title":"LRUCache","text":"<p><code>LRUCache</code> (Least Recently Used Cache) is a bounded cache that whenever an element needs to be added and no space is available, it will forget the element that was least recently accessed. In contrast with the <code>FIFOCache</code>, the eviction policy takes into account the way elements are accessed, making this solution slightly more complex.</p> <p>Access and insertion are both O(1), both include additional overhead due to potential state changes in the underlying data structure. The implementation is a modified version of the one proposed in the Python3.12 standard library that uses a circular doubly linked list + hashmap. </p> <p>Success</p> <p>Use <code>LRUCache</code> when an element that was recently accessed is likely going to be accessed again in the future.</p> <p>Failure</p> <p>The LRU eviction policy guarantees a 0% hit rate when the elements are accessed in order from first to last in multiple cycles, similarly to <code>FIFOCache</code>.</p>"},{"location":"tutorial/cache/#mrucache","title":"MRUCache","text":"<p><code>MRUCache</code> (Most Recently Used Cache) is a bounded cache that whenever an element needs to be added and no space is available, it will forget the element that was most recently accessed. In contrast with the <code>LIFOCache</code>, the eviction policy takes into account the way elements are accessed, making this solution slightly more complex.</p> <p>Access and insertion are both O(1), same costs as <code>LRUCache</code>. </p> <p>Success</p> <p>Use <code>MRUCache</code> when:</p> <ul> <li>Elements are accessed in order from first to last in multiple cycles.  </li> <li>Elements that are recently accessed are likely not going to be accessed again for a long time.</li> </ul> <p>Failure</p> <p>The MRU eviction policy performs terribly when recently inserted elements are likely going to be accessed in subsequent calls.</p>"},{"location":"tutorial/cache/#benchmark","title":"Benchmark","text":"<p>Here is a very naive benchmark of different cache eviction policies compared under different access patterns, under the following conditions:</p> <ul> <li>The dataset consists of 26 elements.</li> <li>The total number of calls is 26000.</li> <li>The maximum cache size is set to 5.</li> </ul> <p>Types of access patterns used:</p> <ul> <li> <p>\"Cyclic\" accesses elements from first to last in multiple cycles. E.g.</p> <p><code>0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 ...</code></p> </li> <li> <p>\"Back and Forth\" accesses elements from first to last in even cycles, and from last to first in odd cycles. E.g. </p> <p><code>0 1 2 3 4 5 6 7 8 9 9 8 7 6 5 4 3 2 1 0 0 1 2 3 ...</code></p> </li> <li> <p>\"Hot Element\" accesses the elements similarly to \"Cyclic\" in odd indexes, but in every even index it accesses the first \"hot\" element:</p> <p><code>0 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 0 0 1 0 2 ...</code></p> </li> <li> <p>\"Blocks\" accesses the elements similarly to \"Cyclic\" in odd indexes, but in chunks of K elements. E.g. with K=4:</p> <p><code>0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 ...</code></p> </li> <li> <p>\"Sliding Window\" accesses the elements in groups of K increasing indexes. E.g. with K=4:</p> <p><code>0 1 2 3 1 2 3 4 2 3 4 5 3 4 5 6 4 5 6 7 5 6 7 8 6 7 8 9 ...</code></p> </li> <li> <p>\"Uniform\" accesses the elements in random order, sampling from a uniform distribution.</p> </li> <li>\"Zipfian\" accesses the elements in random order, sampling from a zipfian distribution.</li> <li>\"Random Walk\" accesses the items in random order, where the i-th index is computed by adding a random shift from the previous one, sampled from a normal distribution with a small positive shift.</li> </ul> <p></p>"},{"location":"tutorial/cache/#checkpoints","title":"Checkpoints","text":"<p>So far we have seen how to mitigate the problem of multiple accesses to the same item using <code>ItemCacheOp</code> cache and how to avoid re-computing the same lazy operation when accessing a dataset with the same index multiple times <code>CacheOp</code>. Pipewine has a final caching mechanism called \"checkpoint\" that can be used when both:</p> <ul> <li>Datasets are not guaranteed to entirely fit into system memory. E.g. using <code>MemoCache</code> would crash your pipeline.</li> <li>Some operations are very slow and the risk of computing them multiple times due to a cache miss is unacceptable. In this case using any policy other than <code>MemoCache</code> would not eliminate this risk entirely.</li> </ul> <p>Checkpoints are a rather stupid but effective idea to deal with this situation: we eagerly write the whole dataset to disk (or to a DB) and then return a lazy dataset that reads from it. </p> <p>This way we use disk space (which we can safely assume to be enough to store the dataset) to cache intermediate results, so that future accesses won't have to re-compute all the lazy operations that were needed to reach that intermediate state.</p> <p>Tip</p> <p>Checkpoints are not a free lunch: you shift the cost from computation to I/O.  As a very approximate rule of thumb, under many simplifying assumptions, checkpoints should be convenient if:</p> <pre><code>(N - 1) * compute_cost &gt; (N + 1) * io_cost\n</code></pre> <p>Where <code>N</code> is the average number of times that the same computation is repeated, <code>compute_cost</code> is the total cost of computing all computations once, <code>io_cost</code> is the total cost of reading or writing the results of all comptuation once. </p> <p>Checkpoints are currently not a class or any software component you can import from Pipewine, they are a functionality of Pipewine Workflows, a higher level concept that was not explained in this tutorial so far. However, you don't need Workflows to use checkpoints, you can simply use a pair of a <code>DatasetSink</code> and <code>DatasetSource</code>.</p> <p>Note</p> <p>When using workflows, placing a checkpoint in the middle of a pipeline is way easier than this: all you need to do is set a boolean flag <code>checkpoint=True</code> where needed, or, if you want to use a checkpoint after every single operation, you can set a flag to enable them by default.</p> <p>Refer to the Workflows section for more info.</p> <p>Example</p> <p>A rather extreme example of checkpoint usage: starting from the \"letters\" toy dataset used in the examples so far, we want to replace the 26 (badly) hand-drawn images with AI-generated ones. Then, we want to replicate the whole dataset 100 times.</p> <p>Yes, we only want to generate a total of 26 images, not 2600, using Gen-AI.</p> <p>Suppose we have access to an API that costs us 1$ per generated image and we wrapped that into a <code>Mapper</code>:</p> <pre><code>class RegenerateImage(Mapper[Sample, Sample]):\n    def __call__(self, idx: int, x: Sample) -&gt; Sample:\n        metadata = x[\"metadata\"]()\n        letter, color = metadata[\"letter\"], metadata[\"color\"]\n        prompt = (\n            f\"Create an artistic representation of the letter {letter} in a visually \"\n            f\"striking style. The letter should be prominently displayed in {color}, \"\n            \"with a background that complements or contrasts it artistically. Use \"\n            \"textures, lighting, and creative elements to make the design unique \"\n            \"and aesthetically appealing.\"\n        )\n        gen_image = extremely_expensive_api_that_costs_1_USD_per_image(prompt)\n        return x.with_value(\"image\", gen_image)\n</code></pre> <p>To (correclty) generate only 26 images, then repeat the whole dataset 100 times, we can use a checkpoint just after applying the <code>RegenerateImage</code> mapper:</p> <pre><code>path = Path(\"tests/sample_data/underfolders/underfolder_0\")\ndataset = UnderfolderSource(path)()\ndataset = MapOp(RegenerateImage())(dataset)\n\n# Checkpoint\nUnderfolderSink(ckpt := Path(\"/tmp/ckpt\"))(dataset)\ndataset = UnderfolderSource(ckpt)()\n\ndataset = RepeatOp(100)(dataset)\n</code></pre> <p>Without the checkpoint, we would instead call the API a total of 2600 times, generating 100x more images than we originally intended and wasting a lot of money.</p>"},{"location":"tutorial/cache/#cache-and-multi-processing","title":"Cache and Multi-processing","text":"<p>Multiprocessing in python does not allow processes to hold references to arbitrary python objects in shared memory. Besides a restricted set of built-in types, all the communication between processes uses the message passing paradigm. For process A to send an object to process B, many expensive things need to happen: - A and B need to hold a pair of connected sockets. - A must serialize the object into bytes. - A must write the bytes into the socket. - B must read the bytes from the socket. - B must de-serialize the bytes into a copy of the original object.</p> <p>As pointed out in the Python docs, using shared state between multiple processes should be avoided as much as possible, for the following reasons:</p> <ul> <li>Synchronization: when processes need to share state, they need mechanisms like locks, semaphores or other synchronization primitives to prevent race conditions and mess everything up.</li> <li>Performance overhead: each time an object is passed from one process to the other, it needs to be serialized and de-serialized, which is generally an expensive operation.</li> <li>Memory overhead: each time an object is passed from one process to the other it is basically as if it were deep-copied, vastly increasing memory usage.</li> <li>Debugging difficulty: debugging race conditions is not fun, they can be very hard to detect and reproduce.</li> <li>Scalability: if many processes depend on shared state, the system might not scale well with increasing number of processes due to contention and synchronization overhead.</li> </ul> <p>To avoid these issues Pipewine caches are not shared between processes, meaning that if process A computes a sample and caches it, the result will only be cached for process A. Later, if process B needs that sample and looks for it in its own cache, it won't find it and will have to compute it.</p> <p>The only exception is if the cache was partially populated in the main process: in this case the cache is cloned and every child process inherits its own copy at the moment of spawning. All state changes that occur later are independent for each child process and are discarded when the processes die. No change is reflected on the original copy of the cache in the main process.</p> <p>We can say that multi-processing makes caching less effective: since processes do not share state and potentially the same sample can be cached multiple times in more than one process we use memory less efficiently. With a pool of N processes we can either have the same hit rate as with a single process by using N times the amount of memory, or we can keep the memory fixed but accomplish a much smaller hit rate.</p> <p>Checkpoints in contrast, assuming that N python processes are not enough to saturate your read/write bandwith, do not suffer this problem: each sample is written exactly once independently from the others without need for synchronization.</p>"},{"location":"tutorial/cache/#comparison-with-pipelime","title":"Comparison with Pipelime","text":"<p>Comparison of cache-related features in Pipelime and Pipewine:</p> Feature Pipelime (OLD) Pipewine (NEW) Item Cache Enabled by default, can be disabled with <code>@no_data_cache()</code> decorator. Disabled by default, can be enabled by either adding a <code>ItemCacheOp</code> or <code>CacheMapper</code> where necessary. Dataset Cache Depends on how the <code>SourceSequence</code> or <code>PipedSequence</code> steps of your pipeline are implemented: some of them construct samples when the <code>__getitem__</code> is called, others construct them upfront and hold references to them. In the former case, no dataset caching is done. In the latter case, if item caching is not disabled, everything will be memorized as the pipeline progresses. In some cases, it is necessary to turn off caching completely to avoid running out of memory. Disabled by default, can be enabled by using <code>CacheOp</code> with the desired eviction policy. Supports many cache eviction policies to bound the number of cached samples to a safe amount and avoid out of memory issues. Checkpoints Every <code>PipelimeCommand</code> reads inputs and writes outputs from/to external sources, essentially making checkpoints enabled by default for every <code>PipelimeCommand</code>. The command also specifies where intermediate steps are written and in which format. Any change in either location and format requires you to apply changes to the command that writes the intermediate result and to every command that reads them. Checkpoints are disabled by default but can be enabled by adding a pair of sink and source between any two steps of the pipeline. The individual steps of the pipeline do not know anything about checkpoints, requiring you to apply no change to your operators. When using workflows Pipewine can be configured to automatically inject a pair of source and sink between any two steps of the pipeline. Cache and Multi-processing No sharing is performed, child processes inherit a copy of the parent process cache. Cache state is lost on process exit. Same as Pipelime."},{"location":"tutorial/cli/","title":"\ud83d\udda5\ufe0f CLI","text":""},{"location":"tutorial/data/","title":"\ud83d\uddc3\ufe0f Data Model","text":""},{"location":"tutorial/data/#overview","title":"Overview","text":"<p>In this section your will learn what are the main data abstractions upon which Pipewine is built, how to interact with them and extend them according to your needs.</p> <p>Pipewine data model is composed of three main abstractions: </p> <ul> <li>Dataset - A Sequence of <code>Sample</code> instances, where \"sequence\" means an ordered collection that supports indexing, slicing and iteration.</li> <li>Sample - A Mapping of strings to <code>Item</code> instances, where \"mapping\" means a set of key-value pairs that supports indexing and iteration. </li> <li>Item - An object that has access to the underlying data unit. E.g. images, text, structured metadata, numpy arrays, and whatever serializable object you may want to include in your dataset.</li> </ul> <p>Plus, some lower level components that are detailed later on. You can disregard them for now:</p> <ul> <li>Parser - Defines how an item should encode/decode the associated data.</li> <li>Reader - Defines how an item should access data stored elsewhere.</li> </ul>"},{"location":"tutorial/data/#dataset","title":"Dataset","text":"<p><code>Dataset</code> is the highest-level container and manages the following information:</p> <ul> <li>How many samples it contains</li> <li>In which order</li> </ul> <p>It provides methods to access individual samples or slices of datasets, as in Python slice.</p> <p>Note</p> <p>A <code>Dataset</code> is an immutable Python Sequence, supporting all its methods.</p> <p>All <code>Dataset</code> objects are Generics, meaning that they can be hinted with information about the type of samples they contain. This is especially useful if you are using a static type checker.</p> <p>Example</p> <p>Example usage of a <code>Dataset</code> object:</p> <pre><code># Given a Dataset of MySample's \ndataset: Dataset[MySample]\n\n# Supports len\nnumber_of_samples = len(dataset)\n\n# Supports indexing\nsample_0 = dataset[0]    # The type checker infers the type: MySample\nsample_51 = dataset[51]  # The type checker infers the type: MySample\n\n# Suppors slicing\nsub_dataset = dataset[10:20] # The type checker infers the type: Dataset[MySample]\n\n# Supports iteration\nfor sample in dataset:\n    ...\n</code></pre> <p>By default Pipewine provides two implementations of the <code>Dataset</code> interface: </p> <ul> <li><code>ListDataset</code></li> <li><code>LazyDataset</code> </li> </ul>"},{"location":"tutorial/data/#listdataset","title":"ListDataset","text":"<p>A <code>ListDataset</code> is basically a wrapper around a Python <code>list</code>, such that, whenever indexed, the result is immediately available. </p> <p>To achieve this, it has two fundamental requirements:</p> <ol> <li>All samples must be known at creation time.</li> <li>All samples must be always loaded into memory.</li> </ol> <p>Due to these limitations, it's rarely used in the built-in operations, since the lazy alternative <code>LazyDataset</code> combined with caching provides a better trade-off, but it may be handy to have when:</p> <ul> <li>The number of samples is small.</li> <li>Samples are lightweight (i.e. no images, 3d data, huge tensors etc...)</li> </ul> <p>Example</p> <p>Example of how to construct a <code>ListDataset</code>:</p> <pre><code># Create a list of samples\nsamples = [ ... ] \n\n# Wrap it in a ListDataset\ndataset = ListDataset(samples)\n</code></pre> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(N) (including the construction of the list)</li> <li>Length - O(1)</li> <li>Indexing - O(1)</li> <li>Slicing - O(N)</li> </ul>"},{"location":"tutorial/data/#lazydataset","title":"LazyDataset","text":"<p>The smarter alternative is <code>LazyDataset</code>, a type of <code>Dataset</code> that defers the computation of the samples as late as possible. That's right, when using a <code>LazyDataset</code> samples are created when it is indexed, using a user-defined function that is passed at creation time.  </p> <p>This has some implications:</p> <ul> <li>Samples are not required to be known at creation time, meaning that you can create a <code>LazyDataset</code> in virually zero time.</li> <li>Samples are not required to be kept loaded into memory the whole time, meaning that the memory required by <code>LazyDataset</code> is constant.</li> <li>Constant-time slicing.</li> <li>The computatonal cost shifts to the indexing part, which now carries the burden of creating and returning samples. </li> </ul> <p>Example</p> <p>Let's see an example of how to create and use a <code>LazyDataset</code>: </p> <pre><code># Define a function that creates samples from an integer index.\ndef get_sample_fn(idx: int) -&gt; Sample:\n    print(f\"Called with index: {idx}\")\n\n    sample = ... # Omitted \n    return sample\n\n# Create a LazyDataset of length 10\ndataset = LazyDataset(10, get_sample_fn)\n\n# Do some indexing\nsample_0 = dataset[0] # Prints 'Called with index: 0'\nsample_1 = dataset[1] # Prints 'Called with index: 1'\nsample_2 = dataset[2] # Prints 'Called with index: 2'\n\n# Indexing the same sample multiple times calls the function multiple times\nsample_1 = dataset[1] # Prints 'Called with index: 1'\n</code></pre> <p>Warning</p> <p>What if my function is very expensive to compute? Is <code>LazyDataset</code> going to call it every time the dataset is indexed?</p> <p>Yes, but that can be avoided by using Caches, which are not managed by the <code>LazyDataset</code> class.</p> <p>Time complexity (N = number of samples):</p> <ul> <li>Creation - O(1)</li> <li>Length - O(1)</li> <li>Indexing - Depends on <code>get_sample_fn</code> and <code>index_fn</code>.</li> <li>Slicing - O(1)</li> </ul>"},{"location":"tutorial/data/#sample","title":"Sample","text":"<p><code>Sample</code> is a mapping-like container of <code>Item</code> objects. If dataset were tables (as in a SQL database), samples would be individual rows. Contrary to samples in a dataset, items in a sample do not have any ordering relationship and instead of being indexed with an integer, they are indexed by key.</p> <p>Note</p> <p>A <code>Sample</code> is an immutable Python Mapping, supporting all its methods.</p> <p>Example</p> <p>Let's see an example on how to use a <code>Sample</code> object as a python mapping:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Get the number of items inside the sample\nnumber_of_items = len(sample) \n\n# Retrieve an item named \"image\".\n# This does not return the actual image, but merely an Item that has access to it.\n# This will be explained in detail later.\nitem_image = sample[\"image\"] \n\n# Retrieve an item named \"metadata\"\nitem_metadata = sample[\"metadata\"]\n\n# Iterate on all keys\nfor key in sample.keys():\n    ...\n\n# Iterate on all items\nfor item in sample.values():\n    ...\n\n# Iterate on all key-item pairs\nfor key, item in sample.items():\n    ...\n</code></pre> <p>In addition to all <code>Mapping</code> methods, <code>Sample</code> provides a set of utility methods to create modified copies (samples are immutable) where new items are added, removed or have their content replaced by new values.</p> <p>Example</p> <p>Example showing how to manipulate <code>Sample</code> objects using utility methods:</p> <pre><code># Given a Sample (let's not worry about its creation)\nsample: Sample\n\n# Add/Replace the item named \"image\" with another item\nnew_sample = sample.with_item(\"image\", new_image_item)\n\n# Add/Replace multiple items at once\nnew_sample = sample.with_items(image=new_image_item, metadata=new_metadata_item)\n\n# Replace the contents of the item named \"image\" with new data\nnew_sample = sample.with_value(\"image\", np.array([[[...]]]))\n\n# Replace the contents of multiple items at once\nnew_sample = sample.with_values(image=np.array([[[...]]]), metadata={\"foo\": 42})\n\n# Remove one or more items\nnew_sample = sample.without(\"image\")\nnew_sample = sample.without(\"image\", \"metadata\") \n\n# Remove everything but one or more items\nnew_sample = sample.with_only(\"image\")\nnew_sample = sample.with_only(\"image\", \"metadata\")\n\n# Rename items\nnew_sample = sample.remap({\"image\": \"THE_IMAGE\", \"metadata\": \"THE_METADATA\"})\n</code></pre> <p>In contrast with <code>Datasets</code>, pipewine does not offer a lazy version of samples, meaning that the all items are always kept memorized. Usually, you want to keep the number of items per sample bound to a constant number.</p> <p>Pipewine provides two main <code>Sample</code> implementations that differ in the way they handle typing information.</p>"},{"location":"tutorial/data/#typelesssample","title":"TypelessSample","text":"<p>The most basic type of <code>Sample</code> is <code>TypelessSample</code>, akin to the old Pipelime <code>Sample</code>.  This class is basically a wrapper around a dictionary of items of unknown type.</p> <p>When using <code>TypelessSample</code> it's your responsibility to know what is the type of each item, meaning that if you access an item you then have to cast it to the expected type.</p> <p>With the old Pipelime, this quickly became a problem and lead to the creation of <code>Entity</code> and <code>Action</code> classes, that provide a type-safe alternative, but unfortunately integrate poorly with the rest of the library, failing to completely remove the need for casts or <code>type: ignore</code> directives. </p> <p>Example</p> <p>The type-checker fails to infer the type of the retrieved item: <pre><code>sample = TypelessSample(**dictionary_of_items)\n\n# When accessing the \"image\" item, the type checker cannot possibly know that the \n# item named \"image\" is an item that accesses an image represented by a numpy array.\nimage_item = sample[\"image\"]\n\n# Thus the need for casting (type-unsafe)\nimage_data = cast(np.ndarray, image_item())\n</code></pre></p> <p>Despite this limitation, <code>TypelessSample</code> allow you to use Pipewine in a quick-and-dirty way that allows for faster experimentation without worrying about type-safety.</p> <p>Example</p> <p>At any moment, you can convert any <code>Sample</code> into a <code>TypelessSample</code>, dropping all typing information by calling the <code>typeless</code> method:</p> <pre><code>sample: MySample\n\n# Construct a typeless copy of the sample\ntl_sample = sample.typeless()\n</code></pre>"},{"location":"tutorial/data/#typedsample","title":"TypedSample","text":"<p><code>TypedSample</code> is the type-safe alternative for samples. It allows you to construct samples that retain information on the type of each item contained within them, making your static type-checker happy.</p> <p><code>TypedSample</code> on its own does not do anything, to use it you always need to define a class that defines the names and the type of the items. This process is very similar to the definition of a Python dataclass, with minimal boilerplate.</p> <p>What you get in return:</p> <ul> <li>No need for <code>t.cast</code> or <code>type: ignore</code> directives that make your code cluttered and error-prone.</li> <li>The type-checker will complain when something is wrong with the way you use your <code>TypedSample</code>, effectively preventing many potential bugs.</li> <li>Intellisense automatically suggests field and method names for auto-completion.</li> <li>Want to rename an item? Any modern IDE is able to quickly rename all occurrences of a <code>TypedSample</code> field without breaking anything.</li> </ul> <p>Example</p> <p>Example creation and usage of a custom <code>TypedSample</code>:</p> <pre><code>class MySample(TypedSample):\n    image_left: Item[np.ndarray]\n    image_right: Item[np.ndarray]\n    category: Item[str]\n\nmy_sample = MySample(\n    image_left=image_left_item,\n    image_right=image_right_item,\n    category=category_item,\n)\n\nimage_left_item = my_sample.image_left # Type-checker infers type Item[np.ndarray]\nimage_left_item = my_sample[\"image_left\"] # Equivalent type-unsafe\n\nimage_right_item = my_sample.image_right # Type-checker infers type Item[np.ndarray]\nimage_right_item = my_sample[\"image_right\"] # Equivalent type-unsafe\n\ncategory_item = my_sample.category # Type-checker infers type Item[str]\ncategory_item = my_sample[\"category\"] # Equivalent type-unsafe\n</code></pre> <p>Warning</p> <p>Beware of naming conflicts when using <code>TypedSample</code>. You should avoid item names conflicting with the methods of the <code>Sample</code> class.  </p>"},{"location":"tutorial/data/#item","title":"Item","text":"<p><code>Item</code> objects represent a single serializable unit of data. They are not the data itself, instead, they only have access to the underlying data.</p> <p>Items do not implement any specific Python abstract type, since they are at the lowest level of the hierarchy and do not need to manage any collection of objects.</p> <p>All items can be provided with typing information about the type of the data they have access to. This enables the type-checker to automatically infer the type of the data when accessed. </p> <p>All <code>Item</code> objects have a <code>Parser</code> inside of them, an object that is responsible to encode/decode the data when reading or writing. These <code>Parser</code> objects are detailed later on.</p> <p>Furthermore, items can be flagged as \"shared\", enabling Pipelime to perform some optimizations when reading/writing them, but essentially leaving their behavior unchanged.</p> <p>Example</p> <p>Example usage of an <code>Item</code>:</p> <pre><code># Given an item that accesses a string\nitem: Item[str]\n\n# Get the actual data by calling the item ()\nactual_data = item()\n\n# Create a copy of the item with data replaced by something else\nnew_item = item.with_value(\"new_string\")\n\n# Get the parser of the item\nparser = item.parser\n\n# Create a copy of the item with another parser\nnew_item = item.with_parser(new_parser)\n\n# Get the sharedness of the the item\nis_shared = item.is_shared\n\n# Set the item as shared/unshared\nnew_item = item.with_sharedness(True)\n</code></pre> <p>Pipewine provides three <code>Item</code> variants, that differ in the way data is accessed or stored.</p>"},{"location":"tutorial/data/#memoryitem","title":"MemoryItem","text":"<p><code>MemoryItem</code> instances are items that directly contain data they are associated with. Accessing data is immediate as it is always loaded in memory and ready to be returned.</p> <p>Tip</p> <p>Use <code>MemoryItem</code> to contain \"new\" data that is the result of a computation. E.g. the output of a complex DL model.</p> <p>Example</p> <p>To create a <code>MemoryItem</code>, you just need to pass the data as-is and the <code>Parser</code> object:</p> <pre><code># Given a numpy array representing an image that is the output of an expensive\n# computation\nimage_data = np.array([[[...]]])\n\n# Create a MemoryItem that contains the data and explicitly tells Pipewine to always\n# encode the data as a JPEG image.\nmy_item = MemoryItem(image_data, JpegParser())\n</code></pre>"},{"location":"tutorial/data/#storeditem","title":"StoredItem","text":"<p><code>StoredItem</code> instances are items that point to external data stored elsewhere. Upon calling the item, the data is read from the storage, parsed and returned. </p> <p><code>StoredItem</code> objects use both <code>Parser</code> and <code>Reader</code> objects to retrieve the data. A <code>Reader</code> is an object that exposes a <code>read</code> method that returns data as bytes.</p> <p>Currently Pipewine provides a <code>Reader</code> for locally available files called <code>LocalFileReader</code>, that essentially all it does is <code>open(path, \"rb\").read()</code>.</p> <p>Tip</p> <p>Use <code>StoredItem</code> to contain data that is yet to be loaded. E.g. when creating a dataset that reads from a DB, do not perform all the loading upfront, use <code>StoredItem</code> to lazily load the data only when requested.</p> <p>Example</p> <p>To create a <code>StoredItem</code>, you need to </p> <pre><code># The reader object responsible for reading the data as bytes\nreader = LocalFileReader(Path(\"/my/file.png\"))\n\n# Create a StoredItem that is able able to read and parse the data when requested.\nmy_item = StoredItem(reader, PngParser())\n</code></pre> <p>Warning</p> <p>Contrary to old Pipelime items, <code>StoredItem</code> do not offer any kind of automatic caching mechanism: if you retrieve the data multiple times, you will perform a full read each time. </p> <p>To counteract this, you need to use Pipewine cache operations. </p>"},{"location":"tutorial/data/#cacheditem","title":"CachedItem","text":"<p><code>CachedItem</code> objects are items that offer a caching mechanism to avoid calling expensive read operations multiple times when the underlying data is left unchanged. </p> <p>To create a <code>CachedItem</code>, you just need to pass an <code>Item</code> of your choice to the <code>CachedItem</code> constructor.</p> <p>Example</p> <p>Example usage of a <code>CachedItem</code>:</p> <pre><code># Suppose we have an item that reads a high resolution BMP image from an old HDD. \nreader = LocalFileReader(Path(\"/extremely/large/file.bmp\"))\nitem = StoredItem(reader, BmpParser())\n\n# Reading data takes ages, and does not get faster if done multiple times.\ndata1 = item() # Slow\ndata2 = item() # Slow\ndata3 = item() # Slow\n\n# With CachedItem, we can memoize the data after the first access, making subsequent\n# accesses immediate\ncached_item = CachedItem(item)\n\ndata1 = cached_item() # Slow\ndata2 = cached_item() # Fast\ndata3 = cached_item() # Fast\n</code></pre>"},{"location":"tutorial/data/#parser","title":"Parser","text":"<p>Pipewine <code>Parser</code> objects are responsible for implementing the serialization/deserialization functions for data:</p> <ul> <li><code>parse</code> transforms bytes into python objects of your choice.</li> <li><code>dump</code> transforms python objects into bytes.</li> </ul>"},{"location":"tutorial/data/#built-in-parsers","title":"Built-in Parsers","text":"<p>Pipewine has some built-in parsers for commonly used data encodings: </p> <ul> <li> <p><code>PickleParser</code>: de/serializes data using Pickle, a binary protocol that can be used to de/serialize most Python objects. Key pros/cons:</p> <ul> <li>\u2705 <code>pickle</code> can efficiently serialize pretty much any python object.</li> <li>\u274c <code>pickle</code> is not secure: you can end up executing malicious code when reading data. </li> <li>\u274c <code>pickle</code> only works with Python, preventing interoperability with other systems.</li> <li>\u274c There are no guarantees that <code>pickle</code> data written today can be correctly read by future python interpreters.  </li> </ul> </li> <li> <p><code>JSONParser</code> and <code>YAMLParser</code> de/serializes data using JSON or YAML, two popular human-readable data serialization languages that support tree-like structures of data that strongly resemble Python builtin types.</p> <ul> <li>\u2705 Both JSON and YAML are interoperable with many existing systems.</li> <li>\u2705 Both JSON and YAML are standard formats that guarantee backward compatibility.</li> <li>\u26a0\ufe0f JSON and YAML only support a limited set of types such as <code>int</code>, <code>float</code>, <code>str</code>, <code>bool</code>, <code>dict</code>, <code>list</code>. </li> <li>\u2705 <code>JSONParser</code> and <code>YAMLParser</code> interoperate with pydantic <code>BaseModel</code> objects, automatically calling pydantic parsing, validation and dumping when reding/writing. </li> <li>\u274c Both JSON and YAML trade efficiency off for human readability. You may want to use different formats when dealing with large data that you don't care to manually read.</li> </ul> </li> <li> <p><code>NumpyNpyParser</code> de/serializes numpy arrays into binary files. </p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type</li> <li>\u274c Only works with Python and Numpy.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> </ul> </li> <li> <p><code>TiffParser</code> de/serializes numpy arrays into TIFF files.</p> <ul> <li>\u2705 Great with dealing with numpy arrays of arbitrary shape and type.</li> <li>\u2705 Produces files that can be read outside of Python.</li> <li>\u2705 Applies zlib lossless compression to reduce the file size. </li> </ul> </li> <li> <p><code>BmpParser</code> de/serializes numpy arrays into BMP files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u274c Does not apply any compression to data, resulting in very large files.</li> <li>\u2705 Fast de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>PngParser</code> de/serializes numpy arrays into PNG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale, RGB and RGBA uint8 images.</li> <li>\u2705 Produces smaller files due to image compression. </li> <li>\u274c Slow de/serialization.</li> <li>\u2705 Lossless.</li> </ul> </li> <li> <p><code>JpegParser</code> de/serializes numpy arrays into JPEG files.</p> <ul> <li>\u26a0\ufe0f Only supports grayscale and RGB uint8 images.</li> <li>\u2705 Produces very small files due to image compression. </li> <li>\u2705 Fast de/serialization.</li> <li>\u274c Lossy.</li> </ul> </li> </ul>"},{"location":"tutorial/data/#custom-parsers","title":"Custom Parsers","text":"<p>With Pipewine you are not limited to use the built-in Parsers, you can implement your own and use it seamlessly as if it were provided by the library.</p> <p>Example</p> <p>Let's create a <code>TrimeshParser</code> that is able to handle 3D meshes using the popular library Trimesh</p> <pre><code>class TrimeshParser(Parser[tm.Trimesh]):\n    def parse(self, data: bytes) -&gt; tm.Trimesh:\n        # Create a binary buffer with the binary data\n        buffer = io.BytesIO(data)\n\n        # Read the buffer and let trimesh load the 3D mesh object\n        return tm.load(buffer, file_type=\"obj\")\n\n    def dump(self, data: tm.Trimesh) -&gt; bytes:\n        # Create an empty buffer\n        buffer = io.BytesIO()\n\n        # Export the mesh to the buffer\n        data.export(buffer, file_type=\"obj\")\n\n        # Return the contents of the buffer\n        return buffer.read()\n\n    @classmethod\n    def extensions(cls) -&gt; Iterable[str]:\n        # This tells pipewine that it can automatically use this parses whenever a \n        # file with .obj extension is found and needs to be parsed.\n        return [\"obj\"]\n</code></pre>"},{"location":"tutorial/data/#immutability","title":"Immutability","text":"<p>All data model types are immutable. Their inner state is hidden in private fields and methods and should never be modified in-place. Instead, they provide public methods that return copies with altered values, leaving the original object intact.</p> <p>With immutability, a design decision inherited by the old Pipelime, we can be certain that every object is in the correct state everytime, since it cannot possibly change, and this prevents many issues when the same function is run multiple times, possibly in non-deterministic order.</p> <p>Example</p> <p>Let's say you have a sample containing an item named <code>image</code> with an RGB image. You want to resize the image reducing the resolution to 50% of the original size.</p> <p>To change the image in a sample, you need to create a new sample in which the <code>image</code> item contains the resized image.</p> <pre><code>def half_res(image: np.ndarray) -&gt; np.ndarray:\n    # Some code that downscales an image by 50%\n    ...\n\n# Read the image (more details later)\nimage = sample[\"image\"]()\n\n# Downscale the image\nhalf_image = half_res(image)\n\n# Create a new sample with the new (downscaled) image\nnew_sample = sample.with_value(\"image\", half_image)\n</code></pre> <p>At the end of the snippet above, the <code>sample</code> variable will still contain the original full-size image. Instead, <code>new_sample</code> will contain the new resized image.</p> <p>There are only two exceptions to this immutability rule:</p> <ol> <li>Caches: They need to change their state to save time when the result of a computation is already known. Since all other data is immutable, caches never need to be invalidated.</li> <li>Inner data: While all pipewine data objects are immutable, this may not be true for the data contained within them. If your item contains mutable objects, you are able to modify them implace. But never do that! </li> </ol> <p>Python, unlike other languages, has no mechanism to enforce read-only access to an object, the only way to do so would be to perform a deep-copy whenever an object is accessed, but that would be a complete disaster performance-wise.</p> <p>So, when dealing with mutable data structures inside your items, make sure you either:</p> <ul> <li>Access the data without applying changes.</li> <li>Create a deep copy of the data before applying in-place changes.</li> </ul> <p>Danger</p> <p>Never do this!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Apply some in-place changes to image\nimage += 1\nimage *= 0.9 \nimage += 1   \n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present both in the old and new sample, violating the immutability rule.</p> <p>Success</p> <p>Do this instead!</p> <pre><code># Read the image from a sample\nimage = sample[\"image\"]() # &lt;-- We need to call the item () to retrieve its content\n\n# Create a copy of the image with modified data\nimage = image + 1\n\n# Since image is now a copy of the original data, you can now apply all \n# the in-place changes you like now. \nimage *= 0.9 # Perfectly safe\nimage += 1   # Perfectly safe\n\n# Create a new sample with the new image\nnew_sample = sample.with_value(\"image\", image)\n</code></pre> <p>The modified image will be present only in the new sample.</p>"},{"location":"tutorial/installation/","title":"\ud83d\udee0\ufe0f Installation","text":""},{"location":"tutorial/installation/#basic-installation","title":"Basic Installation","text":"<p>Before installing make sure you have:</p> <ul> <li>A Python3.12+ interpreter installed on your system.</li> <li>A Virtual Environment, which is highly recommended to avoid messing up your system-level python environment.</li> <li> <p>A relatively up-to-date version of <code>pip</code>. You can upgrade to the latest version using</p> <pre><code>pip install -U pip\n</code></pre> </li> </ul> <p>To install Pipewine, run:</p> <pre><code>pip install pipewine\n</code></pre> <p>You can then verify whether <code>pipewine</code> was correctly installed by calling the CLI:</p> <pre><code>pipewine --version\n</code></pre> <p>Success</p> <p>In case the installation is successful, you should see the version of the current Pipewine installation, e.g:</p> <pre><code>0.1.0\n</code></pre> <p>Failure</p> <p>In case something went wrong, you should see something like (this may vary based on your shell type):</p> <pre><code>bash: command not found: pipewine\n</code></pre> <p>In this case, do the following:</p> <ol> <li>Check for any <code>pip</code> error messages during installation.</li> <li>Go back through all steps and check whether you followed them correctly.</li> <li>Open a GitHub issue describing the installation problem. </li> </ol>"},{"location":"tutorial/installation/#dev-installation","title":"Dev Installation","text":"<p>If you are a dev and want to install Pipewine for development purposes, it's recommended you follow these steps instead:</p> <ol> <li>Clone the github repo in a folder of your choice:     <pre><code>git clone https://github.com/lucabonfiglioli/pipewine.git\ncd pipewine\n</code></pre></li> <li>Create a new virtual environment:     <pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate \n</code></pre></li> <li>Update pip:     <pre><code>pip install -U pip\n</code></pre></li> <li>Install pipewine in edit mode:     <pre><code>pip install -e .\n</code></pre></li> <li> <p>Install pipewine optional dependencies     <pre><code>pip install .[dev] .[docs]\n</code></pre></p> <p>Warning</p> <p>With some shells (like <code>zsh</code>), you may need to escape the square brackets e.g: <code>.\\[dev\\]</code> or <code>.\\[docs\\]</code>.</p> </li> </ol>"},{"location":"tutorial/overview/","title":"\ud83d\udca0 Overview","text":""},{"location":"tutorial/overview/#high-level","title":"High Level","text":"<p>Pipewine provides you with tools to help decouple what you do with data from the way data is represented and stored. It does so by providing a set of abstractions for many aspects of your data pipeline:</p> <ul> <li><code>Dataset</code>, <code>Sample</code>, <code>Item</code> define how the data is structured, how many data samples are there, in which order, what is their content, how are they accessed etc...</li> <li>More low-level abstractions such as <code>Parser</code> and <code>Reader</code> define how data is encoded and stored. </li> <li><code>DatasetSource</code>, <code>DatasetSink</code>, <code>DatasetOperator</code> define how the data is read, written and transformed, and consistitute the base building blocks for workflows.</li> <li><code>Workflow</code> defines how a set of operators are interconnected. They can be seen as DAGs (Directed Acyclic Graph) in which nodes are sources, sinks or operators, and edges are datasets. </li> </ul> <p>All of these components are designed to allow the user to easily create custom implementations that can be seamlessly integrated with all the built-in blocks.</p> <p>By doing so, Pipewine (much like Pipelime) encourages you to write components that are likely to be highly re-usable.</p>"},{"location":"tutorial/overview/#extendibility","title":"Extendibility","text":"<p>Pipewine is completely agnostic on the following aspects of your data:</p> <ul> <li>Storage location: you can store data anywhere you want, on the file system, on a DB of your choice, on the device memory, on a remote source. You just need to implement the necessary components. </li> <li>Data encoding: By default Pipewine supports some popular image encodings, JSON/YAML metadata, numpy encoding for array data and Pickle encoding for generic python objects. You can easily add custom encodings to read/write data as you like.</li> <li>Data format: By default Pipewine supports the same built-in dataset format as Pipelime, a file system based format called \"Underfolder\" that is flexible to most use-cases but has a few limitations. Dataset formats are highly dependent on the application, thus Pipewine allows you to fully take control on how to structure your datasets.</li> <li>Data operators: As mentioned previously, you can define custom operators that do all sorts of things with your data. Built-in operators cover some common things you may want to do at some point such as concatenating two or more datasets, filtering samples based on a criterion, splitting datasets into smaller chunks, apply the same function (called <code>Mapper</code>) to all samples of a dataset.  </li> </ul>"},{"location":"tutorial/overview/#a-note-on-performance","title":"A Note on Performance","text":"<p>Pipewine is a python package and it's currently 100% python, therefore it's certainly going to be orders of magnitude slower than it could be if written in another language.</p> <p>Having said that, Pipewine still tries its best to maximize efficiency by leveraging:</p> <ul> <li>Caching: Results of computations can be cached to avoid being computed multiple times. This was also done by Pipelime, but they way cache works underwent many changes in the rewrite.</li> <li>Parallelism: Many operations are automatically run in parallel with a multi-processing pool of workers. </li> <li>Linking: When writing to file system, Pipewine automatically attempts to leverage hard-links where possible to avoid serializing and writing the same file multiple times.</li> <li>Vectorization: Where possible, Pipewine uses Numpy to perform vectorized computation on batches of data, achieving better performance if compared to plain python code.</li> </ul> <p>Furthermore, when performing complex operations such as image processing, inference with AI models, 3D data processing, the performance overhead of Pipewine will likely become negligible if compared to the complexity of the individual operations.</p>"},{"location":"tutorial/overview/#a-note-on-scalability","title":"A Note on Scalability","text":"<p>Pipewine - and its predecessor Pipelime - are meant to quickly let you manipulate data without either having to:</p> <ul> <li>Coding everything from scratch and come up with meaningful abstractions yourself. </li> <li>Setting up complex and expensive frameworks that can run data pipelines on distributed systems with many nodes.</li> </ul> <p>Warning</p> <p>If you are running data pipelines on petabytes of data, in distributed systems, with strong consistency requirements and the need for data replication at each step, Pipewine is not what you are looking for.</p> <p>Success</p> <p>If you need to run data pipelines on small/medium datasets (in the order of gigabytes) and want a flexible tool to help you do that, then Pipewine might be what you are looking for.</p>"},{"location":"tutorial/workflows/","title":"\u267b\ufe0f Workflows","text":""},{"location":"tutorial/workflows/#overwiew","title":"Overwiew","text":"<p>So far in this tutorial we always focused on simple cases where we only needed to apply a single operation to our data, with the only exception of a few cache-related examples where we saw the interactions and side-effects of two lazy operations applied sequentially. </p> <p>While applying a single operation to your data may be useful in some situations, more often than not you will need to apply multiple operations in a repeatable and organized way. One way of doing this is to write a python script that calls the right operations in the right order and that's it, but with a little extra effort you can turn your script into a Pipewine <code>Workflow</code>: a Directed Acyclic Graph (DAG) where nodes represent Actions (either a source, operator or sinks) and edges represent the dependencies between them.</p> <p>When using Pipewine workflows, you have the following advantages:</p> <ul> <li>Pipewine can automatically draw a 2D representation of your workflow to help you (and others) understand what it does without reading your code. </li> <li>Pipewine automatically attaches callbacks to all operations to track the progress of your workflow while it's running. Progress updates can be visualized live in a TUI (Text-based User Interface) to monitor the progress of long workflows.</li> <li>The code is transformed into a data structure that can be inspected before running it. </li> <li>Pipewine can inject logic into your code (e.g. caches or checkpoints) without you having to write them manually.  </li> </ul>"},{"location":"tutorial/workflows/#example-workflow","title":"Example Workflow","text":"<p>Instead of going through every single workflow component, we will instead focus on how to create and run workflows at a higher level, with an example use case where we perform some operations to augment and rearrange the same old dataset with letters.</p> <p>Here is a summary of all the operations we want to perform in our example DAG:</p> <ol> <li>Read a \"Letter\" dataset - used many times as an example toy dataset in this tutorial. </li> <li>Repeat the dataset 10 times.</li> <li>Apply random color jitter to each sample.</li> <li>Group letters by their type (either 'vowel' or 'consonant').</li> <li>Concatenate the two (vowels and consonants) splits into a single dataset.</li> <li>Write the dataset (A).</li> <li>Sort the vowels by average color brightness.</li> <li>Sort the consonants by average color brightness.</li> <li>Contatenate the sorted splits into a single dataset.</li> <li>Write the dataset (B).</li> </ol> <p>A graphical representation of the operations involved in the workflow (this image was generated using a Pipewine utility to draw workflows).  </p> <p>We start by defining the schema of our data, in this case it's the same as the one used previously in this tutorial:</p> <pre><code>class LetterMetadata(pydantic.BaseModel):\n    letter: str\n    color: str\n\nclass SharedMetadata(pydantic.BaseModel):\n    vowels: list[str]\n    consonants: list[str]\n\nclass LetterSample(TypedSample):\n    image: Item[np.ndarray]\n    metadata: Item[LetterMetadata]\n    shared: Item[SharedMetadata]\n</code></pre> <p>Some steps of our workflow need a custom implementation, or are simply not provided by Pipewine, namely:</p> <ul> <li>The mapper that applies random color jittering (very rudimental): <pre><code>class ColorJitter(Mapper[LetterSample, LetterSample]):\n    def __call__(self, idx: int, x: LetterSample) -&gt; LetterSample:\n        image = x.image()\n        col = np.random.randint(0, 255, (1, 1, 3))\n        alpha = np.random.uniform(0.1, 0.9, [])\n        image = (image * alpha + col * (1 - alpha)).clip(0, 255).astype(np.uint8)\n        return x.with_values(image=image)\n</code></pre></li> <li>The group-by function that separates vowels from consonants: <pre><code>def group_fn(idx: int, sample: LetterSample) -&gt; str:\n    return \"vowel\" if sample.metadata().letter in \"aeiou\" else \"consonant\"\n</code></pre></li> <li>The sort function needed to sort samples by image brightness: <pre><code>def sort_fn(idx: int, sample: LetterSample) -&gt; float:\n    return float(sample.image().mean())\n</code></pre></li> <li>An operation to concatenate all datasets in a dictionary of datasets. Needed because <code>CatOp</code> accepts a sequence of datasets, not a mapping.  <pre><code>class GroupCat(DatasetOperator[Mapping[str, Dataset], Dataset]):\n    def __call__(self, x: Mapping[str, Dataset]) -&gt; Dataset:\n        return CatOp()(list(x.values()))\n</code></pre></li> </ul> <p>Let's pretend we don't know anything about workflows and just write some Python code to apply the operations in the correct order:</p> <pre><code># Input, outputs, grabber\ninput_folder: Path(\"tests/sample_data/underfolders/underfolder_0\")\noutput_a: Path(\"/tmp/out_a\")\noutput_b: Path(\"/tmp/out_b\")\ngrabber = Grabber(8, 50)\n\n# (1) Read the data\ndata = UnderfolderSource(input_folder, sample_type=LetterSample)()\n\n# (2) Repeat the dataset 10 times\ndata = RepeatOp(10)(data)\n\n# (3) Apply random color jitter \n# We need a checkpoint after this operation to avoid recomputing a random function! \ndata = MapOp(ColorJitter())(data)\nUnderfolderSink(ckpt_path := Path(\"/tmp/checkpoint\"), grabber=grabber)\ndata = UnderfolderSource(ckpt_path, sample_type=LetterSample)()\n\n# (4) Group letters by their type\ngroups = GroupByOp(group_fn)(data)\n\n# (5) Concatenate the two types\ndata = GroupCat()(groups)\n\n# (6) Write the dataset A\nUnderfolderSink(out_a, grabber=grabber)(data)\n\n# (7, 8) Sort the two splits by average color brightness\nvowels = SortOp(sort_fn)(groups[\"vowel\"])\nconsonants = SortOp(sort_fn)(groups[\"consonant\"])\n\n# (9) Concatenate the sorted splits \ndata = CatOp()([vowels, consonants])\n\n# (10) Write the dataset B\nUnderfolderSink(out_b, grabber=grabber)(data)\n</code></pre> <p>Now, let's re-write our code using workflows, by applying the following changes:</p> <ol> <li>Create an empty <code>Workflow</code> object named <code>wf</code> at the beginning of our code.</li> <li>Wrap each action call using <code>wf.node()</code>. </li> <li>Call the <code>run_workflow</code> function at the end of our code.</li> </ol> <pre><code># Input, outputs, grabber\ninput_folder: Path(\"tests/sample_data/underfolders/underfolder_0\")\noutput_a: Path(\"/tmp/out_a\")\noutput_b: Path(\"/tmp/out_b\")\ngrabber = Grabber(8, 50)\n\n# Create the worfklow object\nwf = Workflow(WfOptions(checkpoint_grabber=grabber))\n\n# (1) Read the data\ndata = wf.node(UnderfolderSource(input_folder, sample_type=LetterSample))()\n\n# (2) Repeat the dataset 10 times\ndata = wf.node(RepeatOp(10))(data)\n\n# (3) Apply random color jitter (with checkpointing)\ndata = wf.node(MapOp(ColorJitter()), options=WfOptions(checkpoint=True))(data)\n\n# (4) Group letters by their type\ngroups = wf.node(GroupByOp(group_fn))(data)\n\n# (5) Concatenate the two types\ndata = wf.node(GroupCat())(groups)\n\n# (6) Write the dataset A\nwf.node(UnderfolderSink(out_a, grabber=grabber))(data)\n\n# (7, 8) Sort the two splits by average color brightness\nvowels = wf.node(SortOp(sort_fn))(groups[\"vowel\"])\nconsonants = wf.node(SortOp(sort_fn))(groups[\"consonant\"])\n\n# (9) Concatenate the sorted splits \ndata = wf.node(CatOp())([vowels, consonants])\n\n# (10) Write the dataset B\nwf.node(UnderfolderSink(out_b, grabber=grabber))(data)\n\n# Run the workflow\nrun_workflow(wf)\n</code></pre> <p>This code is very similar to the previous and it behaves the same way if executed. What changed is that instead of being executed as soon as operators are called, they are first converted into a DAG, then executed upon calling the <code>run_workflow</code> function. Nothing is done until the last line of code. </p>"},{"location":"tutorial/workflows/#workflow-creation","title":"Workflow Creation","text":"<p>Workflow objects can be created by simply calling the <code>Workflow()</code> constructor. However, you can customize some aspects of your workflow by customizing the workflow options.</p> <p>Workflow options are contained into the <code>WfOptions</code> class and currently include some settings to add a cache or a checkpoint after the operation, automatically collect garbage after an operation completes etc... You can find all details in the API reference. </p> <p>All of these options do not provide a default value, but instead default to an object <code>Default</code>, which is simply a marker that acts as a sentinel. The reason behind this is that you can use <code>WfOptions</code> at the moment in which the workflow is created, but also when adding individual nodes. In these cases, the options of a node always override the ones of the workflow. </p> <p>The true default values of the workflow options is instead specific of the <code>WorkflowExecutor</code>, a component (detailed later) that is responsible for scheduling and executing the pipeline. </p> <p>The order in which the option values are resolved is as follows:</p> <ol> <li>The value specified in the node options, if not <code>Default</code>.</li> <li>The value specified in the workflow options, if not <code>Default</code>.</li> <li>The default value for this <code>WorkflowExecutor</code>. </li> </ol> <p>This is useful because if we want to always write a checkpoint after executing a node, except for a few places, we can write less code by setting <code>checkpoint=True</code> in the workflow options and then setting it to false for the one or two nodes that don't need it.</p>"},{"location":"tutorial/workflows/#workflow-components","title":"Workflow Components","text":"<p>As demonstrated in the previous example, nothing really happens until the <code>run_workflow</code> function is called, despite our code looking pretty much the same as the example without workflows, but how does that happen in practice?</p> <p>When calling the <code>node()</code> method, the action (source, sink or operation) that is passed as first argument is memorized in a data structure inside the <code>Workflow</code> object and a mocked version of it is returned. This mocked version retains exactly the same interface as the true version, but when called it will only record that the operation needs to be called with the given inputs, without actually executing it.</p> <p>Inputs and outputs accepted and returned by the mocked actions are not actual datasets, but only <code>Proxy</code> objects whose only purpose is to serve as a placeholder to create new connections when a new action is called. As actions can accept/return a dataset, a sequence, a tuple, a mapping or a bundle of datasets, mocked actions accept/return proxies for every type of data. </p> <p>Failure</p> <p>With proxies you cannot do the things you would normally do with the actual outputs of an node. E.g. accessing a sample of a proxy dataset, or trying to get its length will raise an error.</p> <p>The dataset is not actually been computed yet, there is no way to know its length in advance!</p> <p>Complete list of things that can or cannot be done with proxies:</p> <ul> <li> <p>Proxy <code>Dataset</code>:</p> <ul> <li>\u274c Getting the length using <code>__len__</code>.</li> <li>\u274c Accessing a sample or a slice using <code>__getitem__</code>.</li> <li>\u274c Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow, either directly or through a list/tuple/mapping/bundle.</li> </ul> </li> <li> <p>Proxy <code>Sequence[Dataset]</code> or <code>tuple[Dataset, ...]</code> where the number of elements is not statically known:</p> <ul> <li>\u274c Getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element using <code>__getitem__</code>. In this case a proxy dataset will be returned.</li> <li>\u274c Extracting a slice using <code>__getitem__</code>. </li> <li>\u274c Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>tuple[Dataset]</code> where the number of elements is statically known: </p> <ul> <li>\u2705 Getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element or a slice using <code>__getitem__</code>. In this case a proxy dataset or a tuple of proxy datasets will be returned.</li> <li>\u2705 Iterate through it using <code>__iter__</code>.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>Mapping[str, Dataset]</code>:</p> <ul> <li>\u274c getting the length using <code>__len__</code>.</li> <li>\u2705 Accessing an element using <code>__getitem__</code>. In this case a proxy dataset will be returned.</li> <li>\u274c Iterate through keys, values or items using <code>keys()</code>, <code>values()</code>, <code>items()</code> or <code>__iter__</code>.</li> <li>\u274c Determine whether the mapping contains a given key.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> <li> <p>Proxy <code>Bundle[Dataset]</code>:</p> <ul> <li>\u2705 Accessing an element using the <code>__getattr__</code> (dot notation). In this case a proxy dataset will be returned.</li> <li>\u2705 Convert it to a regular dictionary of proxy datasets with the <code>as_dict()</code> method.</li> <li>\u2705 Pass it to another node of the workflow.</li> </ul> </li> </ul> <p>Important</p> <p>As you can see, there are many limitations to what can be done with these proxy objects and this may seem like a huge limitation. </p> <p>In reality, these limitations only apply during the workflow definition phase: nothing prevents you from accessing the real length of a list of datasets inside a node of the workflow. </p> <p>These limitations are only apparent. Think about it: you must execute the workflow to get its results, and to execute it you must first construct it. It would not make any sense if in order to construct the workflow we would first need to take decisions based on its results, that would be a circular definition!</p> <p>Furthermore, while Pipewine makes these limitations apparent, they are not something new. In fact, even with the old Pipelime, you cannot take decision on the outputs of nodes before executing a DAG. A typical example of this is when splitting a dataset: you must know in advance the number of splits in order to use them in a DAG.</p> <p>Pipewine actually has less limitations in that regard, because lists (or mappings) of datasets can be passed as-is to other nodes of the workflow, even if their length and content is completely unknown, this was not possible using Pipelime DAGs.</p> <p>Tip</p> <p>If you ever need to know the result of a workflow in order to construct it, it may help to:</p> <ul> <li>Statically define certain aspects of the workflow using constants: e.g. if you know in advance the length of a list of datasets as a constant, it makes no sense to compute it using <code>__len__</code>, use the constant value instead.</li> <li>Restructure your code, maybe you are giving a responsibility to the workflow  that should be given to one or more <code>DatasetOperator</code>. </li> <li>As a last resort, avoid using a workflow and instead use a plain python function.</li> </ul>"},{"location":"tutorial/workflows/#workflow-execution","title":"Workflow Execution","text":"<p>Workflows on their own do not do much: they only consist of the graph data structure that, plus some methods to conveniently add new nodes and edges or to inspect them.</p> <p>To execute a workflow you need a <code>WorkflowExecutor</code>, an object whose main responsibility is running the actions contained in the <code>Workflow</code> object on the correct data inputs and in the correct order. It also manages caches, checkpoints, cleanup and sends events to whoever is listening.</p> <p>The only implementation provided (so far) by Pipewine is a very naive executor that simply topologically sorts the workflow and runs each action sequentially, hence the name <code>SequentialWorkflowExecutor</code>. </p> <p>Using it is very simple: you just have to construct the executor and pass your workflow to the <code>execute</code> method: </p> <pre><code>wf: Workflow\n\nexecutor = SequentialWorkflowExecutor()\nexecutor.execute(wf)\n</code></pre> <p>More conveniently, you can simply call the <code>run_workflow</code> function that does something similar under the hood. If not specified, the workflow executor will default to a new instance of <code>SequentialWorkflowExecutor</code>, to spare you the need to construct it and call it explicitly.</p> <pre><code>wf: Workflow\n\nrun_workflow(wf)\n</code></pre>"},{"location":"tutorial/workflows/#worfklow-progress-tracking","title":"Worfklow Progress Tracking","text":"<p>Each node of the workflow, which corresponds to an action, can start zero or more tasks. By \"task\" we mean a finite, ordered collection of units of work that are executed in a single call to an action.</p> <p>Example</p> <p>In the <code>GroupByOp</code>, an operator that splits the input dataset into many parts that have in common a specific key, we need to iterate over the input dataset and retrieve the value associated to the group-by key. This is a task: </p> <ul> <li>It starts after the <code>GroupByOp</code> is called.</li> <li>It has a number of units equal to the length of the input dataset. </li> <li>The i-th unit corresponds to the computation of the group-by key in the i-th sample.</li> <li>It finishes before the <code>GroupByOp</code> returns.</li> </ul> <p>Note</p> <p>Lazy operators do not typically create tasks when called.</p> <p>Tip</p> <p>The <code>self.loop()</code> method of a Pipewine action will automatically invoke callbacks to start, update and finish a new task each time it is called. </p> <p>If a <code>Grabber</code> is used (to speed up computation through parallelism), all worker sub-processes will be configured to automatically emit events each time they complete a unit of work.</p> <p>Workflow executors can be attached to an <code>EventQueue</code>, a FIFO queue that collects any progress update event from the executor and lets whoever is listening to it, typically a <code>Tracker</code>, receive these updates in the correct order. </p> <p><code>EventQueue</code> objects are designed so that the same queue can receive updates from many emitters, possibly located on different processes. This is crucial to enable progress tracking when using a <code>Grabber</code>. The only <code>EventQueue</code> implementation provided so far by Pipewine is the <code>ProcessSharedEventQueue</code>, built on top of a <code>multiprocessing.Pipe</code> object.</p> <p><code>Tracker</code> objects are instead consumers of progress update events. They constantly listen for new updates and display them somewhere. The only <code>Tracker</code> implementation provided by Pipewine is <code>CursesTracker</code>, that conveniently displays progress updates in a ncurses TUI (Text-based User Interface) for immediate reading. </p> <p>To connect all these things together, you need to do the following things:</p> <ol> <li>Construct and populate a <code>Workflow</code>.</li> <li>Construct a <code>WorkflowExecutor</code>.</li> <li>Construct a <code>EventQueue</code>.</li> <li>Construct a <code>Tracker</code>.</li> <li>Call the <code>start()</code> method of the <code>EventQueue</code>.</li> <li>Attach the executor to the running event queue.</li> <li>Attach the tracker to the running event queue.</li> <li>Execute the workflow.</li> <li>Detach the executor from the event queue.</li> <li>Detach the tracker from the event queue.</li> <li>Close the event queue.</li> </ol> <pre><code># Workflows and other components\nworkflow: Workflow\nexecutor = SequentialWorkflowExecutor()\nevent_queue = ProcessSharedEventQueue()\ntracker = CursesTracker()\n\n# Setup\nevent_queue.start()\nexecutor.attach(event_queue)\ntracker.attach(event_queue)\n\n# Execution\nexecutor.run(workflow)\n\n# Teardown\nexecutor.detach()\ntracker.detach()\nevent_queue.close()\n</code></pre> <p>This is kind of complicated right? To simplify things a bit on your side, the <code>run_workflow</code> function does all this complicated stuff for you. Equivalently to the code above:</p> <pre><code>workflow: Workflow\ntracker = CursesTracker()\n\nrun_workflow(workflow, tracker=tracker)\n</code></pre> <p></p>"},{"location":"tutorial/workflows/#workflow-drawing","title":"Workflow Drawing","text":"<p>Workflows can be drawn using the <code>draw_workflow</code> function:</p> <pre><code>workflow: Workflow\n\ndraw_workflow(workflow, Path(\"/tmp/workflow.svg\"))\n</code></pre> <p>Under the hood, this function calls two different components:</p> <ul> <li><code>Layout</code>: responsible for determining the position and size of workflow nodes in a 2D image plane.</li> <li><code>Drawer</code>: responsible for taking an already laid out workflow and writing a file that can be visualized by external software such as a web browser or an image viewer.</li> </ul> <p>As for the layout part, Pipewine provides <code>OptimizedLayout</code>, a class that attempts to position the nodes on a 2D grid such that the overall flow can be read from left to right such that:</p> <ul> <li>Nodes are not too much spread out, nor too close to each other.</li> <li>Excessively long edges are penalized.</li> <li>Edge-edge intersections are minimized.</li> <li>Edge-node intersections are minimized.</li> <li>Edges are directed towards the right side of the image.</li> </ul> <p>This is achieved using a very simple evolutionary approach. The implementation is kind of difficult to follow due to heavy use of vectorization to eliminate all Python for loops and control flow, crucial to ensure acceptable performance (&lt;1000ms execution time). </p> <p><code>SVGDrawer</code> takes the laid-out graph and writes it into a SVG file that can be opened in any modern web browser and embedded pretty much everywhere.</p>"}]}